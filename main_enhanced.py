# ë°ì´í„° ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤
import pandas as pd  # ì—‘ì…€, CSV íŒŒì¼ ì²˜ë¦¬
import json  # JSON ë°ì´í„° ì²˜ë¦¬
import time  # ëŒ€ê¸° ì‹œê°„ ì²˜ë¦¬
from pathlib import Path  # íŒŒì¼ ê²½ë¡œ ì²˜ë¦¬
import math  # ìˆ˜í•™ ê³„ì‚°
import datetime  # ì‹œê°„ ì²˜ë¦¬
from tqdm import tqdm  # ì§„í–‰ë¥  í‘œì‹œ
from concurrent.futures import ThreadPoolExecutor, as_completed  # ë³‘ë ¬ ì²˜ë¦¬
from collections import Counter  # ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°
import re  # ì •ê·œí‘œí˜„ì‹

# Google Cloud AI ë¼ì´ë¸ŒëŸ¬ë¦¬
import vertexai  # Google Vertex AI í”Œë«í¼
from vertexai.generative_models import GenerativeModel  # AI ëª¨ë¸

# ê³ ë„í™”ëœ AI ê°ì • ë¶„ì„ ì§€ì‹œì‚¬í•­ í…œí”Œë¦¿
ENHANCED_PROMPT_TEMPLATE = """
[í˜ë¥´ì†Œë‚˜]
ë‹¹ì‹ ì€ ì„œìš¸ì•„ì‚°ë³‘ì› ë‚´ë¶€ ì§ì› í˜‘ì—… í”¼ë“œë°±ì„ ë¶„ì„í•˜ëŠ” ê³ ë„í™”ëœ AI ê°ì • ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì˜ë£Œì§„ê°„ í˜‘ì—… ë§¥ë½ì„ ê¹Šì´ ì´í•´í•˜ê³ , 8ê°€ì§€ ì„¸ë¶„í™”ëœ ê°ì •ê³¼ ë³µí•© ê°ì •, ê·¸ë¦¬ê³  ê°ì •ì˜ ì›ì¸ê¹Œì§€ ë¶„ì„í•˜ëŠ” ê²ƒì´ ë‹¹ì‹ ì˜ ì„ë¬´ì…ë‹ˆë‹¤.

[ê³ ë„í™”ëœ ê°ì • ë¶„ì„ ì§€ì‹œì‚¬í•­]

1. **í…ìŠ¤íŠ¸ ì •ì œ ë° ë¹„ì‹ë³„ ì²˜ë¦¬**: ê¸°ì¡´ ê·œì¹™ê³¼ ë™ì¼í•˜ê²Œ ì ìš©

2. **8ê°€ì§€ ì„¸ë¶„í™” ê°ì • ë¶„ì„**:
   - **ê¸ì •êµ°**: "ê¸°ì¨", "ê°ì‚¬", "ì‹ ë¢°", "ë§Œì¡±"
   - **ë¶€ì •êµ°**: "ë¶„ë…¸", "ìŠ¬í””", "ë‘ë ¤ì›€", "ì‹¤ë§"
   - **ì¤‘ë¦½êµ°**: "í‰ì˜¨", "ë¬´ê´€ì‹¬"

3. **ì˜ë£Œ í˜‘ì—… ë§¥ë½ ë¶„ì„**:
   - **í™˜ì_ì•ˆì „**: í™˜ì ì¹˜ë£Œ, ì•ˆì „, ì‘ê¸‰ìƒí™© ê´€ë ¨
   - **ì—…ë¬´_íš¨ìœ¨**: ì¼ì •, í”„ë¡œì„¸ìŠ¤, ì—…ë¬´ íë¦„ ê´€ë ¨
   - **ì¸ê°„_ê´€ê³„**: ì¡´ì¤‘, ì†Œí†µ, ë°°ë ¤, íŒ€ì›Œí¬ ê´€ë ¨
   - **ì „ë¬¸ì„±**: ì˜ë£Œ ì§€ì‹, ê¸°ìˆ , ê²½í—˜, ì—­ëŸ‰ ê´€ë ¨

4. **ë³µí•© ê°ì • ë¶„ì„**:
   - primary_emotion: ì£¼ìš” ê°ì • (ê°€ì¥ ê°•í•œ ê°ì •)
   - secondary_emotion: ë³´ì¡° ê°ì • (ë³µí•© ê°ì •ì¸ ê²½ìš°)
   - emotional_complexity: "ë‹¨ìˆœ" ë˜ëŠ” "ë³µí•©"
   - emotion_mix: ê°ì • ë¹„ìœ¨ (ë³µí•©ì¸ ê²½ìš°ë§Œ)

5. **ê°ì • ì›ì¸ ë¶„ì„** (ë¶€ì • ê°ì •ì¸ ê²½ìš°):
   - **ì†Œí†µ_ë¬¸ì œ**: ì˜ì‚¬ì†Œí†µ ë¶€ì¡±, ì •ë³´ ì „ë‹¬ ì˜¤ë¥˜
   - **ì‹œê°„_ì••ë°•**: ì¼ì • ì§€ì—°, ì—…ë¬´ ê³¼ë¶€í•˜
   - **ê¸°ìˆ _ë¶€ì¡±**: ì „ë¬¸ì„± ë¶€ì¡±, ê²½í—˜ ë¶€ì¡±
   - **íƒœë„_ë¬¸ì œ**: ë¶ˆì¹œì ˆ, ë¹„í˜‘ì¡°ì  íƒœë„
   - **ì‹œìŠ¤í…œ_ë¬¸ì œ**: í”„ë¡œì„¸ìŠ¤ ë¹„íš¨ìœ¨, ë„êµ¬ ë¶€ì¡±

6. **ê°œì„  ë°©ì•ˆ ìë™ ì œì•ˆ** (ë¶€ì • ê°ì •ì¸ ê²½ìš°):
   - ê°ì • ì›ì¸ì— ë”°ë¥¸ êµ¬ì²´ì  ê°œì„  ë°©ì•ˆ ì œì‹œ

[ì˜ë£Œì§„ í˜‘ì—… íŠ¹í™” ê°ì • í‚¤ì›Œë“œ]
- **ê¸°ì¨**: "ì¢‹ì•„ìš”", "í›Œë¥­í•´ìš”", "ë©‹ì ¸ìš”", "ê¸°ë»ìš”"
- **ê°ì‚¬**: "ê°ì‚¬", "ê³ ë§ˆì›Œ", "ë„ì›€", "ìˆ˜ê³ "
- **ì‹ ë¢°**: "ë¯¿ì–´", "ì˜ì§€", "ì „ë¬¸ì ", "ì•ˆì‹¬", "ì‹ ë¢°"
- **ë§Œì¡±**: "ë§Œì¡±", "ì¶©ë¶„", "ê´œì°®ì•„", "ì¢‹ì•˜ì–´"
- **ë¶„ë…¸**: "í™”ë‚˜", "ì§œì¦", "ë‹µë‹µ", "ë¶ˆë§Œ", "ì—´ë°›ì•„"
- **ìŠ¬í””**: "ì†ìƒ", "ìŠ¬í¼", "ì•ˆíƒ€ê¹Œì›Œ", "ìš°ìš¸"
- **ë‘ë ¤ì›€**: "ê±±ì •", "ë¶ˆì•ˆ", "ë¬´ì„œì›Œ", "ì—¼ë ¤"
- **ì‹¤ë§**: "ì•„ì‰¬ì›Œ", "ì‹¤ë§", "ê¸°ëŒ€ì´í•˜", "ë¶€ì¡±"

[ì¶œë ¥ í˜•ì‹]
ë°˜ë“œì‹œ ì•„ë˜ JSON êµ¬ì¡°ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:

{
  "refined_text": "ì •ì œëœ í…ìŠ¤íŠ¸",
  "is_anonymized": false,
  "primary_emotion": "ê°ì‚¬",
  "secondary_emotion": "ì‹ ë¢°",
  "emotional_complexity": "ë³µí•©",
  "emotion_mix": {"ê°ì‚¬": 0.7, "ì‹ ë¢°": 0.3},
  "sentiment_intensity": 8,
  "confidence_score": 9,
  "medical_context": "ì¸ê°„_ê´€ê³„",
  "root_cause": "ì—†ìŒ",
  "improvement_suggestion": "ì—†ìŒ",
  "key_terms": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2"],
  "labels": ["ë¼ë²¨1", "ë¼ë²¨2"]
}

[ë¶„ì„ ì˜ˆì‹œ]

**ì˜ˆì‹œ 1: ë³µí•© ê¸ì • ê°ì •**
ì›ë³¸: "ê¹€ì„ ìƒë‹˜ ë•ë¶„ì— í™˜ì ì¼€ì–´ë„ ì˜ ë˜ê³  ì •ë§ ê°ì‚¬í•´ìš”. ë¯¿ê³  ì¼í•  ìˆ˜ ìˆì–´ì„œ ë“ ë“ í•©ë‹ˆë‹¤."
ì¶œë ¥:
{
  "refined_text": "ê¹€ì„ ìƒë‹˜ ë•ë¶„ì— í™˜ì ì¼€ì–´ë„ ì˜ ë˜ê³  ì •ë§ ê°ì‚¬í•´ìš”. ë¯¿ê³  ì¼í•  ìˆ˜ ìˆì–´ì„œ ë“ ë“ í•©ë‹ˆë‹¤.",
  "is_anonymized": false,
  "primary_emotion": "ê°ì‚¬",
  "secondary_emotion": "ì‹ ë¢°",
  "emotional_complexity": "ë³µí•©",
  "emotion_mix": {"ê°ì‚¬": 0.6, "ì‹ ë¢°": 0.4},
  "sentiment_intensity": 8,
  "confidence_score": 9,
  "medical_context": "ì¸ê°„_ê´€ê³„",
  "root_cause": "ì—†ìŒ",
  "improvement_suggestion": "ì—†ìŒ",
  "key_terms": ["í™˜ìì¼€ì–´", "ê°ì‚¬", "ë¯¿ê³ ì¼í• ìˆ˜ìˆì–´"],
  "labels": ["ì—…ë¬´ íƒœë„", "ìƒí˜¸ ì¡´ì¤‘"]
}

**ì˜ˆì‹œ 2: ë³µí•© ë¶€ì • ê°ì •**
ì›ë³¸: "ì´ì •ì€ ê°„í˜¸ì‚¬ê°€ í™˜ì ì²˜ì¹˜í•  ë•Œ ë„ˆë¬´ ì„œë‘˜ëŸ¬ì„œ ì‹¤ìˆ˜í• ê¹Œ ê±±ì •ë¼ìš”. ì¢€ ë” ì‹ ì¤‘í–ˆìœ¼ë©´ ì¢‹ê² ì–´ìš”."
ì¶œë ¥:
{
  "refined_text": "ë‹´ë‹¹ ê°„í˜¸ì‚¬ê°€ í™˜ì ì²˜ì¹˜í•  ë•Œ ë‹¤ì†Œ ì„œë‘˜ëŸ¬ì„œ ì‹¤ìˆ˜í• ê¹Œ ìš°ë ¤ë©ë‹ˆë‹¤. ì¢€ ë” ì‹ ì¤‘í•œ ì ‘ê·¼ì´ í•„ìš”í•´ ë³´ì…ë‹ˆë‹¤.",
  "is_anonymized": true,
  "primary_emotion": "ë‘ë ¤ì›€",
  "secondary_emotion": "ì‹¤ë§",
  "emotional_complexity": "ë³µí•©",
  "emotion_mix": {"ë‘ë ¤ì›€": 0.7, "ì‹¤ë§": 0.3},
  "sentiment_intensity": 6,
  "confidence_score": 8,
  "medical_context": "í™˜ì_ì•ˆì „",
  "root_cause": "ê¸°ìˆ _ë¶€ì¡±",
  "improvement_suggestion": "í™˜ì ì²˜ì¹˜ ì‹œ ì¶©ë¶„í•œ ì‹œê°„ í™•ë³´ ë° ì‹ ì¤‘í•œ ì ‘ê·¼ êµìœ¡ í•„ìš”",
  "key_terms": ["í™˜ìì²˜ì¹˜", "ì„œë‘˜ëŸ¬", "ì‹¤ìˆ˜ìš°ë ¤"],
  "labels": ["ì „ë¬¸ì„± ë¶€ì¡±"]
}

**ì˜ˆì‹œ 3: ë‹¨ìˆœ ê°ì •**
ì›ë³¸: "ì—…ë¬´ ì²˜ë¦¬ê°€ ëŠ¦ì–´ì„œ ë‹µë‹µí•´ìš”"
ì¶œë ¥:
{
  "refined_text": "ì—…ë¬´ ì²˜ë¦¬ ì†ë„ê°€ ë‹¤ì†Œ ì•„ì‰½ìŠµë‹ˆë‹¤.",
  "is_anonymized": false,
  "primary_emotion": "ë¶„ë…¸",
  "secondary_emotion": "ì—†ìŒ",
  "emotional_complexity": "ë‹¨ìˆœ",
  "emotion_mix": {"ë¶„ë…¸": 1.0},
  "sentiment_intensity": 6,
  "confidence_score": 8,
  "medical_context": "ì—…ë¬´_íš¨ìœ¨",
  "root_cause": "ì‹œê°„_ì••ë°•",
  "improvement_suggestion": "ì—…ë¬´ í”„ë¡œì„¸ìŠ¤ ì ê²€ ë° íš¨ìœ¨ì„± ê°œì„  ë°©ì•ˆ ëª¨ìƒ‰",
  "key_terms": ["ì—…ë¬´ì²˜ë¦¬", "ëŠ¦ì–´ì„œ", "ë‹µë‹µ"],
  "labels": ["ì „ë¬¸ì„± ë¶€ì¡±"]
}

ì´ì œ ì£¼ì–´ì§„ í˜‘ì—… í›„ê¸° í…ìŠ¤íŠ¸ë¥¼ ìœ„ í˜•ì‹ì— ë”°ë¼ ê³ ë„í™”ëœ ê°ì • ë¶„ì„ì„ ìˆ˜í–‰í•˜ì„¸ìš”.
"""

class EnhancedReviewAnalyzer:
    def __init__(self, project_id="mindmap-462708"):
        """ê³ ë„í™”ëœ ë¦¬ë·° ë¶„ì„ê¸° ì´ˆê¸°í™”"""
        self.project_id = project_id
        self.location = "us-central1"
        
        # Vertex AI ì´ˆê¸°í™”
        vertexai.init(project=project_id, location=self.location)
        self.model = GenerativeModel("gemini-2.0-flash")
        
        # í†µê³„ ë³€ìˆ˜ë“¤
        self.stats = {
            'total_processed': 0,
            'high_quality': 0,
            'needs_review': 0,
            'emotion_distribution': Counter(),
            'context_distribution': Counter(),
            'complexity_distribution': Counter()
        }
    
    def _clean_json_response(self, response_text):
        """JSON ì‘ë‹µì—ì„œ ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±° ë° ì •ë¦¬"""
        # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°
        if response_text.startswith("```json"):
            response_text = response_text[7:]
        if response_text.startswith("```"):
            response_text = response_text[3:]
        if response_text.endswith("```"):
            response_text = response_text[:-3]
        
        # ì•ë’¤ ê³µë°± ì œê±°
        response_text = response_text.strip()
        
        return response_text
    
    def analyze_single_text_enhanced(self, text):
        """ë‹¨ì¼ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ê³ ë„í™”ëœ AI ë¶„ì„ ìˆ˜í–‰"""
        if pd.isna(text) or str(text).strip() == "":
            return self._create_empty_result_enhanced()
        
        try:
            # AI ëª¨ë¸ì— ê³ ë„í™”ëœ í”„ë¡¬í”„íŠ¸ë¡œ ë¶„ì„ ìš”ì²­
            full_prompt = f"{ENHANCED_PROMPT_TEMPLATE}\n\në¶„ì„í•  í…ìŠ¤íŠ¸: \"{text}\""
            response = self.model.generate_content(full_prompt)
            
            # JSON ì‘ë‹µ ì •ë¦¬ ë° íŒŒì‹±
            response_text = self._clean_json_response(response.text)
            
            # JSON íŒŒì‹± ì‹œë„
            try:
                result = json.loads(response_text)
            except json.JSONDecodeError:
                # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„
                print(f"JSON íŒŒì‹± ì‹¤íŒ¨, ì¬ì‹œë„...")
                time.sleep(2)
                response = self.model.generate_content(full_prompt)
                response_text = self._clean_json_response(response.text)
                result = json.loads(response_text)
            
            # ê³ ë„í™”ëœ ê²°ê³¼ ê²€ì¦ ë° í’ˆì§ˆ í‰ê°€
            quality_score, quality_issues = self._evaluate_quality_enhanced(result, text)
            needs_review = quality_score < 6 or len(quality_issues) > 2
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            self._update_stats_enhanced(result, quality_score, needs_review)
            
            # ìµœì¢… ê²°ê³¼ êµ¬ì„±
            final_result = {
                'refined_text': result.get('refined_text', ''),
                'is_anonymized': result.get('is_anonymized', False),
                'primary_emotion': result.get('primary_emotion', 'ì•Œ ìˆ˜ ì—†ìŒ'),
                'secondary_emotion': result.get('secondary_emotion', 'ì—†ìŒ'),
                'emotional_complexity': result.get('emotional_complexity', 'ë‹¨ìˆœ'),
                'emotion_mix': result.get('emotion_mix', {}),
                'sentiment_intensity': result.get('sentiment_intensity', 5),
                'confidence_score': result.get('confidence_score', 5),
                'medical_context': result.get('medical_context', 'ê¸°íƒ€'),
                'root_cause': result.get('root_cause', 'ì—†ìŒ'),
                'improvement_suggestion': result.get('improvement_suggestion', 'ì—†ìŒ'),
                'key_terms': result.get('key_terms', []),
                'labels': result.get('labels', []),
                'quality_score': quality_score,
                'needs_review': needs_review,
                'quality_issues': ', '.join(quality_issues) if quality_issues else ''
            }
            
            return final_result
            
        except Exception as e:
            print(f"ë¶„ì„ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return self._create_error_result_enhanced(str(e))
    
    def _create_empty_result_enhanced(self):
        """ë¹ˆ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ê³ ë„í™”ëœ ê¸°ë³¸ ê²°ê³¼ ìƒì„±"""
        return {
            'refined_text': '',
            'is_anonymized': False,
            'primary_emotion': 'ì•Œ ìˆ˜ ì—†ìŒ',
            'secondary_emotion': 'ì—†ìŒ',
            'emotional_complexity': 'ë‹¨ìˆœ',
            'emotion_mix': {},
            'sentiment_intensity': 1,
            'confidence_score': 1,
            'medical_context': 'ê¸°íƒ€',
            'root_cause': 'ì—†ìŒ',
            'improvement_suggestion': 'ì—†ìŒ',
            'key_terms': [],
            'labels': [],
            'quality_score': 1,
            'needs_review': True,
            'quality_issues': 'ë¹ˆ í…ìŠ¤íŠ¸'
        }
    
    def _create_error_result_enhanced(self, error_msg):
        """ì˜¤ë¥˜ ë°œìƒ ì‹œ ê³ ë„í™”ëœ ê¸°ë³¸ ê²°ê³¼ ìƒì„±"""
        return {
            'refined_text': '',
            'is_anonymized': False,
            'primary_emotion': 'ì•Œ ìˆ˜ ì—†ìŒ',
            'secondary_emotion': 'ì—†ìŒ',
            'emotional_complexity': 'ë‹¨ìˆœ',
            'emotion_mix': {},
            'sentiment_intensity': 1,
            'confidence_score': 1,
            'medical_context': 'ê¸°íƒ€',
            'root_cause': 'ë¶„ì„ ì˜¤ë¥˜',
            'improvement_suggestion': 'ì¬ë¶„ì„ í•„ìš”',
            'key_terms': [],
            'labels': [],
            'quality_score': 1,
            'needs_review': True,
            'quality_issues': f'ë¶„ì„ ì˜¤ë¥˜: {error_msg}'
        }
    
    def _evaluate_quality_enhanced(self, result, original_text):
        """ê³ ë„í™”ëœ ë¶„ì„ ê²°ê³¼ì˜ í’ˆì§ˆ í‰ê°€"""
        quality_score = result.get('confidence_score', 5)
        quality_issues = []
        
        # 1. í•„ìˆ˜ í•„ë“œ ê²€ì¦
        required_fields = ['refined_text', 'primary_emotion', 'sentiment_intensity', 'medical_context']
        for field in required_fields:
            if field not in result or result[field] == '':
                quality_issues.append(f'{field} ëˆ„ë½')
                quality_score -= 2
        
        # 2. ê°ì • ì¼ê´€ì„± ê²€ì¦
        primary_emotion = result.get('primary_emotion', '')
        intensity = result.get('sentiment_intensity', 5)
        
        # ê¸ì • ê°ì •ì¸ë° ê°•ë„ê°€ ë‚®ì€ ê²½ìš°
        if primary_emotion in ['ê¸°ì¨', 'ê°ì‚¬', 'ì‹ ë¢°', 'ë§Œì¡±'] and intensity < 4:
            quality_issues.append('ê¸ì • ê°ì •-ê°•ë„ ë¶ˆì¼ì¹˜')
            quality_score -= 1
        
        # ë¶€ì • ê°ì •ì¸ë° ê°•ë„ê°€ ë„ˆë¬´ ë‚®ì€ ê²½ìš°
        if primary_emotion in ['ë¶„ë…¸', 'ìŠ¬í””', 'ë‘ë ¤ì›€', 'ì‹¤ë§'] and intensity < 3:
            quality_issues.append('ë¶€ì • ê°ì •-ê°•ë„ ë¶ˆì¼ì¹˜')
            quality_score -= 1
        
        # 3. ë³µí•© ê°ì • ê²€ì¦
        complexity = result.get('emotional_complexity', 'ë‹¨ìˆœ')
        emotion_mix = result.get('emotion_mix', {})
        
        if complexity == 'ë³µí•©' and len(emotion_mix) < 2:
            quality_issues.append('ë³µí•© ê°ì • êµ¬ì„± ë¶ˆì™„ì „')
            quality_score -= 1
        
        # 4. ì˜ë£Œ ë§¥ë½ ì ì ˆì„± ê²€ì¦
        medical_context = result.get('medical_context', '')
        valid_contexts = ['í™˜ì_ì•ˆì „', 'ì—…ë¬´_íš¨ìœ¨', 'ì¸ê°„_ê´€ê³„', 'ì „ë¬¸ì„±', 'ê¸°íƒ€']
        if medical_context not in valid_contexts:
            quality_issues.append('ì˜ëª»ëœ ì˜ë£Œ ë§¥ë½')
            quality_score -= 1
        
        # 5. í…ìŠ¤íŠ¸ ê¸¸ì´ ê²€ì¦
        refined_text = result.get('refined_text', '')
        if len(original_text.strip()) > 10 and len(refined_text.strip()) < 5:
            quality_issues.append('ì •ì œëœ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìŒ')
            quality_score -= 1
        
        # ìµœì¢… ì ìˆ˜ ë²”ìœ„ ì¡°ì •
        quality_score = max(1, min(10, quality_score))
        
        return quality_score, quality_issues
    
    def _update_stats_enhanced(self, result, quality_score, needs_review):
        """ê³ ë„í™”ëœ í†µê³„ ì •ë³´ ì—…ë°ì´íŠ¸"""
        self.stats['total_processed'] += 1
        
        if quality_score >= 7:
            self.stats['high_quality'] += 1
        
        if needs_review:
            self.stats['needs_review'] += 1
        
        # ê°ì • ë¶„í¬ ì—…ë°ì´íŠ¸
        primary_emotion = result.get('primary_emotion', 'ì•Œ ìˆ˜ ì—†ìŒ')
        self.stats['emotion_distribution'][primary_emotion] += 1
        
        # ì˜ë£Œ ë§¥ë½ ë¶„í¬ ì—…ë°ì´íŠ¸
        medical_context = result.get('medical_context', 'ê¸°íƒ€')
        self.stats['context_distribution'][medical_context] += 1
        
        # ë³µí•©ì„± ë¶„í¬ ì—…ë°ì´íŠ¸
        complexity = result.get('emotional_complexity', 'ë‹¨ìˆœ')
        self.stats['complexity_distribution'][complexity] += 1
    
    def process_batch_enhanced(self, texts, batch_size=5):
        """ê³ ë„í™”ëœ ë°°ì¹˜ ì²˜ë¦¬"""
        results = []
        
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i+batch_size]
            batch_results = []
            
            # ThreadPoolExecutorë¥¼ ì‚¬ìš©í•œ ë³‘ë ¬ ì²˜ë¦¬
            with ThreadPoolExecutor(max_workers=min(batch_size, 10)) as executor:
                # ê° í…ìŠ¤íŠ¸ì— ëŒ€í•´ ë¶„ì„ ì‘ì—… ì œì¶œ
                futures = {executor.submit(self.analyze_single_text_enhanced, text): idx 
                          for idx, text in enumerate(batch)}
                
                # ê²°ê³¼ ìˆ˜ì§‘ (ìˆœì„œ ë³´ì¥)
                batch_results = [None] * len(batch)
                for future in as_completed(futures):
                    idx = futures[future]
                    try:
                        result = future.result()
                        batch_results[idx] = result
                    except Exception as e:
                        print(f"ë°°ì¹˜ ì²˜ë¦¬ ì˜¤ë¥˜ (ì¸ë±ìŠ¤ {idx}): {e}")
                        batch_results[idx] = self._create_error_result_enhanced(str(e))
            
            results.extend(batch_results)
            
            # ì§„í–‰ìƒí™© ì¶œë ¥
            processed = min(i + batch_size, len(texts))
            print(f"ì§„í–‰ë¥ : {processed}/{len(texts)} ({processed/len(texts)*100:.1f}%)")
            
            # API í˜¸ì¶œ ì œí•œì„ ìœ„í•œ ëŒ€ê¸°
            if i + batch_size < len(texts):
                time.sleep(10)
        
        return results
    
    def print_enhanced_stats(self):
        """ê³ ë„í™”ëœ í†µê³„ ì •ë³´ ì¶œë ¥"""
        total = self.stats['total_processed']
        if total == 0:
            print("ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"\n{'='*60}")
        print(f"ğŸ§  ê³ ë„í™”ëœ ê°ì • ë¶„ì„ ê²°ê³¼ í†µê³„")
        print(f"{'='*60}")
        
        print(f"ğŸ“Š ì´ ì²˜ë¦¬ëŸ‰: {total:,}ê°œ")
        print(f"âœ… ê³ í’ˆì§ˆ ë¶„ì„: {self.stats['high_quality']:,}ê°œ ({self.stats['high_quality']/total*100:.1f}%)")
        print(f"âš ï¸  ì¬ê²€í†  í•„ìš”: {self.stats['needs_review']:,}ê°œ ({self.stats['needs_review']/total*100:.1f}%)")
        
        print(f"\nğŸ­ 8ê°€ì§€ ê°ì • ë¶„í¬:")
        for emotion, count in self.stats['emotion_distribution'].most_common():
            percentage = count/total*100
            print(f"  {emotion}: {count:,}ê°œ ({percentage:.1f}%)")
        
        print(f"\nğŸ¥ ì˜ë£Œ ë§¥ë½ ë¶„í¬:")
        for context, count in self.stats['context_distribution'].most_common():
            percentage = count/total*100
            print(f"  {context}: {count:,}ê°œ ({percentage:.1f}%)")
        
        print(f"\nğŸ”€ ê°ì • ë³µí•©ì„± ë¶„í¬:")
        for complexity, count in self.stats['complexity_distribution'].most_common():
            percentage = count/total*100
            print(f"  {complexity}: {count:,}ê°œ ({percentage:.1f}%)")


def main_enhanced():
    """ê³ ë„í™”ëœ ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    # ì„¤ì •ê°’ë“¤
    INPUT_FILE = "ì„¤ë¬¸ì¡°ì‚¬_ì „ì²˜ë¦¬ë°ì´í„°_20250620_0731.xlsx"
    COLUMN_NAME = "í˜‘ì—… í›„ê¸°"
    OUTPUT_FILE = "ì„¤ë¬¸ì¡°ì‚¬_ì „ì²˜ë¦¬ë°ì´í„°_20250620_0731_enhanced.xlsx"
    MAX_ROWS = 5  # í…ŒìŠ¤íŠ¸ìš©, ì „ì²´ ì²˜ë¦¬ì‹œ Noneìœ¼ë¡œ ë³€ê²½
    BATCH_SIZE = 1
    
    print("ğŸ§  ê³ ë„í™”ëœ í˜‘ì—… í›„ê¸° ê°ì • ë¶„ì„ ì‹œìŠ¤í…œ ì‹œì‘")
    print(f"ğŸ“‚ ì…ë ¥ íŒŒì¼: {INPUT_FILE}")
    print(f"ğŸ“Š ë¶„ì„ ì»¬ëŸ¼: {COLUMN_NAME}")
    print(f"ğŸ“ˆ ìµœëŒ€ ì²˜ë¦¬ëŸ‰: {MAX_ROWS if MAX_ROWS else 'ì „ì²´'}")
    
    try:
        # 1. ë°ì´í„° ë¡œë“œ
        print(f"\nğŸ“– ë°ì´í„° ë¡œë”© ì¤‘...")
        df = pd.read_excel(INPUT_FILE)
        
        if COLUMN_NAME not in df.columns:
            raise ValueError(f"ì»¬ëŸ¼ '{COLUMN_NAME}'ì´ íŒŒì¼ì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        
        # ì²˜ë¦¬í•  ë°ì´í„° ë²”ìœ„ ì„¤ì •
        if MAX_ROWS:
            df = df.head(MAX_ROWS)
        
        print(f"âœ… ì´ {len(df):,}ê°œ ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
        
        # 2. ê³ ë„í™”ëœ ë¶„ì„ê¸° ì´ˆê¸°í™”
        print(f"\nğŸ¤– ê³ ë„í™”ëœ AI ë¶„ì„ê¸° ì´ˆê¸°í™” ì¤‘...")
        analyzer = EnhancedReviewAnalyzer()
        
        # 3. í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì „ì²˜ë¦¬
        texts = df[COLUMN_NAME].fillna('').astype(str).tolist()
        print(f"ğŸ“ ë¶„ì„ ëŒ€ìƒ í…ìŠ¤íŠ¸: {len(texts):,}ê°œ")
        
        # 4. ê³ ë„í™”ëœ ë°°ì¹˜ ë¶„ì„ ì‹¤í–‰
        print(f"\nğŸ” ê³ ë„í™”ëœ AI ë¶„ì„ ì‹œì‘ (ë°°ì¹˜ í¬ê¸°: {BATCH_SIZE})")
        start_time = time.time()
        
        results = analyzer.process_batch_enhanced(texts, batch_size=BATCH_SIZE)
        
        end_time = time.time()
        processing_time = end_time - start_time
        
        print(f"â±ï¸  ì´ ì²˜ë¦¬ ì‹œê°„: {processing_time:.1f}ì´ˆ")
        print(f"âš¡ í‰ê·  ì²˜ë¦¬ ì†ë„: {len(texts)/processing_time:.1f}ê°œ/ì´ˆ")
        
        # 5. ê²°ê³¼ë¥¼ DataFrameì— ì¶”ê°€
        print(f"\nğŸ“Š ê²°ê³¼ ë°ì´í„° êµ¬ì„± ì¤‘...")
        
        # ìƒˆë¡œìš´ ì»¬ëŸ¼ë“¤ ì •ì˜
        new_columns = {
            'í˜‘ì—… í›„ê¸°_ì •ì œí…ìŠ¤íŠ¸_ê³ ë„í™”': [r['refined_text'] for r in results],
            'í˜‘ì—… í›„ê¸°_ë¹„ì‹ë³„ì²˜ë¦¬ì—¬ë¶€_ê³ ë„í™”': [r['is_anonymized'] for r in results],
            'í˜‘ì—… í›„ê¸°_ì£¼ìš”ê°ì •': [r['primary_emotion'] for r in results],
            'í˜‘ì—… í›„ê¸°_ë³´ì¡°ê°ì •': [r['secondary_emotion'] for r in results],
            'í˜‘ì—… í›„ê¸°_ê°ì •ë³µí•©ì„±': [r['emotional_complexity'] for r in results],
            'í˜‘ì—… í›„ê¸°_ê°ì •ë¹„ìœ¨': [str(r['emotion_mix']) for r in results],
            'í˜‘ì—… í›„ê¸°_ê°ì •ê°•ë„_ê³ ë„í™”': [r['sentiment_intensity'] for r in results],
            'í˜‘ì—… í›„ê¸°_AIì‹ ë¢°ë„_ê³ ë„í™”': [r['confidence_score'] for r in results],
            'í˜‘ì—… í›„ê¸°_ì˜ë£Œë§¥ë½': [r['medical_context'] for r in results],
            'í˜‘ì—… í›„ê¸°_ê°ì •ì›ì¸': [r['root_cause'] for r in results],
            'í˜‘ì—… í›„ê¸°_ê°œì„ ë°©ì•ˆ': [r['improvement_suggestion'] for r in results],
            'í˜‘ì—… í›„ê¸°_í•µì‹¬í‚¤ì›Œë“œ_ê³ ë„í™”': [', '.join(r['key_terms']) for r in results],
            'í˜‘ì—… í›„ê¸°_ë¶„ë¥˜ë¼ë²¨_ê³ ë„í™”': [', '.join(r['labels']) for r in results],
            'í˜‘ì—… í›„ê¸°_í’ˆì§ˆì ìˆ˜_ê³ ë„í™”': [r['quality_score'] for r in results],
            'í˜‘ì—… í›„ê¸°_ì¬ê²€í† í•„ìš”_ê³ ë„í™”': [r['needs_review'] for r in results],
            'í˜‘ì—… í›„ê¸°_í’ˆì§ˆë¬¸ì œ_ê³ ë„í™”': [r['quality_issues'] for r in results]
        }
        
        # DataFrameì— ìƒˆ ì»¬ëŸ¼ë“¤ ì¶”ê°€
        for col_name, col_data in new_columns.items():
            df[col_name] = col_data
        
        # 6. ê²°ê³¼ ì €ì¥
        print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘: {OUTPUT_FILE}")
        df.to_excel(OUTPUT_FILE, index=False)
        print(f"âœ… ì €ì¥ ì™„ë£Œ!")
        
        # 7. í†µê³„ ì¶œë ¥
        analyzer.print_enhanced_stats()
        
        print(f"\nğŸ¯ ê³ ë„í™”ëœ ë¶„ì„ ì™„ë£Œ!")
        print(f"ğŸ“ ê²°ê³¼ íŒŒì¼: {OUTPUT_FILE}")
        print(f"ğŸ“Š ìƒˆë¡œ ì¶”ê°€ëœ ì»¬ëŸ¼: {len(new_columns)}ê°œ")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise

if __name__ == "__main__":
    main_enhanced()