import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

def improved_clustering_analysis():
    """ê°œì„ ëœ í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„ (ê°ì •-ê·¸ë£¹ëª… ì¼ì¹˜ì„± ê°œì„ )"""
    
    print("ğŸ”§ ê°œì„ ëœ í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„ ì‹œì‘")
    print("=" * 50)
    
    try:
        # 1. ì›ë³¸ ë¶„ì„ ê²°ê³¼ ì½ê¸°
        print("ğŸ“ ì›ë³¸ ë¶„ì„ íŒŒì¼ ë¡œë“œ...")
        df = pd.read_excel('í˜‘ì—…í›„ê¸°_ë¶„ì„ê²°ê³¼_ìƒìœ„200ê±´.xlsx', engine='openpyxl')
        
        # 2. ê°ì •ë³„ë¡œ ë¨¼ì € ë¶„ë¦¬í•˜ì—¬ í´ëŸ¬ìŠ¤í„°ë§
        print("ğŸ“Š ê°ì •ë³„ ë¶„ë¦¬ í´ëŸ¬ìŠ¤í„°ë§ ì¤€ë¹„...")
        valid_mask = df['í˜‘ì—…í›„ê¸°_ì •ì œí…ìŠ¤íŠ¸'].notna() & (df['í˜‘ì—…í›„ê¸°_ì •ì œí…ìŠ¤íŠ¸'].str.len() > 3)
        valid_df = df[valid_mask].copy()
        
        # 3. ê°ì •ë³„ ë°ì´í„° ë¶„ë¦¬
        positive_df = valid_df[valid_df['í˜‘ì—…í›„ê¸°_ì£¼ê°ì •'] == 'ê¸ì •'].copy()
        negative_df = valid_df[valid_df['í˜‘ì—…í›„ê¸°_ì£¼ê°ì •'] == 'ë¶€ì •'].copy()
        neutral_df = valid_df[valid_df['í˜‘ì—…í›„ê¸°_ì£¼ê°ì •'] == 'ì¤‘ë¦½'].copy()
        
        print(f"  ê¸ì •: {len(positive_df)}ê±´")
        print(f"  ë¶€ì •: {len(negative_df)}ê±´") 
        print(f"  ì¤‘ë¦½: {len(neutral_df)}ê±´")
        
        # 4. ê°œì„ ëœ í´ëŸ¬ìŠ¤í„° í• ë‹¹ (ê°ì • ê¸°ë°˜)
        df['í˜‘ì—…í›„ê¸°_í´ëŸ¬ìŠ¤í„°'] = np.nan
        df['í˜‘ì—…í›„ê¸°_í´ëŸ¬ìŠ¤í„°ê·¸ë£¹'] = np.nan
        
        cluster_counter = 0
        cluster_details = []
        
        # ê¸ì • ê°ì • í´ëŸ¬ìŠ¤í„°ë§
        if len(positive_df) > 5:
            pos_texts = positive_df['í˜‘ì—…í›„ê¸°_ì •ì œí…ìŠ¤íŠ¸'].tolist()
            pos_clusters, pos_details = cluster_by_content(pos_texts, "ê¸ì •", cluster_counter)
            
            for i, idx in enumerate(positive_df.index):
                df.loc[idx, 'í˜‘ì—…í›„ê¸°_í´ëŸ¬ìŠ¤í„°'] = pos_clusters[i] + cluster_counter
            
            cluster_details.extend(pos_details)
            cluster_counter += len(pos_details)
        
        # ë¶€ì • ê°ì • í´ëŸ¬ìŠ¤í„°ë§
        if len(negative_df) > 5:
            neg_texts = negative_df['í˜‘ì—…í›„ê¸°_ì •ì œí…ìŠ¤íŠ¸'].tolist()
            neg_clusters, neg_details = cluster_by_content(neg_texts, "ë¶€ì •", cluster_counter)
            
            for i, idx in enumerate(negative_df.index):
                df.loc[idx, 'í˜‘ì—…í›„ê¸°_í´ëŸ¬ìŠ¤í„°'] = neg_clusters[i] + cluster_counter
                
            cluster_details.extend(neg_details)
            cluster_counter += len(neg_details)
        
        # ì¤‘ë¦½ ê°ì • í´ëŸ¬ìŠ¤í„°ë§
        if len(neutral_df) > 3:
            neu_texts = neutral_df['í˜‘ì—…í›„ê¸°_ì •ì œí…ìŠ¤íŠ¸'].tolist() 
            neu_clusters, neu_details = cluster_by_content(neu_texts, "ì¤‘ë¦½", cluster_counter)
            
            for i, idx in enumerate(neutral_df.index):
                df.loc[idx, 'í˜‘ì—…í›„ê¸°_í´ëŸ¬ìŠ¤í„°'] = neu_clusters[i] + cluster_counter
                
            cluster_details.extend(neu_details)
        
        # 5. ê·¸ë£¹ëª… ë§¤í•‘ (ê°ì • ê¸°ë°˜)
        cluster_name_mapping = {}
        for detail in cluster_details:
            cluster_name_mapping[detail['cluster_id']] = detail['group_name']
        
        df['í˜‘ì—…í›„ê¸°_í´ëŸ¬ìŠ¤í„°ê·¸ë£¹'] = df['í˜‘ì—…í›„ê¸°_í´ëŸ¬ìŠ¤í„°'].map(cluster_name_mapping)
        
        # 6. í´ëŸ¬ìŠ¤í„°ë³„ ìƒì„¸ ë¶„ì„
        print("ğŸ“ í´ëŸ¬ìŠ¤í„°ë³„ íŠ¹ì„± ë¶„ì„...")
        cluster_summary = []
        
        for detail in cluster_details:
            cluster_mask = (df['í˜‘ì—…í›„ê¸°_í´ëŸ¬ìŠ¤í„°'] == detail['cluster_id'])
            cluster_data = df[cluster_mask]
            
            if len(cluster_data) == 0:
                continue
                
            # ê°ì • ë¶„í¬
            sentiments = cluster_data['í˜‘ì—…í›„ê¸°_ì£¼ê°ì •'].value_counts()
            sentiment_dict = {sentiment: int(count) for sentiment, count in sentiments.items()}
            sentiment_str = ', '.join([f"{k}:{v}ê±´" for k, v in sentiment_dict.items()])
            
            # ì„¸ë¶€ê°ì • ë¶„í¬
            detailed_sentiments = cluster_data['í˜‘ì—…í›„ê¸°_ì„¸ë¶€ê°ì •'].value_counts()
            main_detailed = detailed_sentiments.index[0] if len(detailed_sentiments) > 0 else "ê¸°íƒ€"
            
            # í‰ê·  ê°ì • ê°•ë„
            avg_intensity = cluster_data['í˜‘ì—…í›„ê¸°_ê°ì •ê°•ë„'].mean() if len(cluster_data) > 0 else 5.0
            
            # ëŒ€í‘œ ì˜ˆì‹œ
            representative_text = ""
            if len(cluster_data) > 0:
                cluster_texts = cluster_data['í˜‘ì—…í›„ê¸°_ì •ì œí…ìŠ¤íŠ¸'].tolist()
                representative_text = cluster_texts[0]
            
            cluster_summary.append({
                'í´ëŸ¬ìŠ¤í„°ê·¸ë£¹': detail['group_name'],
                'í”¼ë“œë°±ìˆ˜': len(cluster_data),
                'ë¹„ìœ¨': f"{len(cluster_data)/len(df)*100:.1f}%",
                'ì£¼ìš”í‚¤ì›Œë“œ': detail['keywords'],
                'ì „ì²´í‚¤ì›Œë“œ': detail['keywords'],
                'ì£¼ê°ì •': detail['sentiment_type'],
                'ì„¸ë¶€ê°ì •': main_detailed,
                'í‰ê· ê°ì •ê°•ë„': f"{avg_intensity:.1f}",
                'ê°ì •ë¶„í¬': sentiment_str,
                'ëŒ€í‘œì˜ˆì‹œ': representative_text[:80] + "..." if len(representative_text) > 80 else representative_text
            })
        
        # 7. ë¶€ë¬¸/ë¶€ì„œ/Unitë³„ ì´ìŠˆ ë¶„ì„ ì¶”ê°€
        print("ğŸ¢ ë¶€ë¬¸/ë¶€ì„œ/Unitë³„ ì´ìŠˆ ë¶„ì„...")
        department_analysis = analyze_department_issues(df)
        
        # 8. ìœ ì‚¬ë„ ë¶„ì„ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
        print("ğŸ“ ìœ ì‚¬ë„ ë†’ì€ í”¼ë“œë°± ìŒ ë¶„ì„...")
        similarity_pairs = analyze_similarity_pairs(valid_df)
        
        # 9. ê°œì„ ëœ ê²°ê³¼ë¥¼ Excel íŒŒì¼ë¡œ ì €ì¥
        print("ğŸ’¾ ê°œì„ ëœ í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ì €ì¥...")
        output_file = 'í˜‘ì—…í›„ê¸°_ë¶„ì„ê²°ê³¼_ê°œì„ ëœí´ëŸ¬ìŠ¤í„°ë§_ìƒìœ„200ê±´.xlsx'
        
        with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
            # ë©”ì¸ ì‹œíŠ¸
            df.to_excel(writer, sheet_name='ë¶„ì„ê²°ê³¼_ê°œì„ ëœí´ëŸ¬ìŠ¤í„°', index=False)
            
            # í´ëŸ¬ìŠ¤í„° ìš”ì•½ ì‹œíŠ¸
            pd.DataFrame(cluster_summary).to_excel(writer, sheet_name='ê°œì„ ëœí´ëŸ¬ìŠ¤í„°ë³„ìš”ì•½', index=False)
            
            # ìœ ì‚¬ í”¼ë“œë°± ìŒ ì‹œíŠ¸
            if similarity_pairs:
                pd.DataFrame(similarity_pairs).to_excel(writer, sheet_name='ìœ ì‚¬í”¼ë“œë°±ìŒ', index=False)
            
            # ë¶€ë¬¸ë³„ ì´ìŠˆ ë¶„ì„
            pd.DataFrame(department_analysis['ë¶€ë¬¸ë³„']).to_excel(writer, sheet_name='ë¶€ë¬¸ë³„ì´ìŠˆë¶„ì„', index=False)
            pd.DataFrame(department_analysis['ë¶€ì„œë³„']).to_excel(writer, sheet_name='ë¶€ì„œë³„ì´ìŠˆë¶„ì„', index=False)
            pd.DataFrame(department_analysis['Unitë³„']).to_excel(writer, sheet_name='Unitë³„ì´ìŠˆë¶„ì„', index=False)
        
        print(f"âœ… ê°œì„  ì™„ë£Œ: {output_file}")
        
        # 10. ê°œì„ ì‚¬í•­ ìš”ì•½
        print(f"\nğŸ‰ ê°œì„  ì‚¬í•­:")
        print(f"1. âœ… ê°ì •ë³„ ë¶„ë¦¬ í´ëŸ¬ìŠ¤í„°ë§ìœ¼ë¡œ ë…¼ë¦¬ì  ì¼ê´€ì„± í™•ë³´")
        print(f"2. âœ… ê·¸ë£¹ëª…ê³¼ ê°ì •ì˜ ì¼ì¹˜ì„± ê°œì„ ")
        print(f"3. âœ… ë¶€ë¬¸/ë¶€ì„œ/Unitë³„ ì£¼ìš” ì´ìŠˆ ë° ê°œì„ ì‚¬í•­ ë¶„ì„ ì¶”ê°€")
        print(f"4. âœ… ì‹œê¸‰ì„±ë³„ ìš°ì„ ìˆœìœ„ ë„ì¶œ")
        
        return output_file, cluster_summary, department_analysis
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return None, None, None

def cluster_by_content(texts, sentiment_type, start_cluster_id):
    """í…ìŠ¤íŠ¸ ë‚´ìš© ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§"""
    
    if len(texts) < 3:
        # ë°ì´í„°ê°€ ì ìœ¼ë©´ í•˜ë‚˜ì˜ í´ëŸ¬ìŠ¤í„°ë¡œ
        return [0], [{
            'cluster_id': start_cluster_id,
            'group_name': f"{sentiment_type}_ê¸°íƒ€ê·¸ë£¹",
            'sentiment_type': sentiment_type,
            'keywords': "ì¼ë°˜ í‚¤ì›Œë“œ"
        }]
    
    # TF-IDF ë²¡í„°í™”
    stop_words = ['ì—†ìŠµë‹ˆë‹¤', 'ê°ì‚¬í•©ë‹ˆë‹¤', 'ë§Œì¡±', 'í•­ìƒ', 'ë§¤ìš°', 'ì •ë§', 'ë„ˆë¬´', 'ì•„ì£¼', 
                 'ì—†ìŒ', 'í•´ë‹¹ì—†ìŒ', 'ë¬´', '...', '....', '.', 'x000d',
                 # ì˜ë£Œìš©ì–´ ì œì™¸
                 'ê°„í˜¸ì‚¬', 'ì˜ì‚¬', 'ì•½ì‚¬', 'ê²€ì‚¬ì‹¤', 'ë³‘ë™', 'ì™¸ë˜', 'ìˆ˜ìˆ ì‹¤', 'ì‘ê¸‰ì‹¤']
    
    vectorizer = TfidfVectorizer(
        max_features=50, min_df=1, max_df=0.8, 
        ngram_range=(1, 2), stop_words=stop_words
    )
    
    try:
        tfidf_matrix = vectorizer.fit_transform(texts)
        feature_names = vectorizer.get_feature_names_out()
        
        # í´ëŸ¬ìŠ¤í„° ìˆ˜ ê²°ì • (ë°ì´í„° ì–‘ì— ë”°ë¼)
        n_clusters = min(3, max(1, len(texts) // 10))
        
        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        clusters = kmeans.fit_predict(tfidf_matrix)
        
        # í´ëŸ¬ìŠ¤í„°ë³„ íŠ¹ì„± ë¶„ì„
        cluster_details = []
        for i in range(n_clusters):
            # ëŒ€í‘œ í‚¤ì›Œë“œ ì¶”ì¶œ
            cluster_center = kmeans.cluster_centers_[i]
            top_indices = cluster_center.argsort()[-3:][::-1]
            keywords = ' | '.join([feature_names[idx] for idx in top_indices])
            
            # ê°ì •ë³„ ê·¸ë£¹ëª… ìƒì„±
            group_name = generate_group_name(sentiment_type, keywords, i)
            
            cluster_details.append({
                'cluster_id': start_cluster_id + i,
                'group_name': group_name,
                'sentiment_type': sentiment_type,
                'keywords': keywords
            })
        
        return clusters, cluster_details
        
    except Exception as e:
        print(f"í´ëŸ¬ìŠ¤í„°ë§ ì˜¤ë¥˜: {e}")
        # ì‹¤íŒ¨ ì‹œ ë‹¨ì¼ í´ëŸ¬ìŠ¤í„°
        return [0] * len(texts), [{
            'cluster_id': start_cluster_id,
            'group_name': f"{sentiment_type}_ì¼ë°˜ê·¸ë£¹",
            'sentiment_type': sentiment_type,
            'keywords': "ì¼ë°˜ í‚¤ì›Œë“œ"
        }]

def generate_group_name(sentiment_type, keywords, cluster_idx):
    """ê°ì •ê³¼ í‚¤ì›Œë“œ ê¸°ë°˜ ê·¸ë£¹ëª… ìƒì„±"""
    
    # í‚¤ì›Œë“œ ê¸°ë°˜ ì¹´í…Œê³ ë¦¬ íŒë‹¨
    keywords_lower = keywords.lower()
    
    if sentiment_type == "ê¸ì •":
        if any(word in keywords_lower for word in ['ê°ì‚¬', 'ê³ ë§ˆ', 'ì¹œì ˆ']):
            return "ê¸ì •_ê°ì‚¬í‘œí˜„ê·¸ë£¹"
        elif any(word in keywords_lower for word in ['ë§Œì¡±', 'ì¢‹', 'í›Œë¥­']):
            return "ê¸ì •_ë§Œì¡±í‰ê°€ê·¸ë£¹"
        elif any(word in keywords_lower for word in ['ë„ì›€', 'ì§€ì›', 'í˜‘ì¡°']):
            return "ê¸ì •_í˜‘ì—…ì§€ì›ê·¸ë£¹"
        else:
            return f"ê¸ì •_ì¼ë°˜ì¹­ì°¬ê·¸ë£¹_{cluster_idx+1}"
    
    elif sentiment_type == "ë¶€ì •":
        if any(word in keywords_lower for word in ['ë¶ˆë§Œ', 'ì‹¤ë§', 'í™”']):
            return "ë¶€ì •_ë¶ˆë§Œí‘œì¶œê·¸ë£¹"
        elif any(word in keywords_lower for word in ['ê°œì„ ', 'ë¬¸ì œ', 'ë¶€ì¡±']):
            return "ë¶€ì •_ê°œì„ ìš”êµ¬ê·¸ë£¹"
        elif any(word in keywords_lower for word in ['ì†Œí†µ', 'íƒœë„', 'ì‘ëŒ€']):
            return "ë¶€ì •_ì†Œí†µë¬¸ì œê·¸ë£¹"
        else:
            return f"ë¶€ì •_ì¼ë°˜ë¶ˆë§Œê·¸ë£¹_{cluster_idx+1}"
    
    else:  # ì¤‘ë¦½
        if any(word in keywords_lower for word in ['ì œì•ˆ', 'ì˜ê²¬']):
            return "ì¤‘ë¦½_ê°œì„ ì œì•ˆê·¸ë£¹"
        elif any(word in keywords_lower for word in ['ì •ë³´', 'í™•ì¸']):
            return "ì¤‘ë¦½_ì •ë³´ë¬¸ì˜ê·¸ë£¹"
        else:
            return f"ì¤‘ë¦½_ì¼ë°˜ì˜ê²¬ê·¸ë£¹_{cluster_idx+1}"

def analyze_department_issues(df):
    """ë¶€ë¬¸/ë¶€ì„œ/Unitë³„ ì´ìŠˆ ë¶„ì„"""
    
    department_analysis = {
        'ë¶€ë¬¸ë³„': [],
        'ë¶€ì„œë³„': [],
        'Unitë³„': []
    }
    
    # ë¶€ë¬¸ë³„ ë¶„ì„
    for ë¶€ë¬¸ in df['í‰ê°€_ë¶€ë¬¸'].unique():
        if pd.notna(ë¶€ë¬¸):
            ë¶€ë¬¸_data = df[df['í‰ê°€_ë¶€ë¬¸'] == ë¶€ë¬¸]
            
            # ì£¼ìš” ì´ìŠˆ ì¶”ì¶œ (ë¶€ì •ì  í”¼ë“œë°± ìœ„ì£¼)
            ë¶€ì •_data = ë¶€ë¬¸_data[ë¶€ë¬¸_data['í˜‘ì—…í›„ê¸°_ì£¼ê°ì •'] == 'ë¶€ì •']
            ì£¼ìš”ì´ìŠˆ = extract_main_issues(ë¶€ì •_data)
            
            # ê°ì • ë¶„í¬
            ê°ì •_ë¶„í¬ = ë¶€ë¬¸_data['í˜‘ì—…í›„ê¸°_ì£¼ê°ì •'].value_counts()
            ê°ì •_ë¶„í¬_str = ', '.join([f"{k}:{v}ê±´" for k, v in ê°ì •_ë¶„í¬.items()])
            
            # ìš°ì„ ìˆœìœ„ íŒë‹¨
            ìš°ì„ ìˆœìœ„ = determine_priority(ë¶€ë¬¸_data)
            
            department_analysis['ë¶€ë¬¸ë³„'].append({
                'ë¶€ë¬¸': ë¶€ë¬¸,
                'ì´í”¼ë“œë°±ìˆ˜': len(ë¶€ë¬¸_data),
                'ê°ì •ë¶„í¬': ê°ì •_ë¶„í¬_str,
                'ë¶€ì •ë¹„ìœ¨': f"{len(ë¶€ì •_data)/len(ë¶€ë¬¸_data)*100:.1f}%",
                'ì£¼ìš”ì´ìŠˆ': ì£¼ìš”ì´ìŠˆ,
                'ê°œì„ ìš°ì„ ìˆœìœ„': ìš°ì„ ìˆœìœ„,
                'ê¶Œì¥ì¡°ì¹˜': get_recommended_actions(ì£¼ìš”ì´ìŠˆ, ìš°ì„ ìˆœìœ„)
            })
    
    # ë¶€ì„œë³„ ë¶„ì„ (ìƒìœ„ 10ê°œ)
    for ë¶€ì„œ in df['í”¼í‰ê°€ëŒ€ìƒ ë¶€ì„œëª…'].value_counts().head(10).index:
        ë¶€ì„œ_data = df[df['í”¼í‰ê°€ëŒ€ìƒ ë¶€ì„œëª…'] == ë¶€ì„œ]
        ë¶€ì •_data = ë¶€ì„œ_data[ë¶€ì„œ_data['í˜‘ì—…í›„ê¸°_ì£¼ê°ì •'] == 'ë¶€ì •']
        ì£¼ìš”ì´ìŠˆ = extract_main_issues(ë¶€ì •_data)
        
        ê°ì •_ë¶„í¬ = ë¶€ì„œ_data['í˜‘ì—…í›„ê¸°_ì£¼ê°ì •'].value_counts()
        ê°ì •_ë¶„í¬_str = ', '.join([f"{k}:{v}ê±´" for k, v in ê°ì •_ë¶„í¬.items()])
        
        ìš°ì„ ìˆœìœ„ = determine_priority(ë¶€ì„œ_data)
        
        department_analysis['ë¶€ì„œë³„'].append({
            'í”¼í‰ê°€ë¶€ì„œ': ë¶€ì„œ,
            'ì´í”¼ë“œë°±ìˆ˜': len(ë¶€ì„œ_data),
            'ê°ì •ë¶„í¬': ê°ì •_ë¶„í¬_str,
            'ë¶€ì •ë¹„ìœ¨': f"{len(ë¶€ì •_data)/len(ë¶€ì„œ_data)*100:.1f}%",
            'ì£¼ìš”ì´ìŠˆ': ì£¼ìš”ì´ìŠˆ,
            'ê°œì„ ìš°ì„ ìˆœìœ„': ìš°ì„ ìˆœìœ„,
            'ê¶Œì¥ì¡°ì¹˜': get_recommended_actions(ì£¼ìš”ì´ìŠˆ, ìš°ì„ ìˆœìœ„)
        })
    
    # Unitë³„ ë¶„ì„
    if 'Unit' in df.columns:
        for unit in df['Unit'].value_counts().head(5).index:
            if pd.notna(unit):
                unit_data = df[df['Unit'] == unit]
                ë¶€ì •_data = unit_data[unit_data['í˜‘ì—…í›„ê¸°_ì£¼ê°ì •'] == 'ë¶€ì •']
                ì£¼ìš”ì´ìŠˆ = extract_main_issues(ë¶€ì •_data)
                
                ê°ì •_ë¶„í¬ = unit_data['í˜‘ì—…í›„ê¸°_ì£¼ê°ì •'].value_counts()
                ê°ì •_ë¶„í¬_str = ', '.join([f"{k}:{v}ê±´" for k, v in ê°ì •_ë¶„í¬.items()])
                
                ìš°ì„ ìˆœìœ„ = determine_priority(unit_data)
                
                department_analysis['Unitë³„'].append({
                    'Unit': unit,
                    'ì´í”¼ë“œë°±ìˆ˜': len(unit_data),
                    'ê°ì •ë¶„í¬': ê°ì •_ë¶„í¬_str,
                    'ë¶€ì •ë¹„ìœ¨': f"{len(ë¶€ì •_data)/len(unit_data)*100:.1f}%",
                    'ì£¼ìš”ì´ìŠˆ': ì£¼ìš”ì´ìŠˆ,
                    'ê°œì„ ìš°ì„ ìˆœìœ„': ìš°ì„ ìˆœìœ„,
                    'ê¶Œì¥ì¡°ì¹˜': get_recommended_actions(ì£¼ìš”ì´ìŠˆ, ìš°ì„ ìˆœìœ„)
                })
    
    return department_analysis

def extract_main_issues(negative_data):
    """ë¶€ì •ì  í”¼ë“œë°±ì—ì„œ ì£¼ìš” ì´ìŠˆ ì¶”ì¶œ"""
    if len(negative_data) == 0:
        return "íŠ¹ë³„í•œ ì´ìŠˆ ì—†ìŒ"
    
    # í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„
    all_keywords = []
    for keywords in negative_data['í˜‘ì—…í›„ê¸°_í‚¤ì›Œë“œ']:
        if pd.notna(keywords):
            all_keywords.extend([k.strip() for k in str(keywords).split(',')])
    
    if not all_keywords:
        return "í‚¤ì›Œë“œ ë¶€ì¡±ìœ¼ë¡œ ì´ìŠˆ ë¶„ì„ ì–´ë ¤ì›€"
    
    from collections import Counter
    keyword_freq = Counter(all_keywords)
    top_issues = [f"{keyword}({count}íšŒ)" for keyword, count in keyword_freq.most_common(3)]
    
    return ", ".join(top_issues)

def determine_priority(dept_data):
    """ê°œì„  ìš°ì„ ìˆœìœ„ íŒë‹¨"""
    ë¶€ì •_ë¹„ìœ¨ = len(dept_data[dept_data['í˜‘ì—…í›„ê¸°_ì£¼ê°ì •'] == 'ë¶€ì •']) / len(dept_data)
    í”¼ë“œë°±_ìˆ˜ = len(dept_data)
    í‰ê· ê°•ë„ = dept_data['í˜‘ì—…í›„ê¸°_ê°ì •ê°•ë„'].mean()
    
    if ë¶€ì •_ë¹„ìœ¨ > 0.6 and í”¼ë“œë°±_ìˆ˜ > 10:
        return "ì‹œê¸‰"
    elif ë¶€ì •_ë¹„ìœ¨ > 0.4 or í”¼ë“œë°±_ìˆ˜ > 15:
        return "ì¤‘ìš”"
    elif ë¶€ì •_ë¹„ìœ¨ > 0.2:
        return "ë³´í†µ"
    else:
        return "ë‚®ìŒ"

def get_recommended_actions(issues, priority):
    """ê¶Œì¥ ì¡°ì¹˜ì‚¬í•­ ì œì•ˆ"""
    if priority == "ì‹œê¸‰":
        return "ì¦‰ì‹œ ê°œì„ íŒ€ êµ¬ì„±, 1ê°œì›” ë‚´ ê°œì„ ê³„íš ìˆ˜ë¦½"
    elif priority == "ì¤‘ìš”":
        return "3ê°œì›” ë‚´ ê°œì„ ê³„íš ìˆ˜ë¦½, ì •ê¸° ëª¨ë‹ˆí„°ë§"
    elif priority == "ë³´í†µ":
        return "6ê°œì›” ë‚´ ì ì§„ì  ê°œì„ , ë¶„ê¸°ë³„ ì ê²€"
    else:
        return "í˜„ ìƒíƒœ ìœ ì§€, ë°˜ê¸°ë³„ ëª¨ë‹ˆí„°ë§"

def analyze_similarity_pairs(valid_df):
    """ìœ ì‚¬ë„ ë¶„ì„ (ê¸°ì¡´ ë¡œì§)"""
    similarity_pairs = []
    
    try:
        texts = valid_df['í˜‘ì—…í›„ê¸°_ì •ì œí…ìŠ¤íŠ¸'].tolist()
        
        stop_words = ['ì—†ìŠµë‹ˆë‹¤', 'ê°ì‚¬í•©ë‹ˆë‹¤', 'ë§Œì¡±', 'í•­ìƒ', 'ë§¤ìš°', 'ì •ë§', 'ë„ˆë¬´', 'ì•„ì£¼']
        vectorizer = TfidfVectorizer(max_features=50, stop_words=stop_words)
        
        sample_size = min(50, len(texts))
        sample_texts = texts[:sample_size]
        sample_matrix = vectorizer.fit_transform(sample_texts)
        similarity_matrix = cosine_similarity(sample_matrix)
        
        for i in range(sample_size):
            for j in range(i+1, sample_size):
                similarity = similarity_matrix[i][j]
                if similarity > 0.3:
                    similarity_pairs.append({
                        'í…ìŠ¤íŠ¸1_ë²ˆí˜¸': i+1,
                        'í…ìŠ¤íŠ¸2_ë²ˆí˜¸': j+1,
                        'ìœ ì‚¬ë„': f"{similarity:.3f}",
                        'í…ìŠ¤íŠ¸1': sample_texts[i][:60] + "...",
                        'í…ìŠ¤íŠ¸2': sample_texts[j][:60] + "...",
                        'ê°™ì€ê°ì •ì—¬ë¶€': 'ì˜ˆ' if valid_df.iloc[i]['í˜‘ì—…í›„ê¸°_ì£¼ê°ì •'] == valid_df.iloc[j]['í˜‘ì—…í›„ê¸°_ì£¼ê°ì •'] else 'ì•„ë‹ˆì˜¤'
                    })
    
    except Exception as e:
        print(f"ìœ ì‚¬ë„ ë¶„ì„ ì˜¤ë¥˜: {e}")
    
    return similarity_pairs

if __name__ == "__main__":
    result_file, summary, dept_analysis = improved_clustering_analysis()
    
    if result_file:
        print(f"\n{'='*60}")
        print("ğŸ“‹ ê°œì„ ëœ ìµœì¢… íŒŒì¼ êµ¬ì„±:")
        print("â€¢ ë¶„ì„ê²°ê³¼_ê°œì„ ëœí´ëŸ¬ìŠ¤í„°: ëª¨ë“  ë¶„ì„ ê²°ê³¼ + ê°œì„ ëœ í´ëŸ¬ìŠ¤í„°")
        print("â€¢ ê°œì„ ëœí´ëŸ¬ìŠ¤í„°ë³„ìš”ì•½: ê°ì •-ê·¸ë£¹ëª… ì¼ì¹˜ì„± ê°œì„ ëœ ë¶„ì„")
        print("â€¢ ìœ ì‚¬í”¼ë“œë°±ìŒ: ì¤‘ë³µ ì´ìŠˆ í›„ë³´ë“¤")
        print("â€¢ ë¶€ë¬¸ë³„ì´ìŠˆë¶„ì„: ë¶€ë¬¸ë³„ ì£¼ìš” ì´ìŠˆ ë° ê°œì„ ì‚¬í•­")
        print("â€¢ ë¶€ì„œë³„ì´ìŠˆë¶„ì„: ë¶€ì„œë³„ ìƒì„¸ ì´ìŠˆ ë¶„ì„")
        print("â€¢ Unitë³„ì´ìŠˆë¶„ì„: Unitë³„ ì„¸ë¶€ ë¶„ì„")