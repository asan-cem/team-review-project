import pandas as pd
import json
import time
import re
from pathlib import Path
import vertexai
from vertexai.generative_models import GenerativeModel
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity
from collections import Counter, defaultdict
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
import warnings
warnings.filterwarnings('ignore')

# ê³ ë„í™”ëœ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
ADVANCED_PROMPT_TEMPLATE = """
[í˜ë¥´ì†Œë‚˜]
ë‹¹ì‹ ì€ ì˜ë£Œê¸°ê´€ ë‚´ë¶€ ì§ì› ë§Œì¡±ë„ ë° í˜‘ì—… í”¼ë“œë°±ì„ ë¶„ì„í•˜ëŠ” ê³ ê¸‰ AI ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì˜ë£Œ ìš©ì–´, ì—…ë¬´ ìš©ì–´, ì•½ì–´ì— ëŒ€í•œ ê¹Šì€ ì´í•´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•œ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

[ì§€ì‹œì‚¬í•­]
1. ì£¼ì–´ì§„ ì›ë³¸ í…ìŠ¤íŠ¸ì˜ í•µì‹¬ ì˜ë¯¸ë¥¼ ë³´ì¡´í•˜ë©´ì„œ, ì˜¤íƒ€ì™€ ë¬¸ë²•ì„ êµì •í•˜ì—¬ refined_textë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
2. ì†ê±°ë‚˜ ê³µê²©ì ì¸ í‘œí˜„ì€ ì „ë¬¸ì ì´ê³  ì •ì¤‘í•œ í‘œí˜„ìœ¼ë¡œ ìˆœí™”í•©ë‹ˆë‹¤.
3. **ë¹„ì‹ë³„ ì²˜ë¦¬ ê·œì¹™** (ë¶€ì •ì  í”¼ë“œë°±ì´ë©´ì„œ ì•„ë˜ ì¡°ê±´ì— í•´ë‹¹í•  ê²½ìš°ì—ë§Œ):
   - ì‹¤ëª…ì´ ëª…ì‹œëœ ê²½ìš° (ì˜ˆ: "ê¹€ë¯¼í¬ ì§ì›", "í˜ˆì•¡ì€í–‰ ê¹€í˜„ì„± ì§ì›")
   - ì†Œìˆ˜ ì¸ì›ìœ¼ë¡œ íŠ¹ì • ê°€ëŠ¥í•œ êµ¬ì²´ì  í˜¸ì¹­ (ì˜ˆ: "íŒ€ì¥", "ê³¼ì¥", "ëŒ€ë¦¬", "ì—¬ì ì§ì›")
   - ë¶€ì„œëª…+ì§ì±…ì´ ê²°í•©ëœ ê²½ìš° (ì˜ˆ: "í˜ˆì•¡ì€í–‰ ê¹€í˜„ì„±", "ë§ˆì¼€íŒ…íŒ€ ê³¼ì¥")
   **ì ˆëŒ€ ê·œì¹™**: ê¸ì •ì ì´ê±°ë‚˜ ì¤‘ë¦½ì  í”¼ë“œë°±ì€ ì–´ë–¤ ê²½ìš°ì—ë„ is_anonymizedë¥¼ falseë¡œ ì„¤ì •
4. "ì—†ìŒ" ë“± ë¬´ì˜ë¯¸í•œ í…ìŠ¤íŠ¸ëŠ” refined_textë¥¼ ë¹ˆ ë¬¸ìì—´ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.

[ê°ì • ë¶„ì„ - 2ë‹¨ê³„ ë¶„ë¥˜]
**1ë‹¨ê³„ (ì£¼ê°ì •)**: "ê¸ì •", "ë¶€ì •", "ì¤‘ë¦½" ì¤‘ í•˜ë‚˜
**2ë‹¨ê³„ (ì„¸ë¶€ê°ì •)**: ì£¼ê°ì •ì— ë”°ë¥¸ ì„¸ë¶€ ë¶„ë¥˜
- ê¸ì •: "ë§Œì¡±", "ê°ì‚¬", "ì¹­ì°¬", "ê²©ë ¤", "ê¸°ëŒ€"
- ë¶€ì •: "ë¶ˆë§Œ", "ì‹¤ë§", "ë¶„ë…¸", "ìš°ë ¤", "ë¹„íŒ"  
- ì¤‘ë¦½: "ì œì•ˆ", "ì •ë³´", "ì§ˆë¬¸", "ê´€ì°°", "ê¸°íƒ€"

[í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ]
í…ìŠ¤íŠ¸ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ í‚¤ì›Œë“œ 3-5ê°œë¥¼ ì¶”ì¶œí•˜ì„¸ìš”. ì˜ë£Œ ìš©ì–´, ë¶€ì„œëª…, ì—…ë¬´ ê´€ë ¨ ìš©ì–´ë¥¼ ìš°ì„ ì ìœ¼ë¡œ í¬í•¨í•˜ì„¸ìš”.

[ë¶„ë¥˜ ì²´ê³„]
ë‹¤ìŒ ì¤‘ í•´ë‹¹í•˜ëŠ” ëª¨ë“  ë¼ë²¨ì„ í¬í•¨:
- "ë¶€ì„œê°„ í˜‘ì—…": ì„œë¡œ ë‹¤ë¥¸ ë¶€ì„œ/íŒ€ ê°„ì˜ ì—…ë¬´ ì—°ê³„ì™€ í˜‘ë ¥ ë¬¸ì œ
- "ì§ì›ê°„ ì†Œí†µ": ê°™ì€ ë¶€ì„œ/íŒ€ ë‚´ ë™ë£Œ ê°„ì˜ ì†Œí†µ ë° ê´€ê³„ ë¬¸ì œ  
- "ì „ë¬¸ì„± ë¶€ì¡±": ê°œì¸ì˜ ì—…ë¬´ ì§€ì‹, ê¸°ìˆ , ê²½í—˜ ë¶€ì¡± ë¬¸ì œ
- "ì—…ë¬´ íƒœë„": ì±…ì„ê°, ì ê·¹ì„± ë“± ì—…ë¬´ë¥¼ ëŒ€í•˜ëŠ” ìì„¸ ë¬¸ì œ
- "ìƒí˜¸ ì¡´ì¤‘": ì¸ê²©ì  ëŒ€ìš°, ë°°ë ¤ ë“± ê´€ê³„ì—ì„œì˜ ì˜ˆì˜ ë¬¸ì œ
- "ì‹œìŠ¤í…œ/í”„ë¡œì„¸ìŠ¤": ì—…ë¬´ ì‹œìŠ¤í…œ, ì ˆì°¨, í™˜ê²½ ê´€ë ¨ ë¬¸ì œ
- "êµìœ¡/í›ˆë ¨": ì§ì› êµìœ¡, ì—­ëŸ‰ ê°œë°œ ê´€ë ¨ ì‚¬í•­
- "ë¦¬ë”ì‹­": ê´€ë¦¬ì, íŒ€ì¥ ë“±ì˜ ë¦¬ë”ì‹­ ê´€ë ¨ ì‚¬í•­

[ì¶œë ¥ í˜•ì‹]
{{
  "refined_text": "ì •ì œëœ í…ìŠ¤íŠ¸",
  "is_anonymized": false,
  "primary_sentiment": "ê¸ì •/ë¶€ì •/ì¤‘ë¦½",
  "detailed_sentiment": "ì„¸ë¶€ê°ì •",
  "sentiment_intensity": ê°ì •ê°•ë„ì ìˆ˜(1-10),
  "keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", "í‚¤ì›Œë“œ3"],
  "labels": ["ë¼ë²¨1", "ë¼ë²¨2"],
  "medical_terms": ["ì˜ë£Œìš©ì–´1", "ì˜ë£Œìš©ì–´2"] 
}}

ì›ë³¸ í…ìŠ¤íŠ¸: "{original_text}"
"""

class AdvancedReviewAnalyzer:
    def __init__(self, project_id: str, location: str = "us-central1"):
        """ê³ ë„í™”ëœ ë¦¬ë·° ë¶„ì„ê¸° ì´ˆê¸°í™”"""
        vertexai.init(project=project_id, location=location)
        self.model = GenerativeModel("gemini-2.0-flash")
        
        # ì˜ë£Œ/ì—…ë¬´ ìš©ì–´ ì‚¬ì „
        self.medical_terms = {
            'OCS', 'EMR', 'PACS', 'HIS', 'LIS', 'RIS', 'EHR', 'ICU', 'ER', 'OR',
            'ì‘ê¸‰ì‹¤', 'ì¤‘í™˜ìì‹¤', 'ìˆ˜ìˆ ì‹¤', 'ì™¸ë˜', 'ë³‘ë™', 'ê°„í˜¸ì‚¬', 'ì˜ì‚¬', 'ì•½ì‚¬',
            'ê²€ì‚¬ì‹¤', 'ë°©ì‚¬ì„ ê³¼', 'ë³‘ë¦¬ê³¼', 'ì¬í™œì˜í•™ê³¼', 'ì •í˜•ì™¸ê³¼', 'ë‚´ê³¼', 'ì™¸ê³¼',
            'ì‚°ë¶€ì¸ê³¼', 'ì†Œì•„ê³¼', 'ì •ì‹ ê³¼', 'í”¼ë¶€ê³¼', 'ì•ˆê³¼', 'ì´ë¹„ì¸í›„ê³¼', 'ë¹„ë‡¨ê¸°ê³¼',
            'í˜ˆì•¡ì€í–‰', 'ì„ìƒë³‘ë¦¬', 'ì§„ë‹¨ê²€ì‚¬', 'CT', 'MRI', 'X-ray', 'ì´ˆìŒíŒŒ',
            'ì±„í˜ˆ', 'íˆ¬ì•½', 'ì²˜ë°©', 'ì§„ë£Œ', 'ì…ì›', 'í‡´ì›', 'ìˆ˜ìˆ ', 'ë§ˆì·¨'
        }
        
        self.batch_results = []
        
    def preprocess_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        if pd.isna(text) or str(text).strip() == "":
            return ""
            
        text = str(text)
        # íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬ (ì˜ë£Œìš©ì–´ ë³´ì¡´)
        text = re.sub(r'[^\w\sê°€-í£.,!?()/-]', ' ', text)
        # ì—°ì†ëœ ê³µë°± ì œê±°
        text = re.sub(r'\s+', ' ', text).strip()
        return text
    
    def extract_medical_terms(self, text: str) -> list:
        """ì˜ë£Œ/ì—…ë¬´ ìš©ì–´ ì¶”ì¶œ"""
        found_terms = []
        text_upper = text.upper()
        for term in self.medical_terms:
            if term in text_upper or term in text:
                found_terms.append(term)
        return found_terms
    
    def analyze_review(self, original_text: str) -> dict:
        """ë‹¨ì¼ ë¦¬ë·° ë¶„ì„ (ê³ ë„í™”)"""
        if not original_text or str(original_text).strip() == "":
            return {
                "original_text": original_text,
                "refined_text": "",
                "is_anonymized": False,
                "primary_sentiment": "ì¤‘ë¦½",
                "detailed_sentiment": "ê¸°íƒ€",
                "sentiment_intensity": 5,
                "keywords": [],
                "labels": [],
                "medical_terms": []
            }
        
        # ì „ì²˜ë¦¬
        processed_text = self.preprocess_text(original_text)
        medical_terms = self.extract_medical_terms(processed_text)
        
        prompt = ADVANCED_PROMPT_TEMPLATE.format(original_text=processed_text)
        
        try:
            print(f"    ğŸ¤– AI ë¶„ì„ ì¤‘...")
            response = self.model.generate_content(prompt)
            response_text = response.text.strip()
            print(f"    âœ… AI ì‘ë‹µ ë°›ìŒ ({len(response_text)}ì)")
            
            # JSON íŒŒì‹±
            try:
                json_start = response_text.find('{')
                json_end = response_text.rfind('}') + 1
                if json_start != -1 and json_end > json_start:
                    json_text = response_text[json_start:json_end]
                    result = json.loads(json_text)
                    result["original_text"] = original_text
                    result["medical_terms"] = medical_terms
                    
                    # ê²°ê³¼ ìš”ì•½ í‘œì‹œ
                    sentiment = result.get('primary_sentiment', 'ì•Œ ìˆ˜ ì—†ìŒ')
                    detailed = result.get('detailed_sentiment', 'ì•Œ ìˆ˜ ì—†ìŒ')
                    print(f"    ğŸ“ˆ ë¶„ì„ ê²°ê³¼: {sentiment} ({detailed})")
                    
                    return result
                else:
                    raise json.JSONDecodeError("No JSON found", response_text, 0)
                    
            except json.JSONDecodeError:
                print(f"    âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {processed_text[:50]}...")
                return self._create_fallback_result(original_text, processed_text, medical_terms)
                
        except Exception as e:
            print(f"    âŒ API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return self._create_fallback_result(original_text, processed_text, medical_terms)
    
    def _create_fallback_result(self, original_text: str, processed_text: str, medical_terms: list) -> dict:
        """API ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ê²°ê³¼ ìƒì„±"""
        return {
            "original_text": original_text,
            "refined_text": processed_text,
            "is_anonymized": False,
            "primary_sentiment": "ì¤‘ë¦½",
            "detailed_sentiment": "ê¸°íƒ€",
            "sentiment_intensity": 5,
            "keywords": [],
            "labels": [],
            "medical_terms": medical_terms
        }
    
    def process_batch(self, df_batch: pd.DataFrame, column_name: str, batch_num: int, delay: float = 0.1) -> pd.DataFrame:
        """ë°°ì¹˜ ë‹¨ìœ„ ì²˜ë¦¬"""
        print(f"\n=== ë°°ì¹˜ {batch_num} ì²˜ë¦¬ ì‹œì‘ ({len(df_batch)}ê±´) ===")
        start_time = time.time()
        
        results = []
        processed_count = 0
        
        for idx, row in df_batch.iterrows():
            original_text = str(row[column_name]) if pd.notna(row[column_name]) else ""
            
            # ë” ìì£¼ ì§„í–‰ë¥  í‘œì‹œ (10ê±´ë§ˆë‹¤)
            if processed_count % 10 == 0:
                elapsed = time.time() - start_time
                if processed_count > 0:
                    avg_time_per_item = elapsed / processed_count
                    remaining_items = len(df_batch) - processed_count
                    estimated_remaining = avg_time_per_item * remaining_items
                    print(f"  ğŸ“Š ì§„í–‰ë¥ : {processed_count + 1}/{len(df_batch)} ({(processed_count/len(df_batch)*100):.1f}%) | "
                          f"ì†Œìš”ì‹œê°„: {elapsed:.1f}ì´ˆ | ì˜ˆìƒ ì”ì—¬ì‹œê°„: {estimated_remaining:.1f}ì´ˆ")
                else:
                    print(f"  ğŸ“Š ì§„í–‰ë¥ : {processed_count + 1}/{len(df_batch)} (ì‹œì‘)")
            
            # í…ìŠ¤íŠ¸ ì²˜ë¦¬ ìƒíƒœ í‘œì‹œ
            if original_text.strip():
                print(f"  ğŸ”„ ì²˜ë¦¬ ì¤‘ #{processed_count + 1}: {original_text[:50]}...")
            else:
                print(f"  â­ï¸  ë¹ˆ í…ìŠ¤íŠ¸ #{processed_count + 1}: ê±´ë„ˆëœ€")
            
            result = self.analyze_review(original_text)
            if "original_text" in result:
                del result["original_text"]
            results.append(result)
            
            processed_count += 1
            
            if delay > 0:
                time.sleep(delay)
        
        # ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
        result_df = pd.DataFrame(results)
        
        # ì›ë³¸ ë°ì´í„°ì™€ ê²°í•©
        processed_batch = df_batch.copy()
        for col in result_df.columns:
            processed_batch[f"{column_name}_{col}"] = result_df[col]
        
        # ë°°ì¹˜ í†µê³„ ì¶œë ¥
        self._print_batch_stats(result_df, batch_num)
        
        return processed_batch
    
    def _print_batch_stats(self, result_df: pd.DataFrame, batch_num: int):
        """ë°°ì¹˜ ì²˜ë¦¬ í†µê³„ ì¶œë ¥"""
        print(f"\n--- ë°°ì¹˜ {batch_num} ì²˜ë¦¬ ê²°ê³¼ ---")
        
        # ê°ì • ë¶„ì„ ê²°ê³¼
        if 'primary_sentiment' in result_df.columns:
            sentiment_counts = result_df['primary_sentiment'].value_counts()
            print("ì£¼ ê°ì • ë¶„í¬:")
            for sentiment, count in sentiment_counts.items():
                print(f"  {sentiment}: {count}ê±´ ({count/len(result_df)*100:.1f}%)")
        
        # ì„¸ë¶€ ê°ì • ë¶„ì„
        if 'detailed_sentiment' in result_df.columns:
            detailed_counts = result_df['detailed_sentiment'].value_counts().head(5)
            print("ì„¸ë¶€ ê°ì • TOP 5:")
            for sentiment, count in detailed_counts.items():
                print(f"  {sentiment}: {count}ê±´")
        
        # ì£¼ìš” ë¼ë²¨
        if 'labels' in result_df.columns:
            all_labels = []
            for labels in result_df['labels']:
                if isinstance(labels, list):
                    all_labels.extend(labels)
            if all_labels:
                label_counts = Counter(all_labels).most_common(5)
                print("ì£¼ìš” ë¼ë²¨ TOP 5:")
                for label, count in label_counts:
                    print(f"  {label}: {count}ê±´")
        
        # ë¹„ì‹ë³„ ì²˜ë¦¬ ë¹„ìœ¨
        if 'is_anonymized' in result_df.columns:
            anon_rate = result_df['is_anonymized'].sum() / len(result_df) * 100
            print(f"ë¹„ì‹ë³„ ì²˜ë¦¬ìœ¨: {anon_rate:.1f}%")
        
        print(f"--- ë°°ì¹˜ {batch_num} ì™„ë£Œ ---\n")
    
    def generate_advanced_insights(self, df: pd.DataFrame, column_name: str) -> dict:
        """ê³ ê¸‰ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        insights = {}
        
        # ê°ì • ê°•ë„ ë¶„ì„
        if f'{column_name}_sentiment_intensity' in df.columns:
            intensity_stats = df[f'{column_name}_sentiment_intensity'].describe()
            insights['intensity_analysis'] = {
                'mean': intensity_stats['mean'],
                'median': intensity_stats['50%'],
                'high_intensity_count': (df[f'{column_name}_sentiment_intensity'] >= 8).sum(),
                'low_intensity_count': (df[f'{column_name}_sentiment_intensity'] <= 3).sum()
            }
        
        # í‚¤ì›Œë“œ ë¶„ì„
        if f'{column_name}_keywords' in df.columns:
            all_keywords = []
            for keywords in df[f'{column_name}_keywords']:
                if isinstance(keywords, list):
                    all_keywords.extend(keywords)
            keyword_freq = Counter(all_keywords).most_common(20)
            insights['top_keywords'] = keyword_freq
        
        # ì˜ë£Œìš©ì–´ ì‚¬ìš© ë¹ˆë„
        if f'{column_name}_medical_terms' in df.columns:
            all_medical = []
            for terms in df[f'{column_name}_medical_terms']:
                if isinstance(terms, list):
                    all_medical.extend(terms)
            medical_freq = Counter(all_medical).most_common(10)
            insights['medical_terms_usage'] = medical_freq
        
        return insights
    
    def process_csv_advanced(self, input_file: str, column_name: str, batch_size: int = 5000, 
                           output_file: str = None, delay: float = 0.1):
        """ê³ ë„í™”ëœ CSV/Excel ì²˜ë¦¬ (ë°°ì¹˜ ë‹¨ìœ„)"""
        
        # íŒŒì¼ í™•ì¥ìì— ë”°ë¼ ì½ê¸° ë°©ë²• ê²°ì •
        file_ext = Path(input_file).suffix.lower()
        
        if file_ext == '.xlsx' or file_ext == '.xls':
            # Excel íŒŒì¼ ì½ê¸°
            try:
                df = pd.read_excel(input_file, engine='openpyxl')
            except Exception as e:
                print(f"Excel íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
                raise
        else:
            # CSV íŒŒì¼ ì½ê¸°
            try:
                df = pd.read_csv(input_file, encoding='utf-8')
            except UnicodeDecodeError:
                try:
                    df = pd.read_csv(input_file, encoding='cp949')
                except UnicodeDecodeError:
                    df = pd.read_csv(input_file, encoding='euc-kr')
        
        if column_name not in df.columns:
            available_columns = list(df.columns)
            raise ValueError(f"'{column_name}' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {available_columns}")
        
        total_rows = len(df)
        num_batches = (total_rows + batch_size - 1) // batch_size
        
        print(f"ì´ {total_rows}ê±´ì„ {num_batches}ê°œ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬í•©ë‹ˆë‹¤ (ë°°ì¹˜ í¬ê¸°: {batch_size})")
        
        if output_file is None:
            output_file = str(Path(input_file).stem) + "_advanced_processed.csv"
        
        processed_batches = []
        
        # ë°°ì¹˜ë³„ ì²˜ë¦¬
        for i in range(num_batches):
            start_idx = i * batch_size
            end_idx = min((i + 1) * batch_size, total_rows)
            batch_df = df.iloc[start_idx:end_idx].copy()
            
            processed_batch = self.process_batch(batch_df, column_name, i + 1, delay)
            processed_batches.append(processed_batch)
            
            # ì¤‘ê°„ ì €ì¥ (ë°°ì¹˜ë§ˆë‹¤)
            batch_output = f"{Path(output_file).stem}_batch_{i+1}.csv"
            processed_batch.to_csv(batch_output, index=False, encoding='utf-8-sig')
            print(f"ë°°ì¹˜ {i+1} ê²°ê³¼ê°€ '{batch_output}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            # ì²« ë²ˆì§¸ ë°°ì¹˜ í›„ ì‚¬ìš©ì í™•ì¸
            if i == 0:
                print(f"\n{'='*50}")
                print("ì²« ë²ˆì§¸ ë°°ì¹˜ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                print("ê²°ê³¼ë¥¼ í™•ì¸í•œ í›„ ë‚˜ë¨¸ì§€ ë°°ì¹˜ ì²˜ë¦¬ ì—¬ë¶€ë¥¼ ê²°ì •í•˜ì„¸ìš”.")
                print(f"{'='*50}")
                return processed_batch, f"ì²« ë²ˆì§¸ ë°°ì¹˜ë§Œ ì²˜ë¦¬ ì™„ë£Œ. íŒŒì¼: {batch_output}"
        
        # ì „ì²´ ê²°ê³¼ í•©ì¹˜ê¸°
        final_df = pd.concat(processed_batches, ignore_index=True)
        final_df.to_csv(output_file, index=False, encoding='utf-8-sig')
        
        # ê³ ê¸‰ ì¸ì‚¬ì´íŠ¸ ìƒì„±
        insights = self.generate_advanced_insights(final_df, column_name)
        
        # ì¸ì‚¬ì´íŠ¸ ì €ì¥
        insights_file = f"{Path(output_file).stem}_insights.json"
        with open(insights_file, 'w', encoding='utf-8') as f:
            json.dump(insights, f, ensure_ascii=False, indent=2)
        
        print(f"ì „ì²´ ì²˜ë¦¬ ì™„ë£Œ! ìµœì¢… ê²°ê³¼: '{output_file}', ì¸ì‚¬ì´íŠ¸: '{insights_file}'")
        return final_df, insights

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    PROJECT_ID = "mindmap-462708"
    EXCEL_FILE = "ì„¤ë¬¸ì¡°ì‚¬_ì „ì²˜ë¦¬ë°ì´í„°_20250620_0731.xlsx"
    COLUMN_NAME = "í˜‘ì—… í›„ê¸°"
    BATCH_SIZE = 5000
    OUTPUT_FILE = "ì„¤ë¬¸ì¡°ì‚¬_ì „ì²˜ë¦¬ë°ì´í„°_20250620_0731_advanced_processed.csv"
    
    try:
        analyzer = AdvancedReviewAnalyzer(project_id=PROJECT_ID)
        
        # ì²« 5ì²œê±´ ì²˜ë¦¬ (ì§€ì—°ì‹œê°„ ë‹¨ì¶•)
        result, message = analyzer.process_csv_advanced(
            EXCEL_FILE, 
            COLUMN_NAME, 
            batch_size=BATCH_SIZE,
            output_file=OUTPUT_FILE,
            delay=0.05  # ì§€ì—°ì‹œê°„ì„ 0.1ì´ˆì—ì„œ 0.05ì´ˆë¡œ ë‹¨ì¶•
        )
        
        print(f"\nì²˜ë¦¬ ì™„ë£Œ: {message}")
        
    except Exception as e:
        import traceback
        print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
        print(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")

if __name__ == "__main__":
    main()