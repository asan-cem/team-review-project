# ë°ì´í„° ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤
import pandas as pd  # ì—‘ì…€, CSV íŒŒì¼ ì²˜ë¦¬
import json  # JSON ë°ì´í„° ì²˜ë¦¬
import time  # ëŒ€ê¸° ì‹œê°„ ì²˜ë¦¬
from pathlib import Path  # íŒŒì¼ ê²½ë¡œ ì²˜ë¦¬
import math  # ìˆ˜í•™ ê³„ì‚°
import datetime  # ì‹œê°„ ì²˜ë¦¬
from tqdm import tqdm  # ì§„í–‰ë¥  í‘œì‹œ
from concurrent.futures import ThreadPoolExecutor, as_completed  # ë³‘ë ¬ ì²˜ë¦¬
from collections import Counter  # ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°
import re  # ì •ê·œí‘œí˜„ì‹
import sys  # ëª…ë ¹í–‰ ì¸ì ì²˜ë¦¬
import argparse  # ëª…ë ¹í–‰ ì¸ì íŒŒì‹±
import os  # íŒŒì¼ ì‹œìŠ¤í…œ ì¡°ì‘
import numpy as np  # ìˆ˜ì¹˜ ê³„ì‚°
from scipy import stats  # í†µê³„ ë¶„ì„
from sklearn.metrics.pairwise import cosine_similarity  # ì½”ì‚¬ì¸ ìœ ì‚¬ë„
import warnings  # ê²½ê³  ë©”ì‹œì§€ ì œì–´
warnings.filterwarnings('ignore')

# Google Cloud AI ë¼ì´ë¸ŒëŸ¬ë¦¬
import vertexai  # Google Vertex AI í”Œë«í¼
from vertexai.generative_models import GenerativeModel  # AI ëª¨ë¸

# ê°ì • ë¶„ë¥˜ ìƒìˆ˜ ì •ì˜
EMOTION_CATEGORIES = {
    "ê¸ì •êµ°": ["ê¸°ì¨", "ê°ì‚¬", "ì‹ ë¢°", "ë§Œì¡±"],
    "ë¶€ì •êµ°": ["ë¶„ë…¸", "ìŠ¬í””", "ë‘ë ¤ì›€", "ì‹¤ë§"], 
    "ì¤‘ë¦½êµ°": ["í‰ì˜¨", "ë¬´ê´€ì‹¬"]
}

MEDICAL_CONTEXT_CATEGORIES = {
    "í™˜ì_ì•ˆì „": ["ì‘ê¸‰ìƒí™©", "íˆ¬ì•½ì˜¤ë¥˜", "ìˆ˜ìˆ í˜‘ë ¥", "ì•ˆì „ì‚¬ê³ ", "ì˜ë£Œì‚¬ê³ "],
    "ì—…ë¬´_íš¨ìœ¨": ["ì¼ì •ì¡°ìœ¨", "ì •ë³´ê³µìœ ", "í”„ë¡œì„¸ìŠ¤", "ì—…ë¬´ë¶„ë‹´", "íš¨ìœ¨ì„±"],
    "ì¸ê°„_ê´€ê³„": ["ì¡´ì¤‘", "ì†Œí†µ", "ë°°ë ¤", "ì˜ˆì˜", "ì¹œì ˆ"],
    "ì „ë¬¸ì„±": ["ì§€ì‹", "ê¸°ìˆ ", "ê²½í—˜", "ì—­ëŸ‰", "ì „ë¬¸ì„±"]
}

# AIì—ê²Œ ë³´ë‚¼ ë¶„ì„ ì§€ì‹œì‚¬í•­ í…œí”Œë¦¿ (ê³ ë„í™” ë²„ì „)
PROMPT_TEMPLATE = """
[í˜ë¥´ì†Œë‚˜]
ë‹¹ì‹ ì€ ì˜ë£Œì§„ ê°„ í˜‘ì—… í”¼ë“œë°±ì„ ë¶„ì„í•˜ëŠ” ì „ë¬¸ AI ë¶„ì„ê°€ì…ë‹ˆë‹¤. 8ê°€ì§€ ì„¸ë¶„í™”ëœ ê°ì •ê³¼ ë³µí•© ê°ì • ë¶„ì„ì„ í†µí•´ ì •í™•í•˜ê³  ê¹Šì´ ìˆëŠ” ê°ì • ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

[ì§€ì‹œì‚¬í•­]
1. ì£¼ì–´ì§„ ì›ë³¸ í…ìŠ¤íŠ¸ì˜ í•µì‹¬ ì˜ë¯¸ë¥¼ ë³´ì¡´í•˜ë©´ì„œ, ì˜¤íƒ€ì™€ ë¬¸ë²•ì„ êµì •í•˜ì—¬ refined_textë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
2. ì†ê±°ë‚˜ ê³µê²©ì ì¸ í‘œí˜„ì€ ì „ë¬¸ì ì´ê³  ì •ì¤‘í•œ í‘œí˜„ìœ¼ë¡œ ìˆœí™”í•©ë‹ˆë‹¤.
3. **ë§¤ìš° ì¤‘ìš” - ë¹„ì‹ë³„ ì²˜ë¦¬ ê·œì¹™**: 
   **3-1. ê¸ì •ì /ì¤‘ë¦½ì  í”¼ë“œë°± ì²˜ë¦¬ ê·œì¹™:**
   - ê¸ì •ì ì´ê±°ë‚˜ ì¤‘ë¦½ì  í”¼ë“œë°±ì€ ì‹¤ëª…ì´ í¬í•¨ë˜ì–´ ìˆì–´ë„ ì ˆëŒ€ ë¹„ì‹ë³„ ì²˜ë¦¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
   - is_anonymizedë¥¼ ë°˜ë“œì‹œ falseë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
   
   **3-2. ë¶€ì •ì  í”¼ë“œë°± ì²˜ë¦¬ ê·œì¹™:**
   - ë¶€ì •ì  í”¼ë“œë°±ì´ë©´ì„œ ì‹¤ëª…ì´ë‚˜ ë§¤ìš° êµ¬ì²´ì ì¸ ê°œì¸ ì‹ë³„ ì •ë³´ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ë¹„ì‹ë³„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
   - ì¼ë°˜ì ì¸ í˜¸ì¹­("ì„ ìƒë‹˜", "ì§ì›ë¶„" ë“±)ì€ ë¹„ì‹ë³„ ì²˜ë¦¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

4. **ê³ ë„í™”ëœ ê°ì • ë¶„ì„**:
   - primary_emotion: ì£¼ìš” ê°ì • (ê¸°ì¨, ê°ì‚¬, ì‹ ë¢°, ë§Œì¡±, ë¶„ë…¸, ìŠ¬í””, ë‘ë ¤ì›€, ì‹¤ë§, í‰ì˜¨, ë¬´ê´€ì‹¬ ì¤‘ 1ê°œ)
   - secondary_emotion: ë³´ì¡° ê°ì • (ìˆëŠ” ê²½ìš°ì—ë§Œ, ì—†ìœ¼ë©´ null)
   - emotion_intensity: ê°ì • ê°•ë„ (1-10)
   - emotional_complexity: "ë‹¨ìˆœ" ë˜ëŠ” "ë³µí•©"
   - emotion_mix: ë³µí•© ê°ì •ì¸ ê²½ìš° ê° ê°ì •ì˜ ë¹„ìœ¨ (í•©ê³„ 1.0)

5. **ì˜ë£Œ í˜‘ì—… ë§¥ë½ ë¶„ì„**:
   - medical_context: í™˜ì_ì•ˆì „, ì—…ë¬´_íš¨ìœ¨, ì¸ê°„_ê´€ê³„, ì „ë¬¸ì„± ì¤‘ í•´ë‹¹í•˜ëŠ” ëª¨ë“  í•­ëª©
   - context_weight: ê° ë§¥ë½ì˜ ì¤‘ìš”ë„ (1-5)

6. confidence_scoreë¥¼ 1-10 ìŠ¤ì¼€ì¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤.
7. key_termsë¥¼ ì¶”ì¶œí•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ì œê³µí•©ë‹ˆë‹¤ (ì£¼ìš” í‚¤ì›Œë“œ 3-5ê°œ).
8. ê¸°ì¡´ ë¶„ë¥˜ ì²´ê³„ë„ ìœ ì§€í•©ë‹ˆë‹¤.

[ê°ì • ë¶„ë¥˜ ê°€ì´ë“œ]
**ê¸ì •êµ°:**
- ê¸°ì¨: "ê¸°ë»ìš”", "ì¦ê±°ì›Œìš”", "í–‰ë³µí•´ìš”"
- ê°ì‚¬: "ê°ì‚¬í•©ë‹ˆë‹¤", "ê³ ë§ˆì›Œìš”", "ë„ì›€ì´ ë˜ì—ˆì–´ìš”"
- ì‹ ë¢°: "ë¯¿ì„ ìˆ˜ ìˆì–´ìš”", "ì „ë¬¸ì ì´ì—ìš”", "ì•ˆì‹¬ì´ ë¼ìš”"
- ë§Œì¡±: "ë§Œì¡±í•´ìš”", "ì¢‹ì•˜ì–´ìš”", "í›Œë¥­í•´ìš”"

**ë¶€ì •êµ°:**
- ë¶„ë…¸: "í™”ë‚˜ìš”", "ì§œì¦ë‚˜ìš”", "ë¶„í•´ìš”"
- ìŠ¬í””: "ìŠ¬í¼ìš”", "ìš°ìš¸í•´ìš”", "ë§ˆìŒì´ ì•„íŒŒìš”"
- ë‘ë ¤ì›€: "ë¬´ì„œì›Œìš”", "ê±±ì •ë¼ìš”", "ë¶ˆì•ˆí•´ìš”"
- ì‹¤ë§: "ì‹¤ë§í•´ìš”", "ì•„ì‰¬ì›Œìš”", "ê¸°ëŒ€ì— ëª» ë¯¸ì³ìš”"

**ì¤‘ë¦½êµ°:**
- í‰ì˜¨: "ê´œì°®ì•„ìš”", "ë³´í†µì´ì—ìš”", "í‰ë²”í•´ìš”"
- ë¬´ê´€ì‹¬: "ìƒê´€ì—†ì–´ìš”", "ê·¸ëƒ¥ ê·¸ë˜ìš”"

[ì¶œë ¥ í˜•ì‹]
- JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.
- ì•„ë˜ í‚¤ë“¤ì„ ëª¨ë‘ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.

ì˜ˆì‹œ í˜•ì‹:
{{
  "refined_text": "ìµœì¢… ì •ì œ ë° ë¹„ì‹ë³„ ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸",
  "is_anonymized": false,
  "primary_emotion": "ê°ì‚¬",
  "secondary_emotion": "ì‹ ë¢°",
  "emotion_intensity": 7,
  "emotional_complexity": "ë³µí•©",
  "emotion_mix": {{"ê°ì‚¬": 0.7, "ì‹ ë¢°": 0.3}},
  "medical_context": ["ì¸ê°„_ê´€ê³„", "ì „ë¬¸ì„±"],
  "context_weight": {{"ì¸ê°„_ê´€ê³„": 4, "ì „ë¬¸ì„±": 3}},
  "confidence_score": 8,
  "key_terms": ["ê°ì‚¬", "ì „ë¬¸ì ", "ë„ì›€"],
  "labels": ["ìƒí˜¸ ì¡´ì¤‘", "ì—…ë¬´ íƒœë„"],
  "sentiment": "ê¸ì •",
  "sentiment_intensity": 7
}}

[ê¸°ì¡´ í˜¸í™˜ì„± í•„ë“œ]
- sentiment: ê¸°ì¡´ 3ë¶„ë¥˜ (ê¸ì •/ë¶€ì •/ì¤‘ë¦½) ìœ ì§€
- sentiment_intensity: ê¸°ì¡´ ê°•ë„ ì ìˆ˜ ìœ ì§€

[ì˜ˆì‹œ]
- ì›ë³¸ í…ìŠ¤íŠ¸: "ê¹€ì² ìˆ˜ íŒ€ì¥ ì¼ì²˜ë¦¬ ë„ˆë¬´ ë‹µë‹µí•˜ê³  ì†Œí†µë„ ì•ˆë¨. ê°œì„ ì´ ì‹œê¸‰í•¨"
- JSON ì¶œë ¥:
{{"refined_text": "ë‹´ë‹¹ìì˜ ì¼ ì²˜ë¦¬ê°€ ë‹¤ì†Œ ì•„ì‰½ê³ , ì†Œí†µ ë°©ì‹ì˜ ê°œì„ ì´ í•„ìš”í•´ ë³´ì…ë‹ˆë‹¤.", "is_anonymized": true, "sentiment": "ë¶€ì •", "sentiment_intensity": 8, "confidence_score": 9, "key_terms": ["ì¼ì²˜ë¦¬", "ì†Œí†µ", "ê°œì„ "], "labels": ["ì „ë¬¸ì„± ë¶€ì¡±", "ì§ì›ê°„ ì†Œí†µ"]}}

- ì›ë³¸ í…ìŠ¤íŠ¸: "ì„ ìƒë‹˜ë“¤ì´ ì—…ë¬´ ì²˜ë¦¬ê°€ ë„ˆë¬´ ëŠë ¤ì„œ ë‹µë‹µí•©ë‹ˆë‹¤"
- JSON ì¶œë ¥:
{{"refined_text": "ì„ ìƒë‹˜ë“¤ì˜ ì—…ë¬´ ì²˜ë¦¬ ì†ë„ê°€ ë‹¤ì†Œ ì•„ì‰½ìŠµë‹ˆë‹¤.", "is_anonymized": false, "sentiment": "ë¶€ì •", "sentiment_intensity": 6, "labels": ["ì „ë¬¸ì„± ë¶€ì¡±"]}}

- ì›ë³¸ í…ìŠ¤íŠ¸: "ë°•ì˜í¬ ì„ ìƒë‹˜ì€ í•­ìƒ ë™ë£Œë“¤ì„ ë¨¼ì € ì±™ê¸°ê³  ë°°ë ¤í•˜ëŠ” ëª¨ìŠµì´ ë³´ê¸° ì¢‹ìŠµë‹ˆë‹¤"
- JSON ì¶œë ¥:
{{"refined_text": "ë°•ì˜í¬ ì„ ìƒë‹˜ì€ í•­ìƒ ë™ë£Œë“¤ì„ ë¨¼ì € ì±™ê¸°ê³  ë°°ë ¤í•˜ëŠ” ëª¨ìŠµì´ ë³´ê¸° ì¢‹ìŠµë‹ˆë‹¤.", "is_anonymized": false, "sentiment": "ê¸ì •", "sentiment_intensity": 8, "labels": ["ìƒí˜¸ ì¡´ì¤‘", "ì—…ë¬´ íƒœë„"]}}

- ì›ë³¸ í…ìŠ¤íŠ¸: "ì—¬ì ì§ì›ë¶„ì´ ë¶ˆì¹œì ˆí•´ì„œ ê¸°ë¶„ì´ ë‚˜ë¹´ìŠµë‹ˆë‹¤"
- JSON ì¶œë ¥:
{{"refined_text": "ë‹´ë‹¹ ì§ì›ë¶„ì˜ ì„œë¹„ìŠ¤ íƒœë„ê°€ ì•„ì‰¬ì› ìŠµë‹ˆë‹¤.", "is_anonymized": true, "sentiment": "ë¶€ì •", "sentiment_intensity": 7, "labels": ["ìƒí˜¸ ì¡´ì¤‘"]}}

- ì›ë³¸ í…ìŠ¤íŠ¸: "ì—¬ì ì§ì›ë¶„ì´ ë„ì›€ì„ ë§ì´ ì£¼ì…¨ì–´ìš”"
- JSON ì¶œë ¥:
{{"refined_text": "ì—¬ì ì§ì›ë¶„ì´ ë„ì›€ì„ ë§ì´ ì£¼ì…¨ì–´ìš”.", "is_anonymized": false, "sentiment": "ê¸ì •", "sentiment_intensity": 6, "labels": ["ì—…ë¬´ íƒœë„"]}}

- ì›ë³¸ í…ìŠ¤íŠ¸: "ì´ì •ì€ ì„ ìƒë‹˜ì´ ë¶ˆì¹œì ˆí•˜ê³  ì—…ë¬´ ì²˜ë¦¬ê°€ ëŠë ¤ìš”"
- JSON ì¶œë ¥:
{{"refined_text": "ë‹´ë‹¹ ì§ì›ë¶„ì˜ ì„œë¹„ìŠ¤ íƒœë„ì™€ ì—…ë¬´ ì²˜ë¦¬ ì†ë„ê°€ ì•„ì‰¬ì› ìŠµë‹ˆë‹¤.", "is_anonymized": true, "sentiment": "ë¶€ì •", "sentiment_intensity": 7, "labels": ["ìƒí˜¸ ì¡´ì¤‘", "ì „ë¬¸ì„± ë¶€ì¡±"]}}

- ì›ë³¸ í…ìŠ¤íŠ¸: "ì´ì •ì€ ì„ ìƒë‹˜ ë•ë¶„ì— ì—…ë¬´ê°€ ìˆ˜ì›”í–ˆìŠµë‹ˆë‹¤"
- JSON ì¶œë ¥:
{{"refined_text": "ì´ì •ì€ ì„ ìƒë‹˜ ë•ë¶„ì— ì—…ë¬´ê°€ ìˆ˜ì›”í–ˆìŠµë‹ˆë‹¤.", "is_anonymized": false, "sentiment": "ê¸ì •", "sentiment_intensity": 7, "labels": ["ì—…ë¬´ íƒœë„"]}}

- ì›ë³¸ í…ìŠ¤íŠ¸: "ê¹€ì² ìˆ˜ ê³¼ì¥ë‹˜ ì •ë§ ì„±ì˜ì—†ê²Œ ì¼í•˜ì‹œë„¤ìš”"
- JSON ì¶œë ¥:
{{"refined_text": "ë‹´ë‹¹ìì˜ ì—…ë¬´ ì²˜ë¦¬ ë°©ì‹ì´ ì¢€ ë” ì‹ ì¤‘í–ˆìœ¼ë©´ ì¢‹ê² ìŠµë‹ˆë‹¤.", "is_anonymized": true, "sentiment": "ë¶€ì •", "sentiment_intensity": 8, "labels": ["ì—…ë¬´ íƒœë„"]}}

- ì›ë³¸ í…ìŠ¤íŠ¸: "ê°„í˜¸íŒ€ ë°•ì˜í¬ë‹˜ì´ í•­ìƒ ì¹œì ˆí•˜ì„¸ìš”"
- JSON ì¶œë ¥:
{{"refined_text": "ê°„í˜¸íŒ€ ë°•ì˜í¬ë‹˜ì´ í•­ìƒ ì¹œì ˆí•˜ì„¸ìš”.", "is_anonymized": false, "sentiment": "ê¸ì •", "sentiment_intensity": 7, "labels": ["ìƒí˜¸ ì¡´ì¤‘"]}}

- ì›ë³¸ í…ìŠ¤íŠ¸: "í˜ˆê´€ì´ ì—†ì–´ì„œ ì‹¤íŒ¨í•˜ë©´ ì‹¤íŒ¨í•˜ë‹¤ê³  ì¸ê³„ì£¼ê³  ê°€ì‹­ë‹ˆë‹¤.. ã… ã… "
- JSON ì¶œë ¥:
{{"refined_text": "í˜ˆê´€ í™•ë³´ê°€ ì–´ë ¤ì›Œ ì‹¤íŒ¨í•  ê²½ìš°, ìƒí™©ì„ ì¸ê³„í•˜ê³  ê°€ì‹œëŠ” ê²½ìš°ê°€ ìˆì–´ ì•„ì‰½ìŠµë‹ˆë‹¤.", "is_anonymized": false, "sentiment": "ë¶€ì •", "sentiment_intensity": 6, "labels": ["ì—…ë¬´ íƒœë„", "ì§ì›ê°„ ì†Œí†µ"]}}

- ì›ë³¸ í…ìŠ¤íŠ¸: "ì—…ë¬´ ì²˜ë¦¬ê°€ ë„ˆë¬´ ëŠ¦ê³  ë‹µë‹µí•´ìš”"
- JSON ì¶œë ¥:
{{"refined_text": "ì—…ë¬´ ì²˜ë¦¬ ì†ë„ê°€ ë‹¤ì†Œ ì•„ì‰½ìŠµë‹ˆë‹¤.", "is_anonymized": false, "sentiment": "ë¶€ì •", "sentiment_intensity": 6, "labels": ["ì „ë¬¸ì„± ë¶€ì¡±"]}}

- ì›ë³¸ í…ìŠ¤íŠ¸: "ë¶ˆì¹œì ˆí•˜ê³  ë§ë„ ì•ˆ ë“¤ì–´ì¤˜ìš”"
- JSON ì¶œë ¥:
{{"refined_text": "ì„œë¹„ìŠ¤ íƒœë„ì™€ ì†Œí†µ ë°©ì‹ì´ ê°œì„ ë˜ì—ˆìœ¼ë©´ ì¢‹ê² ìŠµë‹ˆë‹¤.", "is_anonymized": false, "sentiment": "ë¶€ì •", "sentiment_intensity": 7, "labels": ["ìƒí˜¸ ì¡´ì¤‘", "ì§ì›ê°„ ì†Œí†µ"]}}

ì›ë³¸ í…ìŠ¤íŠ¸: "{original_text}"
"""

class ReviewAnalyzer:
    """
    í…ìŠ¤íŠ¸ ë¦¬ë·°ë¥¼ AIë¡œ ë¶„ì„í•˜ëŠ” í´ë˜ìŠ¤
    Googleì˜ Vertex AIë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ì˜ ê°ì •, ê°œì„ ëœ í‘œí˜„, ë¶„ë¥˜ ë¼ë²¨ ë“±ì„ ë¶„ì„í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, project_id: str, location: str = "us-central1"):
        """
        ë¶„ì„ê¸°ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        
        Args:
            project_id: Google Cloud í”„ë¡œì íŠ¸ ID (í•„ìˆ˜)
            location: AI ëª¨ë¸ì´ ì‹¤í–‰ë  ì§€ì—­ (ê¸°ë³¸ê°’: us-central1)
        """
        # Google Cloud AI í”Œë«í¼ ì´ˆê¸°í™”
        vertexai.init(project=project_id, location=location)
        
        # ì‚¬ìš©í•  AI ëª¨ë¸ ì„¤ì • (Gemini 2.0 Flash ëª¨ë¸)
        self.model = GenerativeModel("gemini-2.0-flash")
    
    def analyze_refined_text(self, refined_text: str, is_already_anonymized: bool = False) -> dict:
        """
        ì´ë¯¸ ì •ì œëœ í…ìŠ¤íŠ¸ë¥¼ AIë¡œ ë¶„ì„í•˜ì—¬ ê°ì • ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        í…ìŠ¤íŠ¸ ì •ì œì™€ ë¹„ì‹ë³„ ì²˜ë¦¬ëŠ” ê±´ë„ˆë›°ê³  ê°ì • ë¶„ì„ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        
        Args:
            refined_text: ì´ë¯¸ ì •ì œëœ í…ìŠ¤íŠ¸
            is_already_anonymized: ì´ë¯¸ ë¹„ì‹ë³„ ì²˜ë¦¬ ì—¬ë¶€
            
        Returns:
            dict: ë¶„ì„ ê²°ê³¼ (ê°ì •, ê°œì„ ëœ í…ìŠ¤íŠ¸, ë¼ë²¨ ë“±)
        """
        # ë¹ˆ í…ìŠ¤íŠ¸ ì²˜ë¦¬
        if not refined_text or refined_text.strip() == "":
            return {
                "original_text": refined_text,
                "refined_text": "",
                "is_anonymized": is_already_anonymized,
                "primary_emotion": "í‰ì˜¨",
                "secondary_emotion": None,
                "emotion_intensity": 5,
                "emotional_complexity": "ë‹¨ìˆœ",
                "emotion_mix": {"í‰ì˜¨": 1.0},
                "medical_context": [],
                "context_weight": {},
                "sentiment": "ì¤‘ë¦½",
                "sentiment_intensity": 5,
                "confidence_score": 10,
                "key_terms": [],
                "labels": []
            }
        
        # ì •ì œëœ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ê°ì • ë¶„ì„ ì „ìš© í”„ë¡¬í”„íŠ¸
        refined_prompt = f"""
[í˜ë¥´ì†Œë‚˜]
ë‹¹ì‹ ì€ ì˜ë£Œì§„ ê°„ í˜‘ì—… í”¼ë“œë°±ì„ ë¶„ì„í•˜ëŠ” ì „ë¬¸ AI ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì´ë¯¸ ì •ì œëœ í…ìŠ¤íŠ¸ì— ëŒ€í•´ ê°ì • ë¶„ì„ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤.

[ì§€ì‹œì‚¬í•­]
ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ëŠ” ì´ë¯¸ ì •ì œ ë° ë¹„ì‹ë³„ ì²˜ë¦¬ê°€ ì™„ë£Œëœ ìƒíƒœì…ë‹ˆë‹¤. 
í…ìŠ¤íŠ¸ ìˆ˜ì • ì—†ì´ ê°ì • ë¶„ì„ë§Œ ìˆ˜í–‰í•˜ì„¸ìš”.

1. **ê³ ë„í™”ëœ ê°ì • ë¶„ì„**:
   - primary_emotion: ì£¼ìš” ê°ì • (ê¸°ì¨, ê°ì‚¬, ì‹ ë¢°, ë§Œì¡±, ë¶„ë…¸, ìŠ¬í””, ë‘ë ¤ì›€, ì‹¤ë§, í‰ì˜¨, ë¬´ê´€ì‹¬ ì¤‘ 1ê°œ)
   - secondary_emotion: ë³´ì¡° ê°ì • (ìˆëŠ” ê²½ìš°ì—ë§Œ, ì—†ìœ¼ë©´ null)
   - emotion_intensity: ê°ì • ê°•ë„ (1-10)
   - emotional_complexity: "ë‹¨ìˆœ" ë˜ëŠ” "ë³µí•©"
   - emotion_mix: ë³µí•© ê°ì •ì¸ ê²½ìš° ê° ê°ì •ì˜ ë¹„ìœ¨ (í•©ê³„ 1.0)

2. **ì˜ë£Œ í˜‘ì—… ë§¥ë½ ë¶„ì„**:
   - medical_context: í™˜ì_ì•ˆì „, ì—…ë¬´_íš¨ìœ¨, ì¸ê°„_ê´€ê³„, ì „ë¬¸ì„± ì¤‘ í•´ë‹¹í•˜ëŠ” ëª¨ë“  í•­ëª©
   - context_weight: ê° ë§¥ë½ì˜ ì¤‘ìš”ë„ (1-5)

3. confidence_scoreë¥¼ 1-10 ìŠ¤ì¼€ì¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤.
4. key_termsë¥¼ ì¶”ì¶œí•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ì œê³µí•©ë‹ˆë‹¤ (ì£¼ìš” í‚¤ì›Œë“œ 3-5ê°œ).
5. labelsë¥¼ ë¶„ë¥˜ ì²´ê³„ì— ë”°ë¼ ì œê³µí•©ë‹ˆë‹¤.

[ë¶„ë¥˜ ì²´ê³„]
- "ë¶€ì„œê°„ í˜‘ì—…": ì„œë¡œ ë‹¤ë¥¸ ë¶€ì„œ/íŒ€ ê°„ì˜ ì—…ë¬´ ì—°ê³„ì™€ í˜‘ë ¥ ë¬¸ì œ.
- "ì§ì›ê°„ ì†Œí†µ": ê°™ì€ ë¶€ì„œ/íŒ€ ë‚´ ë™ë£Œ ê°„ì˜ ì†Œí†µ ë° ê´€ê³„ ë¬¸ì œ.
- "ì „ë¬¸ì„± ë¶€ì¡±": ê°œì¸ì˜ ì—…ë¬´ ì§€ì‹, ê¸°ìˆ , ê²½í—˜ ë¶€ì¡± ë¬¸ì œ.
- "ì—…ë¬´ íƒœë„": ì±…ì„ê°, ì ê·¹ì„± ë“± ì—…ë¬´ë¥¼ ëŒ€í•˜ëŠ” ìì„¸ ë¬¸ì œ.
- "ìƒí˜¸ ì¡´ì¤‘": ì¸ê²©ì  ëŒ€ìš°, ë°°ë ¤ ë“± ê´€ê³„ì—ì„œì˜ ì˜ˆì˜ ë¬¸ì œ.

[ì¶œë ¥ í˜•ì‹ - JSONë§Œ ì‘ë‹µ]
{{
  "refined_text": "{refined_text}",
  "is_anonymized": {str(is_already_anonymized).lower()},
  "primary_emotion": "ê°ì‚¬",
  "secondary_emotion": null,
  "emotion_intensity": 7,
  "emotional_complexity": "ë‹¨ìˆœ",
  "emotion_mix": {{"ê°ì‚¬": 1.0}},
  "medical_context": ["ì¸ê°„_ê´€ê³„"],
  "context_weight": {{"ì¸ê°„_ê´€ê³„": 4}},
  "confidence_score": 8,
  "key_terms": ["ê°ì‚¬", "ë„ì›€"],
  "labels": ["ìƒí˜¸ ì¡´ì¤‘"],
  "sentiment": "ê¸ì •",
  "sentiment_intensity": 7
}}

ë¶„ì„í•  ì •ì œëœ í…ìŠ¤íŠ¸: "{refined_text}"
"""
        
        try:
            # AI ëª¨ë¸ì— ë¶„ì„ ìš”ì²­
            response = self.model.generate_content(refined_prompt)
            response_text = response.text.strip()
            
            # AI ì‘ë‹µì—ì„œ JSON ë¶€ë¶„ ì¶”ì¶œ ë° íŒŒì‹±
            try:
                json_start = response_text.find('{')
                json_end = response_text.rfind('}') + 1
                if json_start != -1 and json_end > json_start:
                    json_text = response_text[json_start:json_end]
                    result = json.loads(json_text)
                    result["original_text"] = refined_text
                    return result
                else:
                    raise json.JSONDecodeError("No JSON found", response_text, 0)
            except json.JSONDecodeError:
                print(f"AI ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {refined_text[:50]}...")
                return {
                    "original_text": refined_text,
                    "refined_text": refined_text,
                    "is_anonymized": is_already_anonymized,
                    "primary_emotion": "í‰ì˜¨",
                    "secondary_emotion": None,
                    "emotion_intensity": 5,
                    "emotional_complexity": "ë‹¨ìˆœ",
                    "emotion_mix": {"í‰ì˜¨": 1.0},
                    "medical_context": [],
                    "context_weight": {},
                    "sentiment": "ì¤‘ë¦½",
                    "sentiment_intensity": 5,
                    "confidence_score": 1,
                    "key_terms": [],
                    "labels": []
                }
                
        except Exception as e:
            print(f"AI ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                "original_text": refined_text,
                "refined_text": refined_text,
                "is_anonymized": is_already_anonymized,
                "primary_emotion": "í‰ì˜¨",
                "secondary_emotion": None,
                "emotion_intensity": 5,
                "emotional_complexity": "ë‹¨ìˆœ",
                "emotion_mix": {"í‰ì˜¨": 1.0},
                "medical_context": [],
                "context_weight": {},
                "sentiment": "ì¤‘ë¦½",
                "sentiment_intensity": 5,
                "confidence_score": 1,
                "key_terms": [],
                "labels": []
            }

    def analyze_review(self, original_text: str) -> dict:
        """
        í…ìŠ¤íŠ¸ë¥¼ AIë¡œ ë¶„ì„í•˜ì—¬ ê°ì •, ê°œì„ ëœ í…ìŠ¤íŠ¸ ë“±ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        Args:
            original_text: ë¶„ì„í•  ì›ë³¸ í…ìŠ¤íŠ¸
            
        Returns:
            dict: ë¶„ì„ ê²°ê³¼ (ê°ì •, ê°œì„ ëœ í…ìŠ¤íŠ¸, ë¼ë²¨ ë“±)
        """
        # ë¹ˆ í…ìŠ¤íŠ¸ ì²˜ë¦¬
        if not original_text or original_text.strip() == "":
            return {
                "original_text": original_text,
                "refined_text": "",
                "is_anonymized": False,
                "primary_emotion": "í‰ì˜¨",
                "secondary_emotion": None,
                "emotion_intensity": 5,
                "emotional_complexity": "ë‹¨ìˆœ",
                "emotion_mix": {"í‰ì˜¨": 1.0},
                "medical_context": [],
                "context_weight": {},
                "sentiment": "ì¤‘ë¦½",
                "sentiment_intensity": 5,
                "confidence_score": 10,
                "key_terms": [],
                "labels": []
            }
        
        # AI ë¶„ì„ í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = PROMPT_TEMPLATE.format(original_text=original_text)
        
        try:
            # AI ëª¨ë¸ì— ë¶„ì„ ìš”ì²­
            response = self.model.generate_content(prompt)
            response_text = response.text.strip()
            
            # AI ì‘ë‹µì—ì„œ JSON ë¶€ë¶„ ì¶”ì¶œ ë° íŒŒì‹±
            try:
                json_start = response_text.find('{')
                json_end = response_text.rfind('}') + 1
                if json_start != -1 and json_end > json_start:
                    json_text = response_text[json_start:json_end]
                    result = json.loads(json_text)
                    result["original_text"] = original_text
                    return result
                else:
                    raise json.JSONDecodeError("No JSON found", response_text, 0)
            except json.JSONDecodeError:
                print(f"AI ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {original_text[:50]}...")
                return {
                    "original_text": original_text,
                    "refined_text": original_text,
                    "is_anonymized": False,
                    "primary_emotion": "í‰ì˜¨",
                    "secondary_emotion": None,
                    "emotion_intensity": 5,
                    "emotional_complexity": "ë‹¨ìˆœ",
                    "emotion_mix": {"í‰ì˜¨": 1.0},
                    "medical_context": [],
                    "context_weight": {},
                    "sentiment": "ì¤‘ë¦½",
                    "sentiment_intensity": 5,
                    "confidence_score": 1,
                    "key_terms": [],
                    "labels": []
                }
                
        except Exception as e:
            print(f"AI ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                "original_text": original_text,
                "refined_text": original_text,
                "is_anonymized": False,
                "primary_emotion": "í‰ì˜¨",
                "secondary_emotion": None,
                "emotion_intensity": 5,
                "emotional_complexity": "ë‹¨ìˆœ",
                "emotion_mix": {"í‰ì˜¨": 1.0},
                "medical_context": [],
                "context_weight": {},
                "sentiment": "ì¤‘ë¦½",
                "sentiment_intensity": 5,
                "confidence_score": 1,
                "key_terms": [],
                "labels": []
            }
    
    def process_csv(self, input_file: str, output_file: str = None, delay: float = 0.1, max_rows: int = None):
        """
        CSV íŒŒì¼ì˜ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
        
        Args:
            input_file: ë¶„ì„í•  CSV íŒŒì¼ ê²½ë¡œ
            output_file: ê²°ê³¼ë¥¼ ì €ì¥í•  íŒŒì¼ ê²½ë¡œ (ìë™ ìƒì„± ê°€ëŠ¥)
            delay: AI í˜¸ì¶œ ê°„ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
            max_rows: í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì²˜ë¦¬í•  ìµœëŒ€ í–‰ ìˆ˜
        """
        # CSV íŒŒì¼ì„ ì½ì–´ì„œ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
        df = pd.read_csv(input_file)
        
        # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì¼ë¶€ ë°ì´í„°ë§Œ ì²˜ë¦¬í•˜ëŠ” ê²½ìš°
        if max_rows:
            df = df.head(max_rows)
        
        # ì¶œë ¥ íŒŒì¼ëª… ìë™ ìƒì„±
        if output_file is None:
            output_file = str(Path(input_file).stem) + "_processed.csv"
        
        results = []  # ë¶„ì„ ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
        total_rows = len(df)
        
        print(f"ì´ {total_rows}ê°œì˜ ë¦¬ë·°ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤...")
        
        # ê° í–‰ì˜ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ í•˜ë‚˜ì”© ë¶„ì„
        for idx, row in df.iterrows():
            # í…ìŠ¤íŠ¸ ë°ì´í„° ì¶”ì¶œ (ì»¬ëŸ¼ëª…ì— ë”°ë¼ ìë™ ì„ íƒ)
            original_text = str(row['original_review']) if 'original_review' in row else str(row.iloc[0])
            
            print(f"ë¶„ì„ ì¤‘... ({idx + 1}/{total_rows})")
            
            # AIë¡œ í…ìŠ¤íŠ¸ ë¶„ì„ ìˆ˜í–‰
            result = self.analyze_review(original_text)
            results.append(result)
            
            # AI ì„œë¹„ìŠ¤ ê³¼ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ëŒ€ê¸°
            if delay > 0:
                time.sleep(delay)
        
        # ë¶„ì„ ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥
        result_df = pd.DataFrame(results)
        result_df.to_csv(output_file, index=False, encoding='utf-8-sig')
        
        print(f"ë¶„ì„ ì™„ë£Œ! ê²°ê³¼ê°€ '{output_file}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # ê°ì • ë¶„ì„ í†µê³„ ì¶œë ¥
        sentiment_counts = result_df['sentiment'].value_counts()
        print("\nê°ì • ë¶„ì„ ê²°ê³¼:")
        for sentiment, count in sentiment_counts.items():
            print(f"  {sentiment}: {count}ê°œ")


    def analyze_batch(self, texts: list, batch_size: int = 10) -> list:
        """
        ì—¬ëŸ¬ í…ìŠ¤íŠ¸ë¥¼ ë°°ì¹˜ë¡œ ì²˜ë¦¬í•˜ì—¬ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.
        
        Args:
            texts: ë¶„ì„í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            batch_size: í•œ ë²ˆì— ì²˜ë¦¬í•  í…ìŠ¤íŠ¸ ìˆ˜
            
        Returns:
            list: ë¶„ì„ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        results = []
        
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬ (ìˆœì„œ ë³´ì¥)
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i+batch_size]
            batch_results = []
            
            # ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ThreadPoolExecutor ì‚¬ìš©
            with ThreadPoolExecutor(max_workers=min(5, len(batch))) as executor:
                # ê° í…ìŠ¤íŠ¸ì— ëŒ€í•´ ë¹„ë™ê¸° ì‘ì—… ì œì¶œ (ì¸ë±ìŠ¤ì™€ í•¨ê»˜)
                future_to_index = {}
                for idx, text in enumerate(batch):
                    future = executor.submit(self.analyze_review, text)
                    future_to_index[future] = (idx, text)
                
                # ë°°ì¹˜ í¬ê¸°ë§Œí¼ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
                batch_results = [None] * len(batch)
                
                # ê²°ê³¼ ìˆ˜ì§‘ (ì™„ë£Œ ìˆœì„œì™€ ê´€ê³„ì—†ì´ ì›ë˜ ìˆœì„œ ìœ ì§€)
                for future in as_completed(future_to_index):
                    idx, original_text = future_to_index[future]
                    try:
                        result = future.result()
                        batch_results[idx] = result
                    except Exception as e:
                        print(f"ë°°ì¹˜ ì²˜ë¦¬ ì˜¤ë¥˜: {e} - {original_text[:50]}...")
                        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
                        batch_results[idx] = {
                            "original_text": original_text,
                            "refined_text": original_text,
                            "is_anonymized": False,
                            "primary_emotion": "í‰ì˜¨",
                            "secondary_emotion": None,
                            "emotion_intensity": 5,
                            "emotional_complexity": "ë‹¨ìˆœ",
                            "emotion_mix": {"í‰ì˜¨": 1.0},
                            "medical_context": [],
                            "context_weight": {},
                            "sentiment": "ì¤‘ë¦½",
                            "sentiment_intensity": 5,
                            "confidence_score": 1,
                            "key_terms": [],
                            "labels": []
                        }
            
            # ë°°ì¹˜ ê²°ê³¼ë¥¼ ì „ì²´ ê²°ê³¼ì— ìˆœì„œëŒ€ë¡œ ì¶”ê°€
            results.extend(batch_results)
        
        return results
    
    def analyze_refined_batch(self, texts_and_flags: list, batch_size: int) -> list:
        """
        ì—¬ëŸ¬ ì •ì œëœ í…ìŠ¤íŠ¸ë¥¼ ë°°ì¹˜ë¡œ ì²˜ë¦¬í•˜ì—¬ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.
        
        Args:
            texts_and_flags: (ì •ì œëœ_í…ìŠ¤íŠ¸, ë¹„ì‹ë³„_ì—¬ë¶€) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
            batch_size: ë°°ì¹˜ í¬ê¸°
            
        Returns:
            list: ë¶„ì„ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        results = []
        
        # ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ThreadPoolExecutor ì‚¬ìš©
        with ThreadPoolExecutor(max_workers=min(5, len(texts_and_flags))) as executor:
            # ê° í…ìŠ¤íŠ¸ì— ëŒ€í•´ ë¹„ë™ê¸° ì‘ì—… ì œì¶œ (ì¸ë±ìŠ¤ì™€ í•¨ê»˜)
            future_to_index = {}
            for idx, (refined_text, is_anonymized) in enumerate(texts_and_flags):
                future = executor.submit(self.analyze_refined_text, refined_text, is_anonymized)
                future_to_index[future] = (idx, refined_text, is_anonymized)
            
            # ë°°ì¹˜ í¬ê¸°ë§Œí¼ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
            batch_results = [None] * len(texts_and_flags)
            
            # ê²°ê³¼ ìˆ˜ì§‘ (ì™„ë£Œ ìˆœì„œì™€ ê´€ê³„ì—†ì´ ì›ë˜ ìˆœì„œ ìœ ì§€)
            for future in as_completed(future_to_index):
                idx, refined_text, is_anonymized = future_to_index[future]
                try:
                    result = future.result()
                    batch_results[idx] = result
                except Exception as e:
                    print(f"ë°°ì¹˜ ì²˜ë¦¬ ì˜¤ë¥˜: {e} - {refined_text[:50]}...")
                    # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
                    batch_results[idx] = {
                        "original_text": refined_text,
                        "refined_text": refined_text,
                        "is_anonymized": is_anonymized,
                        "primary_emotion": "í‰ì˜¨",
                        "secondary_emotion": None,
                        "emotion_intensity": 5,
                        "emotional_complexity": "ë‹¨ìˆœ",
                        "emotion_mix": {"í‰ì˜¨": 1.0},
                        "medical_context": [],
                        "context_weight": {},
                        "sentiment": "ì¤‘ë¦½",
                        "sentiment_intensity": 5,
                        "confidence_score": 1,
                        "key_terms": [],
                        "labels": []
                    }
        
        return batch_results
    
    def retry_low_quality_analysis_refined(self, texts_and_flags: list, results: list, quality_results: list, max_retries: int = 2) -> tuple:
        """
        ì •ì œëœ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ë‚®ì€ í’ˆì§ˆì˜ ë¶„ì„ ê²°ê³¼ë¥¼ ì¬ë¶„ì„í•©ë‹ˆë‹¤.
        
        Args:
            texts_and_flags: (ì •ì œëœ_í…ìŠ¤íŠ¸, ë¹„ì‹ë³„_ì—¬ë¶€) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
            results: ë¶„ì„ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            quality_results: í’ˆì§ˆ ê²€ì¦ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
            
        Returns:
            tuple: (ê°œì„ ëœ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸, ê°œì„ ëœ í’ˆì§ˆ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸)
        """
        improved_results = results.copy()
        improved_quality = quality_results.copy()
        
        # ì¬ê²€í† ê°€ í•„ìš”í•œ í•­ëª©ë“¤ ì°¾ê¸°
        retry_indices = [i for i, q in enumerate(quality_results) if q['needs_review']]
        
        if not retry_indices:
            print("ì¬ê²€í† ê°€ í•„ìš”í•œ í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
            return improved_results, improved_quality
        
        print(f"\n{len(retry_indices)}ê°œ í•­ëª©ì„ ì¬ë¶„ì„í•©ë‹ˆë‹¤...")
        
        retry_count = 0
        total_improved = 0
        
        with tqdm(total=len(retry_indices), desc="ì¬ë¶„ì„ ì§„í–‰ë¥ ", unit="ê±´") as pbar:
            for idx in retry_indices:
                if retry_count >= max_retries:
                    break
                
                refined_text, is_anonymized = texts_and_flags[idx]
                original_quality = quality_results[idx]['quality_score']
                
                # ì¬ë¶„ì„ ìˆ˜í–‰
                new_result = self.analyze_refined_text(refined_text, is_anonymized)
                new_quality = self.validate_analysis_quality(new_result, refined_text)
                
                # ê°œì„ ëœ ê²½ìš°ì—ë§Œ ê²°ê³¼ ë°˜ì˜
                if new_quality['quality_score'] > original_quality:
                    # original_text ì œê±°
                    if "original_text" in new_result:
                        del new_result["original_text"]
                    
                    improved_results[idx] = new_result
                    improved_quality[idx] = new_quality
                    total_improved += 1
                
                retry_count += 1
                pbar.update(1)
                
                # API ì œí•œì„ ìœ„í•œ ì§§ì€ ëŒ€ê¸°
                time.sleep(0.5)
        
        print(f"ì¬ë¶„ì„ ì™„ë£Œ: {total_improved}ê°œ í•­ëª©ì´ ê°œì„ ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        return improved_results, improved_quality
    
    def _create_explanation_sheets(self, writer, vector_analysis, statistical_results, advanced_metrics):
        """
        ìˆ˜í•™ì /í†µê³„ì  ë¶„ì„ ê²°ê³¼ì— ëŒ€í•œ ìƒì„¸ ì„¤ëª… ì‹œíŠ¸ë“¤ì„ ìƒì„±í•©ë‹ˆë‹¤.
        ë¹„ì „ê³µìë„ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ìì„¸í•œ ì„¤ëª…ì„ í¬í•¨í•©ë‹ˆë‹¤.
        """
        
        # 1. ë¶„ì„ ë°©ë²•ë¡  ì„¤ëª… ì‹œíŠ¸
        methodology_data = [
            ["ë¶„ì„ í•­ëª©", "ë¶„ì„ ëª©ì ", "ë¶„ì„ ë°©ë²•", "í™œìš© ë¶„ì•¼"],
            ["", "", "", ""],
            ["ğŸ“Š ë²¡í„°ë¶„ì„", "ê°ì •ì„ ìˆ˜í•™ì  ê³µê°„ì— ë°°ì¹˜í•˜ì—¬ íŒ¨í„´ íŒŒì•…", "8ì°¨ì› ë²¡í„° ê³µê°„ì—ì„œ ê°ì •ì˜ ìœ„ì¹˜ì™€ ê´€ê³„ ê³„ì‚°", "ê°ì •ì˜ ë‹¤ì–‘ì„±, ì‘ì§‘ë„, ê·¹ì„± ë¶„ì„"],
            ["", "â€¢ ê°ì • ê°„ì˜ ìœ ì‚¬ì„±ê³¼ ì°¨ì´ì  ì •ëŸ‰í™”", "â€¢ ê° ê°ì •ì„ ê³ ìœ í•œ ìˆ«ì ë²¡í„°ë¡œ í‘œí˜„", "â€¢ ì¡°ì§ ë‚´ ê°ì • ë¶„í¬ ê· í˜• í‰ê°€"],
            ["", "â€¢ ì „ì²´ì ì¸ ê°ì • ë™í–¥ íŒŒì•…", "â€¢ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¡œ ê°ì • ê°„ ê´€ë ¨ì„± ì¸¡ì •", "â€¢ ê°ì • ë³€í™” ì¶”ì´ ëª¨ë‹ˆí„°ë§"],
            ["", "", "", ""],
            ["ğŸ“ˆ ê¸°ìˆ í†µê³„ëŸ‰", "ë°ì´í„°ì˜ ê¸°ë³¸ì ì¸ íŠ¹ì„± íŒŒì•…", "í‰ê· , ë¶„ì‚°, ì™œë„, ì²¨ë„ ë“± ê³„ì‚°", "ë°ì´í„° ë¶„í¬ì˜ ì¤‘ì‹¬ê³¼ í¼ì§ ì •ë„ íŒŒì•…"],
            ["", "â€¢ ê°ì • ê°•ë„ì˜ ì „ë°˜ì ì¸ ìˆ˜ì¤€ í™•ì¸", "â€¢ í‰ê· : ì¤‘ì‹¬ ê²½í–¥", "â€¢ ì¡°ì§ ì „ì²´ì˜ ê°ì • ìˆ˜ì¤€ ì§„ë‹¨"],
            ["", "â€¢ ë°ì´í„°ì˜ ë³€ë™ì„±ê³¼ ì¹˜ìš°ì¹¨ ë¶„ì„", "â€¢ ë¶„ì‚°: í¼ì§ ì •ë„", "â€¢ ê·¹ë‹¨ì  ê°ì • ë°˜ì‘ ê°ì§€"],
            ["", "", "â€¢ ì™œë„: ë¹„ëŒ€ì¹­ ì •ë„", "â€¢ ê°ì • ë¶„í¬ì˜ ê· í˜•ì„± í‰ê°€"],
            ["", "", "â€¢ ì²¨ë„: ë¾°ì¡±í•¨ ì •ë„", ""],
            ["", "", "", ""],
            ["ğŸ” ì •ê·œì„±ê²€ì •", "ë°ì´í„°ê°€ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥´ëŠ”ì§€ í™•ì¸", "Shapiro-Wilk ê²€ì • ìˆ˜í–‰", "ì ì ˆí•œ í†µê³„ë¶„ì„ ë°©ë²• ì„ íƒì˜ ê¸°ì¤€"],
            ["", "â€¢ í†µê³„ì  ì¶”ë¡ ì˜ ì‹ ë¢°ì„± í™•ë³´", "â€¢ p-ê°’ì´ 0.05ë³´ë‹¤ í¬ë©´ ì •ê·œë¶„í¬", "â€¢ ëª¨ìˆ˜í†µê³„ vs ë¹„ëª¨ìˆ˜í†µê³„ íŒë‹¨"],
            ["", "â€¢ ì´ìƒê°’ê³¼ íŠ¹ì´ íŒ¨í„´ íƒì§€", "â€¢ p-ê°’ì´ 0.05ë³´ë‹¤ ì‘ìœ¼ë©´ ë¹„ì •ê·œë¶„í¬", "â€¢ ë°ì´í„° í’ˆì§ˆ í‰ê°€"],
            ["", "", "", ""],
            ["ğŸ”— ìƒê´€ê´€ê³„", "ê°ì • ìœ í˜•ê³¼ ê°•ë„ ê°„ì˜ ê´€ë ¨ì„± ë¶„ì„", "í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜ ê³„ì‚°", "ê°ì • íŒ¨í„´ì˜ ì¼ê´€ì„± í‰ê°€"],
            ["", "â€¢ ê¸ì •/ë¶€ì • ê°ì •ê³¼ ê°•ë„ì˜ ì—°ê´€ì„±", "â€¢ -1~1 ì‚¬ì´ì˜ ê°’", "â€¢ ê°ì • í‘œí˜„ì˜ ì ì ˆì„± ì§„ë‹¨"],
            ["", "â€¢ ê°ì • ë°˜ì‘ì˜ ì˜ˆì¸¡ ê°€ëŠ¥ì„±", "â€¢ 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ê°•í•œ ì–‘ì˜ ìƒê´€", "â€¢ ì¡°ì§ ë¬¸í™” íŠ¹ì„± íŒŒì•…"],
            ["", "", "â€¢ -1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ê°•í•œ ìŒì˜ ìƒê´€", ""],
            ["", "", "", ""],
            ["ğŸ“ ì‹ ë¢°êµ¬ê°„", "ëª¨ì§‘ë‹¨ í‰ê· ì˜ ì¶”ì • ë²”ìœ„ ì œì‹œ", "95% ì‹ ë¢°êµ¬ê°„ ê³„ì‚°", "ì˜ì‚¬ê²°ì •ì˜ ë¶ˆí™•ì‹¤ì„± ê´€ë¦¬"],
            ["", "â€¢ í‘œë³¸ ë°ì´í„°ë¡œë¶€í„° ì „ì²´ ì¶”ì •", "â€¢ t-ë¶„í¬ ê¸°ë°˜ êµ¬ê°„ ì¶”ì •", "â€¢ ì •ì±… ìˆ˜ë¦½ ì‹œ ê³ ë ¤ì‚¬í•­"],
            ["", "â€¢ í†µê³„ì  ì‹ ë¢°ë„ ì •ëŸ‰í™”", "â€¢ 95% í™•ë¥ ë¡œ ì‹¤ì œê°’ì´ í¬í•¨ë˜ëŠ” êµ¬ê°„", "â€¢ ê°œì„  ëª©í‘œ ì„¤ì •ì˜ ê¸°ì¤€"],
            ["", "", "", ""],
            ["ğŸ¯ ê³ ê¸‰ë©”íŠ¸ë¦­", "ê°ì •ì˜ ë³µì¡ì„±ê³¼ ì•ˆì •ì„± í‰ê°€", "ë‹¤ì–‘í•œ ì§€ìˆ˜ì™€ ì—”íŠ¸ë¡œí”¼ ê³„ì‚°", "ì¡°ì§ ê°ì • ê±´ê°•ë„ ì¢…í•© ì§„ë‹¨"],
            ["", "â€¢ ê°ì • ì¼ê´€ì„±: ì‹œê°„ì— ë”°ë¥¸ ì•ˆì •ì„±", "â€¢ ì„€ë„Œ ì—”íŠ¸ë¡œí”¼: ì •ë³´ëŸ‰ ì¸¡ì •", "â€¢ ê°œì… í•„ìš” ì˜ì—­ ì‹ë³„"],
            ["", "â€¢ ê°ì • ë³µì¡ë„: ë‹¨ìˆœ/ë³µí•© ê°ì • ë¹„ìœ¨", "â€¢ Herfindahl ì§€ìˆ˜: ì§‘ì¤‘ë„ ì¸¡ì •", "â€¢ ì¡°ì§ ê°œë°œ ì „ëµ ìˆ˜ë¦½"],
            ["", "â€¢ ì˜ë£Œ ë§¥ë½ ë‹¤ì–‘ì„±: ì—…ë¬´ ì˜ì—­ë³„ ë¶„í¬", "â€¢ ë³€ë™ ê³„ìˆ˜: ìƒëŒ€ì  ë³€ë™ì„±", "â€¢ ë§ì¶¤í˜• ê°œì„  ë°©ì•ˆ ë„ì¶œ"]
        ]
        
        methodology_df = pd.DataFrame(methodology_data)
        methodology_df.to_excel(writer, sheet_name='ë¶„ì„ë°©ë²•_ì„¤ëª…', index=False, header=False)
        
        # 2. ì§€í‘œ í•´ì„ ê°€ì´ë“œ ì‹œíŠ¸
        interpretation_data = [
            ["ì§€í‘œëª…", "ê°’ì˜ ë²”ìœ„", "í•´ì„ ê¸°ì¤€", "ìš°ìˆ˜ ê¸°ì¤€", "ì£¼ì˜ ê¸°ì¤€", "ì‹¤ì œ í™œìš© ì˜ˆì‹œ"],
            ["", "", "", "", "", ""],
            ["ğŸ¯ ê°ì • ë‹¤ì–‘ì„± ì§€ìˆ˜", "0.0 ~ 1.0", "ê°ì • í‘œí˜„ì˜ í’ë¶€í•¨ ì •ë„", "0.3 ì´ìƒ", "0.1 ì´í•˜", "ë‹¤ì–‘ì„±ì´ ë†’ìœ¼ë©´ ì†”ì§í•œ í”¼ë“œë°± ë¬¸í™”"],
            ["", "", "ë†’ì„ìˆ˜ë¡ ë‹¤ì–‘í•œ ê°ì • í‘œí˜„", "ê· í˜•ì¡íŒ ê°ì • ë¶„í¬", "íšì¼ì  ê°ì • ë°˜ì‘", "ë‹¤ì–‘ì„±ì´ ë‚®ìœ¼ë©´ í‘œí˜„ ì–µì œ ê°€ëŠ¥ì„±"],
            ["", "", "", "", "", ""],
            ["âš–ï¸ í‰ê·  ê°ì • ê·¹ì„±", "-1.0 ~ 1.0", "ì „ì²´ì ì¸ ê°ì •ì˜ ê¸ì •/ë¶€ì • ì •ë„", "0.2 ì´ìƒ", "-0.2 ì´í•˜", "ê·¹ì„±ì´ ë†’ìœ¼ë©´ ê¸ì •ì  ì¡°ì§ ë¬¸í™”"],
            ["", "", "ì–‘ìˆ˜: ê¸ì •ì , ìŒìˆ˜: ë¶€ì •ì ", "ê¸ì •ì  ì—…ë¬´ í™˜ê²½", "ë¶€ì •ì  ì—…ë¬´ í™˜ê²½", "ê·¹ì„±ì´ ë‚®ìœ¼ë©´ ê°œì„  ë…¸ë ¥ í•„ìš”"],
            ["", "", "", "", "", ""],
            ["ğŸ¤ ê°ì • ì‘ì§‘ë„", "0.0 ~ 1.0", "ê°ì • ë°˜ì‘ì˜ ì¼ê´€ì„± ì •ë„", "0.7 ì´ìƒ", "0.3 ì´í•˜", "ì‘ì§‘ë„ê°€ ë†’ìœ¼ë©´ ê³µê°ëŒ€ í˜•ì„±"],
            ["", "", "ë†’ì„ìˆ˜ë¡ ë¹„ìŠ·í•œ ê°ì • íŒ¨í„´", "íŒ€ì›Œí¬ì™€ ë™ì§ˆê°", "ì˜ê²¬ ë¶„ì‚°ê³¼ ê°ˆë“±", "ì‘ì§‘ë„ê°€ ë‚®ìœ¼ë©´ ì†Œí†µ ê°•í™” í•„ìš”"],
            ["", "", "", "", "", ""],
            ["ğŸ“Š ê°•ë„ ì•ˆì •ì„±", "0.0 ~ 1.0", "ê°ì • ê°•ë„ ë³€ë™ì˜ ì•ˆì •ì„±", "0.8 ì´ìƒ", "0.5 ì´í•˜", "ì•ˆì •ì„±ì´ ë†’ìœ¼ë©´ ì˜ˆì¸¡ ê°€ëŠ¥í•œ ë°˜ì‘"],
            ["", "", "ë†’ì„ìˆ˜ë¡ ì¼ì •í•œ ê°ì • í‘œí˜„", "ì•ˆì •ëœ ê°ì • ìƒíƒœ", "ë³€ë™ì„±ì´ í° ê°ì •", "ì•ˆì •ì„±ì´ ë‚®ìœ¼ë©´ ìŠ¤íŠ¸ë ˆìŠ¤ ìš”ì¸ ì ê²€"],
            ["", "", "", "", "", ""],
            ["ğŸ“ˆ ê°ì • ê°•ë„ í‰ê· ", "1.0 ~ 10.0", "ê°ì • í‘œí˜„ì˜ ê°•ë ¬í•¨ ì •ë„", "6.0 ~ 8.0", "4.0 ì´í•˜", "ì ì • ìˆ˜ì¤€: ì ê·¹ì ì´ë©´ì„œ ì•ˆì •ì "],
            ["", "", "ë†’ì„ìˆ˜ë¡ ê°•í•œ ê°ì • ë°˜ì‘", "ì ê·¹ì  ì°¸ì—¬", "ì†Œê·¹ì  ë°˜ì‘", "ë„ˆë¬´ ë†’ìœ¼ë©´ ê³¼ë„í•œ ìŠ¤íŠ¸ë ˆìŠ¤"],
            ["", "", "", "", "", "ë„ˆë¬´ ë‚®ìœ¼ë©´ ë¬´ê´€ì‹¬ ìƒíƒœ"],
            ["", "", "", "", "", ""],
            ["ğŸ² ê°ì • ì—”íŠ¸ë¡œí”¼", "0.0 ~ logâ‚‚(ê°ì •ìˆ˜)", "ê°ì • ë¶„í¬ì˜ ê· ë“±ì„±", "ìµœëŒ€ê°’ì˜ 80%", "ìµœëŒ€ê°’ì˜ 30%", "ë†’ìœ¼ë©´ ë‹¤ì–‘í•˜ê³  ê· í˜•ì¡íŒ ê°ì •"],
            ["", "", "ë†’ì„ìˆ˜ë¡ ê³ ë¥¸ ê°ì • ë¶„í¬", "ê±´ê°•í•œ ê°ì • ë‹¤ì–‘ì„±", "í¸ì¤‘ëœ ê°ì • ë°˜ì‘", "ë‚®ìœ¼ë©´ íŠ¹ì • ê°ì •ì— ì¹˜ìš°ì¹¨"],
            ["", "", "", "", "", ""],
            ["âš¡ ê°ì • ì¼ê´€ì„± ì§€ìˆ˜", "0.0 ~ 1.0", "ì—°ì†ëœ ê°ì •ì˜ ì•ˆì •ì„±", "0.6 ì´ìƒ", "0.3 ì´í•˜", "ì¼ê´€ì„±ì´ ë†’ìœ¼ë©´ ì•ˆì •ëœ ê´€ê³„"],
            ["", "", "ë†’ì„ìˆ˜ë¡ ì˜ˆì¸¡ ê°€ëŠ¥í•œ ë°˜ì‘", "ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” í™˜ê²½", "ì˜ˆì¸¡ ì–´ë ¤ìš´ ë°˜ì‘", "ì¼ê´€ì„±ì´ ë‚®ìœ¼ë©´ ê´€ê³„ ê°œì„  í•„ìš”"],
            ["", "", "", "", "", ""],
            ["ğŸ”„ ê°ì • ë³µì¡ë„ ì§€ìˆ˜", "0.0 ~ 1.0", "ë³µí•© ê°ì •ì˜ ë¹„ìœ¨", "0.2 ~ 0.4", "0.1 ì´í•˜", "ì ì • ë³µì¡ë„: ë¯¸ë¬˜í•œ ê°ì • í‘œí˜„"],
            ["", "", "ë†’ì„ìˆ˜ë¡ ë³µì¡í•œ ê°ì • í˜¼ì¬", "ì„±ìˆ™í•œ ê°ì • í‘œí˜„", "ë‹¨ìˆœí•œ ê°ì •ë§Œ", "ê³¼ë„í•˜ë©´ í˜¼ë€ìŠ¤ëŸ¬ìš´ ìƒíƒœ"],
            ["", "", "", "", "", ""],
            ["ğŸ¥ ì˜ë£Œë§¥ë½ ë‹¤ì–‘ì„±", "0.0 ~ 1.0", "ì—…ë¬´ ì˜ì—­ë³„ ê³ ë¥¸ ë¶„í¬", "0.7 ì´ìƒ", "0.4 ì´í•˜", "ë‹¤ì–‘ì„±ì´ ë†’ìœ¼ë©´ ì „ë°©ìœ„ì  ê°œì„ "],
            ["", "", "ë†’ì„ìˆ˜ë¡ ì „ ì˜ì—­ ê³ ë¥¸ í”¼ë“œë°±", "í¬ê´„ì  ì—…ë¬´ ê´€ì‹¬", "íŠ¹ì • ì˜ì—­ í¸ì¤‘", "ë‹¤ì–‘ì„±ì´ ë‚®ìœ¼ë©´ íŠ¹ì • ì˜ì—­ ì§‘ì¤‘"],
            ["", "", "", "", "", ""],
            ["ğŸ’« ì „ì²´ ì •êµì„±", "0.0 ~ 1.0", "ì¢…í•©ì ì¸ ê°ì • ë¶„ì„ í’ˆì§ˆ", "0.6 ì´ìƒ", "0.3 ì´í•˜", "ì •êµì„±ì´ ë†’ìœ¼ë©´ ì‹ ë¢°í•  ë§Œí•œ ë¶„ì„"],
            ["", "", "ë†’ì„ìˆ˜ë¡ ì •ë°€í•œ ê°ì • ë¶„ì„", "ê³ í’ˆì§ˆ í”¼ë“œë°±", "ë‹¨ìˆœí•œ ê°ì • ë°˜ì‘", "ì •êµì„±ì´ ë‚®ìœ¼ë©´ ë” ì„¸ë°€í•œ ì¡°ì‚¬ í•„ìš”"]
        ]
        
        interpretation_df = pd.DataFrame(interpretation_data)
        interpretation_df.to_excel(writer, sheet_name='ì§€í‘œí•´ì„_ê°€ì´ë“œ', index=False, header=False)
        
        # 3. í™œìš© ë°©ì•ˆ ì•ˆë‚´ ì‹œíŠ¸
        utilization_data = [
            ["í™œìš© ì˜ì—­", "ì£¼ìš” ì§€í‘œ", "íŒë‹¨ ê¸°ì¤€", "ê°œì„  ë°©í–¥", "êµ¬ì²´ì  ì•¡ì…˜"],
            ["", "", "", "", ""],
            ["ğŸ¯ ì¡°ì§ ë¬¸í™” ì§„ë‹¨", "í‰ê·  ê°ì • ê·¹ì„±", "0.2 ì´ìƒì´ë©´ ê¸ì •ì ", "ë¶€ì •ì  ë¬¸í™” ê°œì„ ", "â€¢ ì†Œí†µ ì±„ë„ í™•ëŒ€"],
            ["", "ê°ì • ë‹¤ì–‘ì„± ì§€ìˆ˜", "0.3 ì´ìƒì´ë©´ ê±´ê°•í•¨", "í‘œí˜„ ììœ ë„ ì¦ì§„", "â€¢ ìµëª… í”¼ë“œë°± ì‹œìŠ¤í…œ"],
            ["", "ê°ì • ì‘ì§‘ë„", "0.7 ì´ìƒì´ë©´ í†µí•©ë¨", "íŒ€ì›Œí¬ ê°•í™”", "â€¢ íŒ€ë¹Œë”© í™œë™"],
            ["", "", "", "", ""],
            ["ğŸ“Š ê°œì„  ìš°ì„ ìˆœìœ„", "ê°ì • ê°•ë„ ë¶„í¬", "4.0 ì´í•˜ë©´ ì‹œê¸‰", "ì°¸ì—¬ë„ ì œê³ ", "â€¢ ë™ê¸°ë¶€ì—¬ í”„ë¡œê·¸ë¨"],
            ["", "ë¶€ì • ê°ì • ë¹„ìœ¨", "40% ì´ìƒì´ë©´ ì£¼ì˜", "ë§Œì¡±ë„ í–¥ìƒ", "â€¢ ê³ ì¶© ì²˜ë¦¬ ì‹œìŠ¤í…œ"],
            ["", "ì¬ê²€í†  í•„ìš” í•­ëª©", "20% ì´ìƒì´ë©´ ì ê²€", "í”¼ë“œë°± í’ˆì§ˆ í–¥ìƒ", "â€¢ ì„¤ë¬¸ ë°©ì‹ ê°œì„ "],
            ["", "", "", "", ""],
            ["ğŸ” ì„¸ë¶€ ì˜ì—­ ë¶„ì„", "ì˜ë£Œë§¥ë½ ë‹¤ì–‘ì„±", "ì˜ì—­ë³„ ê· í˜• í™•ì¸", "ì „ë°©ìœ„ì  ê°œì„ ", "â€¢ ì˜ì—­ë³„ ë§ì¶¤ êµìœ¡"],
            ["", "í‚¤ì›Œë“œ ë¹ˆë„", "ì£¼ìš” ì´ìŠˆ íŒŒì•…", "í•µì‹¬ ë¬¸ì œ í•´ê²°", "â€¢ ì›Œë“œí´ë¼ìš°ë“œ ì‹œê°í™”"],
            ["", "ê°ì • ë³µì¡ë„", "0.4 ì´ìƒì´ë©´ ë³µì¡", "ëª…í™•í•œ ì†Œí†µ", "â€¢ ì˜ì‚¬ì†Œí†µ êµìœ¡"],
            ["", "", "", "", ""],
            ["ğŸ“ˆ ëª¨ë‹ˆí„°ë§ ì²´ê³„", "ê°ì • ì¼ê´€ì„±", "ì‹œê°„ë³„ ë³€í™” ì¶”ì ", "ì§€ì†ì  ê°œì„ ", "â€¢ ì •ê¸° ì„¤ë¬¸ ì‹¤ì‹œ"],
            ["", "ì•ˆì •ì„± ì§€ìˆ˜", "ê³„ì ˆë³„/ë¶„ê¸°ë³„ ë¹„êµ", "ì˜ˆë°©ì  ê´€ë¦¬", "â€¢ íŠ¸ë Œë“œ ë¶„ì„ ëŒ€ì‹œë³´ë“œ"],
            ["", "ì‹ ë¢°êµ¬ê°„", "í†µê³„ì  ìœ ì˜ì„± í™•ì¸", "ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì˜ì‚¬ê²°ì •", "â€¢ í‘œë³¸ í¬ê¸° ìµœì í™”"],
            ["", "", "", "", ""],
            ["ğŸ­ ê°ì •ë³„ ë§ì¶¤ ëŒ€ì‘", "ì£¼ìš” ê°ì • ë¶„í¬", "'ê°ì‚¬' ë§ìœ¼ë©´ â†’ ì¸ì • ë¬¸í™”", "ê¸ì • ê°ì • ê°•í™”", "â€¢ ì¹­ì°¬ ì œë„ í™•ëŒ€"],
            ["", "", "'ì‹¤ë§' ë§ìœ¼ë©´ â†’ ê¸°ëŒ€ ê´€ë¦¬", "ë¶€ì • ê°ì • í•´ì†Œ", "â€¢ ê¸°ëŒ€ì¹˜ ëª…í™•í™”"],
            ["", "", "'ë¶ˆì•ˆ' ë§ìœ¼ë©´ â†’ ì•ˆì •ì„± ì œê³µ", "ì‹¬ë¦¬ì  ì•ˆì „ê°", "â€¢ ë³€í™” ê´€ë¦¬ í”„ë¡œê·¸ë¨"],
            ["", "", "", "", ""],
            ["ğŸ† ì„±ê³¼ ì¸¡ì •", "í’ˆì§ˆ ì ìˆ˜", "8.0 ì´ìƒì´ë©´ ìš°ìˆ˜", "ë¶„ì„ ì‹ ë¢°ë„ í–¥ìƒ", "â€¢ AI ëª¨ë¸ ì •êµí™”"],
            ["", "ì „ì²´ ì •êµì„±", "0.6 ì´ìƒì´ë©´ ì–‘í˜¸", "ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬", "â€¢ ì „ì²˜ë¦¬ ê³¼ì • ê°œì„ "],
            ["", "ì •ê·œì„± ê²€ì •", "p>0.05ë©´ ì •ê·œë¶„í¬", "ì ì ˆí•œ ë¶„ì„ ë°©ë²•", "â€¢ ë¹„ëª¨ìˆ˜ í†µê³„ ê³ ë ¤"],
            ["", "", "", "", ""],
            ["ğŸš€ ì „ëµì  í™œìš©", "ë²¡í„° ì¤‘ì‹¬ì ", "ì¡°ì§ì˜ ê°ì • ë¬´ê²Œì¤‘ì‹¬", "ì¡°ì§ ì •ì²´ì„± ê°•í™”", "â€¢ í•µì‹¬ ê°€ì¹˜ ì¬ì •ë¦½"],
            ["", "ê·¹ì„± í‘œì¤€í¸ì°¨", "ê°ì • ë¶„ì‚° ì •ë„", "í†µí•©ëœ ì¡°ì§ ë¬¸í™”", "â€¢ ê³µí†µ ëª©í‘œ ì„¤ì •"],
            ["", "ì—”íŠ¸ë¡œí”¼ ë¹„ìœ¨", "ì •ë³´ í™œìš©ë„", "íš¨ê³¼ì  í”¼ë“œë°± ì‹œìŠ¤í…œ", "â€¢ ë§ì¶¤í˜• ê°œì„  ê³„íš"]
        ]
        
        utilization_df = pd.DataFrame(utilization_data)
        utilization_df.to_excel(writer, sheet_name='í™œìš©ë°©ì•ˆ_ì•ˆë‚´', index=False, header=False)
        
        # 4. ì‹¤ì œ ê²°ê³¼ í•´ì„ ì˜ˆì‹œ ì‹œíŠ¸ (í˜„ì¬ ë¶„ì„ ê²°ê³¼ ê¸°ë°˜)
        if vector_analysis and statistical_results and advanced_metrics:
            example_data = [
                ["ğŸ“‹ í˜„ì¬ ë¶„ì„ ê²°ê³¼ í•´ì„ ì˜ˆì‹œ", "", "", ""],
                ["", "", "", ""],
                ["ì§€í‘œ", "ì¸¡ì •ê°’", "í•´ì„", "ê¶Œì¥ ì¡°ì¹˜"],
                ["", "", "", ""],
                ["ğŸ¯ ê°ì • ë‹¤ì–‘ì„± ì§€ìˆ˜", f"{vector_analysis.get('emotion_diversity', 0):.3f}", 
                 "ë‚®ìŒ" if vector_analysis.get('emotion_diversity', 0) < 0.3 else "ë³´í†µ" if vector_analysis.get('emotion_diversity', 0) < 0.6 else "ë†’ìŒ",
                 "í‘œí˜„ì˜ ììœ ë„ í–¥ìƒ í•„ìš”" if vector_analysis.get('emotion_diversity', 0) < 0.3 else "ì–‘í˜¸í•œ ìˆ˜ì¤€"],
                 
                ["âš–ï¸ í‰ê·  ê°ì • ê·¹ì„±", f"{vector_analysis.get('avg_polarity', 0):.3f}",
                 "ë¶€ì •ì " if vector_analysis.get('avg_polarity', 0) < -0.2 else "ì¤‘ë¦½ì " if vector_analysis.get('avg_polarity', 0) < 0.2 else "ê¸ì •ì ",
                 "ê¸ì • ë¬¸í™” ì¡°ì„± í•„ìš”" if vector_analysis.get('avg_polarity', 0) < 0 else "ê¸ì •ì  ë¬¸í™” ìœ ì§€"],
                 
                ["ğŸ¤ ê°ì • ì‘ì§‘ë„", f"{vector_analysis.get('emotion_cohesion', 0):.3f}",
                 "ë‚®ìŒ" if vector_analysis.get('emotion_cohesion', 0) < 0.3 else "ë³´í†µ" if vector_analysis.get('emotion_cohesion', 0) < 0.7 else "ë†’ìŒ",
                 "íŒ€ì›Œí¬ ê°•í™” í”„ë¡œê·¸ë¨" if vector_analysis.get('emotion_cohesion', 0) < 0.5 else "í˜„ì¬ ìˆ˜ì¤€ ìœ ì§€"],
                 
                ["ğŸ“Š ê°ì • ê°•ë„ í‰ê· ", f"{statistical_results.get('descriptive_stats', {}).get('intensity', {}).get('mean', 0):.2f}",
                 "ë‚®ìŒ" if statistical_results.get('descriptive_stats', {}).get('intensity', {}).get('mean', 0) < 4 else 
                 "ì ì •" if statistical_results.get('descriptive_stats', {}).get('intensity', {}).get('mean', 0) < 8 else "ë†’ìŒ",
                 "ì°¸ì—¬ ìœ ë„ í•„ìš”" if statistical_results.get('descriptive_stats', {}).get('intensity', {}).get('mean', 0) < 5 else "ì ì ˆí•œ ìˆ˜ì¤€"],
                 
                ["âš¡ ê°ì • ì¼ê´€ì„±", f"{advanced_metrics.get('emotion_consistency_index', 0):.3f}",
                 "ë‚®ìŒ" if advanced_metrics.get('emotion_consistency_index', 0) < 0.3 else "ë³´í†µ" if advanced_metrics.get('emotion_consistency_index', 0) < 0.6 else "ë†’ìŒ",
                 "ê´€ê³„ ì•ˆì •ì„± í–¥ìƒ" if advanced_metrics.get('emotion_consistency_index', 0) < 0.5 else "ì•ˆì •ì  ê´€ê³„ ìœ ì§€"],
                 
                ["ğŸ”„ ê°ì • ë³µì¡ë„", f"{advanced_metrics.get('emotion_complexity_index', 0):.3f}",
                 "ë‹¨ìˆœ" if advanced_metrics.get('emotion_complexity_index', 0) < 0.2 else "ì ì •" if advanced_metrics.get('emotion_complexity_index', 0) < 0.4 else "ë³µì¡",
                 "ê°ì • í‘œí˜„ ë‹¤ì–‘í™”" if advanced_metrics.get('emotion_complexity_index', 0) < 0.2 else "í˜„ì¬ ìˆ˜ì¤€ ì ì ˆ"],
                 
                ["ğŸ’« ì „ì²´ ì •êµì„±", f"{advanced_metrics.get('overall_sophistication', 0):.3f}",
                 "ë‚®ìŒ" if advanced_metrics.get('overall_sophistication', 0) < 0.3 else "ë³´í†µ" if advanced_metrics.get('overall_sophistication', 0) < 0.6 else "ë†’ìŒ",
                 "í”¼ë“œë°± í’ˆì§ˆ í–¥ìƒ" if advanced_metrics.get('overall_sophistication', 0) < 0.5 else "ê³ í’ˆì§ˆ í”¼ë“œë°± ìœ ì§€"],
                 
                ["", "", "", ""],
                ["ğŸ“Š ì¢…í•© ì§„ë‹¨", "", "", ""],
                ["", "â€¢ í˜„ì¬ ì¡°ì§ì˜ ê°ì • ìƒíƒœë¥¼ ì¢…í•©ì ìœ¼ë¡œ í‰ê°€í•œ ê²°ê³¼ì…ë‹ˆë‹¤.", "", ""],
                ["", "â€¢ ê° ì§€í‘œë¥¼ í†µí•´ ê°•ì ê³¼ ê°œì„ ì ì„ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.", "", ""],
                ["", "â€¢ ì •ê¸°ì ì¸ ëª¨ë‹ˆí„°ë§ì„ í†µí•´ ë³€í™” ì¶”ì´ë¥¼ ê´€ì°°í•˜ì„¸ìš”.", "", ""],
                ["", "â€¢ êµ¬ì²´ì ì¸ ê°œì„  ë°©ì•ˆì€ 'í™œìš©ë°©ì•ˆ_ì•ˆë‚´' ì‹œíŠ¸ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.", "", ""]
            ]
            
            example_df = pd.DataFrame(example_data)
            example_df.to_excel(writer, sheet_name='ê²°ê³¼í•´ì„_ì˜ˆì‹œ', index=False, header=False)
    
    def extract_keywords(self, texts: list, min_length: int = 2, exclude_words: set = None) -> dict:
        """
        í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ê³  ë¹ˆë„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
        
        Args:
            texts: ë¶„ì„í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            min_length: ìµœì†Œ í‚¤ì›Œë“œ ê¸¸ì´
            exclude_words: ì œì™¸í•  ë‹¨ì–´ ì§‘í•©
            
        Returns:
            dict: í‚¤ì›Œë“œ ë¹ˆë„ ë”•ì…”ë„ˆë¦¬
        """
        if exclude_words is None:
            # ì¼ë°˜ì ì¸ ë¶ˆìš©ì–´ ì„¤ì •
            exclude_words = {
                'ì´', 'ê°€', 'ì€', 'ëŠ”', 'ì´ë‹¤', 'ìˆë‹¤', 'í•˜ë‹¤', 'ë˜ë‹¤', 'ì•„ë‹ˆë‹¤',
                'ê·¸', 'ê·¸ë…€', 'ê·¸ë“¤', 'ìš°ë¦¬', 'ì €í¬', 'ë„ˆí¬', 'ìì‹ ', 'ëˆ„êµ°ê°€',
                'ë¬´ì—‡', 'ì–¸ì œ', 'ì–´ë””', 'ì–´ë–»ê²Œ', 'ì™œ', 'ë§¤ìš°', 'ì¡°ê¸ˆ', 'ì¡°ê¸ˆ',
                'ë„ˆë¬´', 'ì •ë§', 'ë§¤ìš°', 'ì¢‹ë‹¤', 'ë‚˜ì˜ë‹¤', 'ë‚˜', 'ì €', 'ì „', 'í›„',
                'ì—¬ê¸°', 'ê±°ê¸°', 'ì €ê¸°', 'ì§€ê¸ˆ', 'ì˜¤ëŠ˜', 'ì–´ì œ', 'ë‚´ì¼', 'ì´ë²ˆ',
                'ì„ ìƒë‹˜', 'ì§ì›ë¶„', 'ë‹´ë‹¹ì', 'ê´€ë¦¬ì', 'ê°„í˜¸ì‚¬', 'ì˜ì‚¬'
            }
        
        all_keywords = []
        
        # ëª¨ë“  í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
        for text in texts:
            if not text or pd.isna(text):
                continue
                
            # í•œê¸€, ì˜ì–´, ìˆ«ìë§Œ ìœ ì§€
            clean_text = re.sub(r'[^\w\s\uac00-\ud7a3]', ' ', str(text))
            
            # ë‹¨ì–´ ë¶„ë¦¬
            words = clean_text.split()
            
            for word in words:
                word = word.strip()
                # ê¸¸ì´ ë° ë¹ˆ ë¬¸ìì—´ ê²€ì‚¬
                if len(word) >= min_length and word not in exclude_words:
                    all_keywords.append(word)
        
        # ë¹ˆë„ ê³„ì‚°
        keyword_freq = Counter(all_keywords)
        
        return dict(keyword_freq.most_common(50))  # ìƒìœ„ 50ê°œ ë°˜í™˜
    
    def calculate_emotion_vectors(self, results: list) -> dict:
        """
        ê°ì • ë°ì´í„°ë¥¼ ë²¡í„° ê³µê°„ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ìˆ˜í•™ì  ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        
        Args:
            results: ë¶„ì„ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            dict: ë²¡í„° ê³µê°„ ë¶„ì„ ê²°ê³¼
        """
        # ê°ì •ì„ ìˆ«ì ë²¡í„°ë¡œ ë§¤í•‘
        emotion_mapping = {
            "ê¸°ì¨": [1, 0, 0, 0, 0, 0, 0, 0],     # ê¸ì •êµ°
            "ê°ì‚¬": [0, 1, 0, 0, 0, 0, 0, 0],
            "ì‹ ë¢°": [0, 0, 1, 0, 0, 0, 0, 0], 
            "ë§Œì¡±": [0, 0, 0, 1, 0, 0, 0, 0],
            "ë¶„ë…¸": [0, 0, 0, 0, 1, 0, 0, 0],     # ë¶€ì •êµ°
            "ìŠ¬í””": [0, 0, 0, 0, 0, 1, 0, 0],
            "ë‘ë ¤ì›€": [0, 0, 0, 0, 0, 0, 1, 0],
            "ì‹¤ë§": [0, 0, 0, 0, 0, 0, 0, 1],
            "í‰ì˜¨": [0, 0, 0, 0, 0, 0, 0, 0],     # ì¤‘ë¦½êµ° (ì›ì )
            "ë¬´ê´€ì‹¬": [0, 0, 0, 0, 0, 0, 0, 0]
        }
        
        # ê°ì • ë²¡í„° ìƒì„±
        emotion_vectors = []
        emotion_intensities = []
        
        for result in results:
            primary = result.get('primary_emotion', 'í‰ì˜¨')
            secondary = result.get('secondary_emotion')
            intensity = result.get('emotion_intensity', 5)
            emotion_mix = result.get('emotion_mix', {})
            
            if primary in emotion_mapping:
                # ë³µí•© ê°ì •ì¸ ê²½ìš° ê°€ì¤‘ í‰ê·  ë²¡í„° ìƒì„±
                if secondary and emotion_mix:
                    primary_weight = emotion_mix.get(primary, 1.0)
                    secondary_weight = emotion_mix.get(secondary, 0.0)
                    
                    primary_vec = np.array(emotion_mapping[primary]) * primary_weight
                    secondary_vec = np.array(emotion_mapping.get(secondary, [0]*8)) * secondary_weight
                    
                    emotion_vector = primary_vec + secondary_vec
                else:
                    emotion_vector = np.array(emotion_mapping[primary])
                
                # ê°•ë„ë¡œ ë²¡í„° í¬ê¸° ì¡°ì •
                emotion_vector = emotion_vector * (intensity / 10.0)
                emotion_vectors.append(emotion_vector)
                emotion_intensities.append(intensity)
        
        if not emotion_vectors:
            return {}
        
        emotion_matrix = np.array(emotion_vectors)
        
        # 1. ì¤‘ì‹¬ ë²¡í„° (í‰ê·  ê°ì • ë²¡í„°)
        centroid = np.mean(emotion_matrix, axis=0)
        
        # 2. ê°ì • ë‹¤ì–‘ì„± (ë²¡í„° ë¶„ì‚°)
        emotion_variance = np.var(emotion_matrix, axis=0)
        emotion_diversity = np.mean(emotion_variance)
        
        # 3. ê°ì • ê·¹ì„± (ê¸ì •-ë¶€ì • ì¶•)
        positive_axis = np.sum(emotion_matrix[:, :4], axis=1)  # ê¸ì • ê°ì • í•©
        negative_axis = np.sum(emotion_matrix[:, 4:8], axis=1)  # ë¶€ì • ê°ì • í•©
        polarity_scores = positive_axis - negative_axis
        avg_polarity = np.mean(polarity_scores)
        polarity_std = np.std(polarity_scores)
        
        # 4. ê°ì • ì‘ì§‘ë„ (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜)
        if len(emotion_vectors) > 1:
            similarity_matrix = cosine_similarity(emotion_matrix)
            # ëŒ€ê°ì„  ì œì™¸í•œ í‰ê·  ìœ ì‚¬ë„
            mask = ~np.eye(similarity_matrix.shape[0], dtype=bool)
            avg_similarity = np.mean(similarity_matrix[mask])
        else:
            avg_similarity = 1.0
        
        # 5. ê°ì • ì•ˆì •ì„± (ê°•ë„ ë³€ë™ ê³„ìˆ˜)
        intensity_cv = np.std(emotion_intensities) / np.mean(emotion_intensities) if np.mean(emotion_intensities) > 0 else 0
        
        return {
            "centroid_vector": centroid.tolist(),
            "emotion_diversity": float(emotion_diversity),
            "avg_polarity": float(avg_polarity),
            "polarity_std": float(polarity_std),
            "emotion_cohesion": float(avg_similarity),
            "intensity_stability": float(1 - intensity_cv),  # ë†’ì„ìˆ˜ë¡ ì•ˆì •ì 
            "vector_magnitude": float(np.linalg.norm(centroid)),
            "emotion_distribution": {
                "positive_ratio": float(np.mean(positive_axis > 0)),
                "negative_ratio": float(np.mean(negative_axis > 0)),
                "neutral_ratio": float(np.mean((positive_axis == 0) & (negative_axis == 0)))
            }
        }
    
    def statistical_analysis(self, results: list) -> dict:
        """
        ê°ì • ë°ì´í„°ì— ëŒ€í•œ í†µê³„ì  ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        
        Args:
            results: ë¶„ì„ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            dict: í†µê³„ ë¶„ì„ ê²°ê³¼
        """
        if not results:
            return {}
        
        # ë°ì´í„° ì¶”ì¶œ
        emotions = [r.get('primary_emotion', 'í‰ì˜¨') for r in results]
        intensities = [r.get('emotion_intensity', 5) for r in results]
        confidences = [r.get('confidence_score', 5) for r in results]
        complexities = [r.get('emotional_complexity', 'ë‹¨ìˆœ') for r in results]
        
        # 1. ê¸°ìˆ  í†µê³„ëŸ‰
        intensity_stats = stats.describe(intensities)
        confidence_stats = stats.describe(confidences)
        
        # 2. ë¶„í¬ ì •ê·œì„± ê²€ì • (Shapiro-Wilk)
        if len(intensities) >= 3:
            intensity_normality = stats.shapiro(intensities)
            confidence_normality = stats.shapiro(confidences)
        else:
            intensity_normality = (0.0, 1.0)
            confidence_normality = (0.0, 1.0)
        
        # 3. ê°ì •ê³¼ ê°•ë„ ê°„ ìƒê´€ê´€ê³„
        emotion_intensity_corr = self._calculate_emotion_intensity_correlation(emotions, intensities)
        
        # 4. ì‹ ë¢°êµ¬ê°„ ê³„ì‚° (95%)
        intensity_ci = stats.t.interval(0.95, len(intensities)-1, 
                                       loc=np.mean(intensities), 
                                       scale=stats.sem(intensities))
        confidence_ci = stats.t.interval(0.95, len(confidences)-1,
                                        loc=np.mean(confidences),
                                        scale=stats.sem(confidences))
        
        # 5. ê°ì • ì—”íŠ¸ë¡œí”¼ ê³„ì‚°
        emotion_counts = Counter(emotions)
        total = len(emotions)
        emotion_entropy = -sum((count/total) * math.log2(count/total) 
                              for count in emotion_counts.values())
        
        # 6. ë³µí•©ì„± ë¹„ìœ¨
        complex_ratio = emotions.count('ë³µí•©') / len(complexities) if complexities else 0
        
        # 7. ì´ìƒê°’ íƒì§€ (IQR ë°©ë²•)
        q1_intensity = np.percentile(intensities, 25)
        q3_intensity = np.percentile(intensities, 75)
        iqr_intensity = q3_intensity - q1_intensity
        intensity_outliers = [x for x in intensities 
                             if x < q1_intensity - 1.5*iqr_intensity or x > q3_intensity + 1.5*iqr_intensity]
        
        return {
            "descriptive_stats": {
                "intensity": {
                    "mean": float(intensity_stats.mean),
                    "variance": float(intensity_stats.variance),
                    "skewness": float(intensity_stats.skewness),
                    "kurtosis": float(intensity_stats.kurtosis),
                    "min": float(intensity_stats.minmax[0]),
                    "max": float(intensity_stats.minmax[1]),
                    "std": float(np.std(intensities)),
                    "median": float(np.median(intensities)),
                    "q1": float(q1_intensity),
                    "q3": float(q3_intensity)
                },
                "confidence": {
                    "mean": float(confidence_stats.mean),
                    "variance": float(confidence_stats.variance),
                    "std": float(np.std(confidences)),
                    "median": float(np.median(confidences))
                }
            },
            "normality_tests": {
                "intensity_shapiro": {
                    "statistic": float(intensity_normality[0]),
                    "p_value": float(intensity_normality[1]),
                    "is_normal": intensity_normality[1] > 0.05
                },
                "confidence_shapiro": {
                    "statistic": float(confidence_normality[0]),
                    "p_value": float(confidence_normality[1]),
                    "is_normal": confidence_normality[1] > 0.05
                }
            },
            "correlations": emotion_intensity_corr,
            "confidence_intervals": {
                "intensity_95ci": [float(intensity_ci[0]), float(intensity_ci[1])],
                "confidence_95ci": [float(confidence_ci[0]), float(confidence_ci[1])]
            },
            "information_theory": {
                "emotion_entropy": float(emotion_entropy),
                "max_entropy": float(math.log2(len(set(emotions)))),
                "entropy_ratio": float(emotion_entropy / math.log2(len(set(emotions)))) if len(set(emotions)) > 1 else 0
            },
            "complexity_analysis": {
                "complex_ratio": float(complex_ratio),
                "simple_ratio": float(1 - complex_ratio)
            },
            "outlier_detection": {
                "intensity_outliers": len(intensity_outliers),
                "outlier_ratio": float(len(intensity_outliers) / len(intensities))
            }
        }
    
    def _calculate_emotion_intensity_correlation(self, emotions: list, intensities: list) -> dict:
        """ê°ì • ì¹´í…Œê³ ë¦¬ì™€ ê°•ë„ ê°„ì˜ ìƒê´€ê´€ê³„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
        # ê°ì •ì„ ìˆ«ìë¡œ ë§¤í•‘
        emotion_scores = []
        for emotion in emotions:
            if emotion in ["ê¸°ì¨", "ê°ì‚¬", "ì‹ ë¢°", "ë§Œì¡±"]:
                emotion_scores.append(1)  # ê¸ì •
            elif emotion in ["ë¶„ë…¸", "ìŠ¬í””", "ë‘ë ¤ì›€", "ì‹¤ë§"]:
                emotion_scores.append(-1)  # ë¶€ì •
            else:
                emotion_scores.append(0)  # ì¤‘ë¦½
        
        if len(set(emotion_scores)) > 1:
            correlation, p_value = stats.pearsonr(emotion_scores, intensities)
            return {
                "pearson_r": float(correlation),
                "p_value": float(p_value),
                "significant": p_value < 0.05
            }
        else:
            return {"pearson_r": 0.0, "p_value": 1.0, "significant": False}
    
    def advanced_emotion_metrics(self, results: list) -> dict:
        """
        ê³ ê¸‰ ê°ì • ë¶„ì„ ë©”íŠ¸ë¦­ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
        
        Args:
            results: ë¶„ì„ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            dict: ê³ ê¸‰ ë©”íŠ¸ë¦­ ê²°ê³¼
        """
        if not results:
            return {}
        
        # ë°ì´í„° ì¶”ì¶œ
        primary_emotions = [r.get('primary_emotion', 'í‰ì˜¨') for r in results]
        secondary_emotions = [r.get('secondary_emotion') for r in results]
        emotion_mixes = [r.get('emotion_mix', {}) for r in results]
        medical_contexts = [r.get('medical_context', []) for r in results]
        
        # 1. ê°ì • ì¼ê´€ì„± ì§€ìˆ˜ (Emotion Consistency Index)
        consistency_scores = []
        for i in range(len(results)-1):
            curr_emotion = primary_emotions[i]
            next_emotion = primary_emotions[i+1]
            
            # ê°™ì€ ê°ì •êµ°ì¸ì§€ í™•ì¸
            same_group = self._same_emotion_group(curr_emotion, next_emotion)
            consistency_scores.append(1.0 if same_group else 0.0)
        
        emotion_consistency = np.mean(consistency_scores) if consistency_scores else 0.0
        
        # 2. ê°ì • ë³µì¡ë„ ì§€ìˆ˜ (Emotion Complexity Index)
        complexity_scores = []
        for emotion_mix in emotion_mixes:
            if emotion_mix and len(emotion_mix) > 1:
                # ì„€ë„Œ ì—”íŠ¸ë¡œí”¼ ê¸°ë°˜ ë³µì¡ë„
                values = list(emotion_mix.values())
                entropy = -sum(v * math.log2(v) for v in values if v > 0)
                complexity_scores.append(entropy)
            else:
                complexity_scores.append(0.0)
        
        avg_complexity = np.mean(complexity_scores) if complexity_scores else 0.0
        
        # 3. ì˜ë£Œ ë§¥ë½ ë‹¤ì–‘ì„± ì§€ìˆ˜
        all_contexts = []
        for contexts in medical_contexts:
            if isinstance(contexts, list):
                all_contexts.extend(contexts)
        
        if all_contexts:
            context_counts = Counter(all_contexts)
            total_contexts = len(all_contexts)
            context_entropy = -sum((count/total_contexts) * math.log2(count/total_contexts) 
                                  for count in context_counts.values())
            max_context_entropy = math.log2(len(context_counts))
            context_diversity = context_entropy / max_context_entropy if max_context_entropy > 0 else 0
        else:
            context_diversity = 0.0
        
        # 4. ê°ì • ë³€ë™ì„± ì§€ìˆ˜ (Emotional Volatility Index)
        intensities = [r.get('emotion_intensity', 5) for r in results]
        if len(intensities) > 1:
            # ì—°ì†ëœ ê°•ë„ ì°¨ì´ì˜ í‰ê· 
            intensity_changes = [abs(intensities[i+1] - intensities[i]) for i in range(len(intensities)-1)]
            volatility = np.mean(intensity_changes) / 10.0  # ì •ê·œí™” (0-1)
        else:
            volatility = 0.0
        
        # 5. ê°ì • ì§‘ì¤‘ë„ ì§€ìˆ˜ (Emotion Concentration Index)
        emotion_counts = Counter(primary_emotions)
        total_emotions = len(primary_emotions)
        # Herfindahl-Hirschman Index ë³€í˜•
        concentration = sum((count/total_emotions)**2 for count in emotion_counts.values())
        
        return {
            "emotion_consistency_index": float(emotion_consistency),
            "emotion_complexity_index": float(avg_complexity),
            "medical_context_diversity": float(context_diversity),
            "emotional_volatility_index": float(volatility),
            "emotion_concentration_index": float(concentration),
            "stability_score": float(1 - volatility),  # ë³€ë™ì„±ì˜ ì—­ìˆ˜
            "balance_score": float(1 - concentration),  # ì§‘ì¤‘ë„ì˜ ì—­ìˆ˜
            "overall_sophistication": float((avg_complexity + context_diversity) / 2)
        }
    
    def _same_emotion_group(self, emotion1: str, emotion2: str) -> bool:
        """ë‘ ê°ì •ì´ ê°™ì€ ê·¸ë£¹ì— ì†í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
        positive = ["ê¸°ì¨", "ê°ì‚¬", "ì‹ ë¢°", "ë§Œì¡±"]
        negative = ["ë¶„ë…¸", "ìŠ¬í””", "ë‘ë ¤ì›€", "ì‹¤ë§"]
        neutral = ["í‰ì˜¨", "ë¬´ê´€ì‹¬"]
        
        if emotion1 in positive and emotion2 in positive:
            return True
        elif emotion1 in negative and emotion2 in negative:
            return True
        elif emotion1 in neutral and emotion2 in neutral:
            return True
        else:
            return False
    
    def validate_analysis_quality(self, result: dict, original_text: str) -> dict:
        """
        AI ë¶„ì„ ê²°ê³¼ì˜ í’ˆì§ˆì„ ê²€ì¦í•˜ê³  ì‹ ë¢°ë„ë¥¼ í‰ê°€í•©ë‹ˆë‹¤.
        
        Args:
            result: AI ë¶„ì„ ê²°ê³¼
            original_text: ì›ë³¸ í…ìŠ¤íŠ¸
            
        Returns:
            dict: í’ˆì§ˆ ê²€ì¦ ê²°ê³¼
        """
        quality_issues = []
        overall_confidence = result.get('confidence_score', 5)
        
        # 1. ê¸°ë³¸ í•„ë“œ ì™„ì„±ë„ ê²€ì‚¬
        required_fields = ['refined_text', 'sentiment', 'sentiment_intensity', 'labels']
        missing_fields = [field for field in required_fields if field not in result or not result[field]]
        
        if missing_fields:
            # í•„ìˆ˜ í•„ë“œ ëˆ„ë½ ì •ë³´ëŠ” ì¶”ê°€í•˜ì§€ ì•ŠìŒ (ê³µë€ ì²˜ë¦¬)
            overall_confidence -= 3
        
        # 2. ê°ì • ê°•ë„ì™€ ê°ì • ë¶„ë¥˜ ì¼ì¹˜ì„± ê²€ì‚¬
        sentiment = result.get('sentiment', '')
        intensity = result.get('sentiment_intensity', 5)
        
        if sentiment == 'ê¸ì •' and intensity < 6:
            quality_issues.append("ê¸ì • ê°ì •ì¸ë° ê°•ë„ê°€ ë‚®ìŒ")
            overall_confidence -= 2
        elif sentiment == 'ë¶€ì •' and intensity > 5:
            quality_issues.append("ë¶€ì • ê°ì •ì¸ë° ê°•ë„ê°€ ë†’ìŒ")
            overall_confidence -= 2
        
        # 3. í…ìŠ¤íŠ¸ ê¸¸ì´ ë¹„êµ
        original_len = len(original_text.strip()) if original_text else 0
        refined_len = len(result.get('refined_text', '').strip())
        
        if original_len > 10 and refined_len < original_len * 0.3:
            quality_issues.append("ì •ì œëœ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìŒ")
            overall_confidence -= 2
        elif refined_len > original_len * 2:
            quality_issues.append("ì •ì œëœ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¹€")
            overall_confidence -= 1
        
        # 4. í‚¤ì›Œë“œ ì¶”ì¶œ í’ˆì§ˆ ê²€ì‚¬
        key_terms = result.get('key_terms', [])
        if len(original_text.strip()) > 20 and len(key_terms) == 0:
            quality_issues.append("í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨")
            overall_confidence -= 2
        
        # 5. ë¹„ì‹ë³„ ì²˜ë¦¬ ì¼ì¹˜ì„± ê²€ì‚¬
        is_anonymized = result.get('is_anonymized', False)
        refined_text = result.get('refined_text', '')
        
        # ì‹¤ëª… íŒ¨í„´ ê²€ì‚¬ (ê°„ë‹¨í•œ í•œê¸€ ì´ë¦„ íŒ¨í„´)
        name_pattern = r'[\uac00-\ud7a3]{2,3}\s*ì„ ìƒë‹˜|[\uac00-\ud7a3]{2,3}\s*ê³¼ì¥|[\uac00-\ud7a3]{2,3}\s*íŒ€ì¥|[\uac00-\ud7a3]{2,3}\s*ëŒ€ë¦¬'
        
        if is_anonymized and re.search(name_pattern, refined_text):
            quality_issues.append("ë¹„ì‹ë³„ ì²˜ë¦¬ ë¶ˆì™„ì „ - ì‹¤ëª… ì”ì¡´")
            overall_confidence -= 3
        
        # ìµœì¢… ì‹ ë¢°ë„ ì¡°ì •
        overall_confidence = max(1, min(10, overall_confidence))
        
        return {
            'quality_score': overall_confidence,
            'issues': quality_issues,
            'needs_review': overall_confidence < 6 or len(quality_issues) > 2,
            'is_reliable': overall_confidence >= 7 and len(quality_issues) <= 1
        }
    
    def retry_low_quality_analysis(self, texts: list, results: list, quality_results: list, max_retries: int = 2) -> tuple:
        """
        ë‚®ì€ í’ˆì§ˆì˜ ë¶„ì„ ê²°ê³¼ë¥¼ ì¬ë¶„ì„í•©ë‹ˆë‹¤.
        
        Args:
            texts: ì›ë³¸ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            results: ë¶„ì„ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            quality_results: í’ˆì§ˆ ê²€ì¦ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
            
        Returns:
            tuple: (ê°œì„ ëœ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸, ê°œì„ ëœ í’ˆì§ˆ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸)
        """
        improved_results = results.copy()
        improved_quality = quality_results.copy()
        
        # ì¬ê²€í† ê°€ í•„ìš”í•œ í•­ëª©ë“¤ ì°¾ê¸°
        retry_indices = [i for i, q in enumerate(quality_results) if q['needs_review']]
        
        if not retry_indices:
            print("ì¬ê²€í† ê°€ í•„ìš”í•œ í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
            return improved_results, improved_quality
        
        print(f"\n{len(retry_indices)}ê°œ í•­ëª©ì„ ì¬ë¶„ì„í•©ë‹ˆë‹¤...")
        
        retry_count = 0
        total_improved = 0
        
        with tqdm(total=len(retry_indices), desc="ì¬ë¶„ì„ ì§„í–‰ë¥ ", unit="ê±´") as pbar:
            for idx in retry_indices:
                if retry_count >= max_retries:
                    break
                
                original_text = texts[idx]
                original_quality = quality_results[idx]['quality_score']
                
                # ì¬ë¶„ì„ ìˆ˜í–‰
                new_result = self.analyze_review(original_text)
                new_quality = self.validate_analysis_quality(new_result, original_text)
                
                # ê°œì„ ëœ ê²½ìš°ì—ë§Œ ê²°ê³¼ ë°˜ì˜
                if new_quality['quality_score'] > original_quality:
                    # original_text ì œê±°
                    if "original_text" in new_result:
                        del new_result["original_text"]
                    
                    improved_results[idx] = new_result
                    improved_quality[idx] = new_quality
                    total_improved += 1
                
                retry_count += 1
                pbar.update(1)
                
                # API ì œí•œì„ ìœ„í•œ ì§§ì€ ëŒ€ê¸°
                time.sleep(0.5)
        
        print(f"ì¬ë¶„ì„ ì™„ë£Œ: {total_improved}ê°œ í•­ëª©ì´ ê°œì„ ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        return improved_results, improved_quality
    
    def process_xlsx_with_refined_text(self, input_file: str, refined_column: str, anonymized_column: str = None, output_file: str = None, delay: float = 0.1, max_rows: int = None, use_batch: bool = True, batch_size: int = 10, enable_quality_retry: bool = True):
        """
        ì´ë¯¸ ì •ì œëœ í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ì—‘ì…€ íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
        
        Args:
            input_file: ì…ë ¥ ì—‘ì…€ íŒŒì¼ ê²½ë¡œ
            refined_column: ì •ì œëœ í…ìŠ¤íŠ¸ê°€ í¬í•¨ëœ ì»¬ëŸ¼ëª…
            anonymized_column: ë¹„ì‹ë³„ ì²˜ë¦¬ ì—¬ë¶€ ì»¬ëŸ¼ëª… (ì„ íƒì‚¬í•­)
            output_file: ê²°ê³¼ë¥¼ ì €ì¥í•  íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: ì…ë ¥íŒŒì¼ëª…_analyzed.xlsx)
            delay: API í˜¸ì¶œ ê°„ ëŒ€ê¸° ì‹œê°„ (ì´ˆ) - ë°°ì¹˜ ì²˜ë¦¬ ì‹œ ë¬´ì‹œë¨
            max_rows: í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì²˜ë¦¬í•  ìµœëŒ€ í–‰ ìˆ˜
            use_batch: ë°°ì¹˜ ì²˜ë¦¬ ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
            batch_size: ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 10)
            enable_quality_retry: ë‚®ì€ í’ˆì§ˆ í•­ëª© ì¬ë¶„ì„ ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
        """
        # ì—‘ì…€ íŒŒì¼ ì½ê¸°
        df = pd.read_excel(input_file)
        
        # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì¼ë¶€ ë°ì´í„°ë§Œ ì²˜ë¦¬í•˜ëŠ” ê²½ìš°
        if max_rows:
            df = df.head(max_rows)
            print(f"ì „ì²´ ë°ì´í„°ì—ì„œ ìƒë‹¨ {len(df)}ê°œ ì¶”ì¶œ")
        
        # ì§€ì •í•œ ì»¬ëŸ¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        if refined_column not in df.columns:
            available_columns = list(df.columns)
            raise ValueError(f"'{refined_column}' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {available_columns}")
        
        # ì¶œë ¥ íŒŒì¼ëª… ì„¤ì •
        if output_file is None:
            output_file = str(Path(input_file).stem) + "_analyzed.xlsx"
        
        # ì •ì œëœ í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ë°ì´í„°ë§Œ í•„í„°ë§
        valid_data = df[df[refined_column].notna() & (df[refined_column] != "")]
        total_rows = len(valid_data)
        
        print(f"ì´ {len(df)}ê°œ í–‰ ì¤‘ {total_rows}ê°œì˜ ìœ íš¨í•œ '{refined_column}' í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤...")
        
        # í…ìŠ¤íŠ¸ì™€ ë¹„ì‹ë³„ ì •ë³´ ì¶”ì¶œ
        texts_and_flags = []
        for _, row in valid_data.iterrows():
            refined_text = str(row[refined_column]) if pd.notna(row[refined_column]) else ""
            is_anonymized = bool(row[anonymized_column]) if anonymized_column and anonymized_column in df.columns and pd.notna(row[anonymized_column]) else False
            texts_and_flags.append((refined_text, is_anonymized))
        
        # ë°°ì¹˜ ì²˜ë¦¬ ë˜ëŠ” ìˆœì°¨ ì²˜ë¦¬ ì„ íƒ
        if use_batch:
            print(f"ë°°ì¹˜ ì²˜ë¦¬ ëª¨ë“œ (ë°°ì¹˜ í¬ê¸°: {batch_size})")
            print(f"ì˜ˆìƒ ì†Œìš”ì‹œê°„: ì•½ {math.ceil(total_rows / batch_size) * 2}ë¶„ (ë°°ì¹˜ ì²˜ë¦¬ ê¸°ì¤€)")
            
            # ì§„í–‰ë¥  í‘œì‹œì™€ í•¨ê»˜ ë°°ì¹˜ ì²˜ë¦¬
            results = []
            with tqdm(total=total_rows, desc="ë¶„ì„ ì§„í–‰ë¥ ", unit="ê±´") as pbar:
                for i in range(0, len(texts_and_flags), batch_size):
                    batch_texts_flags = texts_and_flags[i:i+batch_size]
                    batch_results = self.analyze_refined_batch(batch_texts_flags, len(batch_texts_flags))
                    results.extend(batch_results)
                    pbar.update(len(batch_texts_flags))
        else:
            print(f"ìˆœì°¨ ì²˜ë¦¬ ëª¨ë“œ")
            est_sec = total_rows * delay
            est_time = str(datetime.timedelta(seconds=math.ceil(est_sec)))
            print(f"ì˜ˆìƒ ì†Œìš”ì‹œê°„: {est_time} (ì§€ì—° {delay}ì´ˆ/ê±´ ê¸°ì¤€)")
            
            # ìˆœì°¨ ì²˜ë¦¬ì™€ ì§„í–‰ë¥  í‘œì‹œ
            results = []
            with tqdm(total=total_rows, desc="ë¶„ì„ ì§„í–‰ë¥ ", unit="ê±´") as pbar:
                for refined_text, is_anonymized in texts_and_flags:
                    result = self.analyze_refined_text(refined_text, is_anonymized)
                    # ì›ë³¸ í…ìŠ¤íŠ¸ëŠ” ê²°ê³¼ì—ì„œ ì œê±°
                    if "original_text" in result:
                        del result["original_text"]
                    results.append(result)
                    pbar.update(1)
                    
                    # API í˜¸ì¶œ ì œí•œì„ ìœ„í•œ ëŒ€ê¸°
                    if delay > 0:
                        time.sleep(delay)
        
        # ê²°ê³¼ì—ì„œ ì›ë³¸ í…ìŠ¤íŠ¸ ì œê±° ë° ì •ë¦¬
        for result in results:
            if "original_text" in result:
                del result["original_text"]
        
        # í’ˆì§ˆ ê²€ì¦ ìˆ˜í–‰
        quality_results = []
        print("\ní’ˆì§ˆ ê²€ì¦ ìˆ˜í–‰ ì¤‘...")
        for idx, (result, (refined_text, _)) in enumerate(zip(results, texts_and_flags)):
            quality_check = self.validate_analysis_quality(result, refined_text)
            quality_results.append(quality_check)
        
        # ë‚®ì€ í’ˆì§ˆ í•­ëª© ì¬ë¶„ì„ (í™œì„±í™”ëœ ê²½ìš°ì—ë§Œ)
        if enable_quality_retry:
            results, quality_results = self.retry_low_quality_analysis_refined(texts_and_flags, results, quality_results, max_retries=3)
        
        # ìµœì¢… ë‚®ì€ í’ˆì§ˆ í•­ëª© ì°¾ê¸°
        low_quality_indices = [i for i, q in enumerate(quality_results) if q['needs_review']]
        
        # í’ˆì§ˆ ê²€ì¦ ê²°ê³¼ë¥¼ ê²°ê³¼ì— ì¶”ê°€
        for i, result in enumerate(results):
            result['quality_score'] = quality_results[i]['quality_score']
            result['needs_review'] = quality_results[i]['needs_review']
            result['quality_issues'] = '; '.join(quality_results[i]['issues']) if quality_results[i]['issues'] else ''
        
        # í‚¤ì›Œë“œ ë¶„ì„
        print("\ní‚¤ì›Œë“œ ë¶„ì„ ìˆ˜í–‰ ì¤‘...")
        all_refined_texts = [r.get('refined_text', '') for r in results]
        keyword_freq = self.extract_keywords(all_refined_texts)
        
        # ìˆ˜í•™ì /í†µê³„ì  ë¶„ì„ ìˆ˜í–‰
        print("\nìˆ˜í•™ì  ë¶„ì„ ìˆ˜í–‰ ì¤‘...")
        vector_analysis = self.calculate_emotion_vectors(results)
        
        print("í†µê³„ì  ë¶„ì„ ìˆ˜í–‰ ì¤‘...")
        statistical_results = self.statistical_analysis(results)
        
        print("ê³ ê¸‰ ê°ì • ë©”íŠ¸ë¦­ ê³„ì‚° ì¤‘...")
        advanced_metrics = self.advanced_emotion_metrics(results)
        
        # ë¶„ì„ ê²°ê³¼ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
        result_df = pd.DataFrame(results)
        
        # ì›ë³¸ ë°ì´í„°ì™€ ë¶„ì„ ê²°ê³¼ë¥¼ í•©ì¹œ ìƒˆë¡œìš´ ë°ì´í„°í”„ë ˆì„ ìƒì„±
        # ìœ íš¨í•œ ë°ì´í„°ì˜ ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ë§¤í•‘
        processed_df = valid_data.copy().reset_index(drop=True)
        
        # ì»¬ëŸ¼ëª…ì„ í•œêµ­ì–´ë¡œ ë§¤í•‘ (ê³ ë„í™” ë²„ì „)
        korean_column_names = {
            'refined_text': 'ë¶„ì„í…ìŠ¤íŠ¸',
            'is_anonymized': 'ë¹„ì‹ë³„ì²˜ë¦¬ì—¬ë¶€',
            'primary_emotion': 'ì£¼ìš”ê°ì •',
            'secondary_emotion': 'ë³´ì¡°ê°ì •', 
            'emotion_intensity': 'ê°ì •ê°•ë„',
            'emotional_complexity': 'ê°ì •ë³µí•©ì„±',
            'emotion_mix': 'ê°ì •ë¹„ìœ¨',
            'medical_context': 'ì˜ë£Œë§¥ë½',
            'context_weight': 'ë§¥ë½ì¤‘ìš”ë„',
            'sentiment': 'ê°ì •ë¶„ì„',
            'sentiment_intensity': 'ê°ì •ê°•ë„ì ìˆ˜',
            'confidence_score': 'AIì‹ ë¢°ë„',
            'key_terms': 'í•µì‹¬í‚¤ì›Œë“œ',
            'labels': 'ë¶„ë¥˜ë¼ë²¨',
            'quality_score': 'í’ˆì§ˆì ìˆ˜',
            'needs_review': 'ì¬ê²€í† í•„ìš”',
            'quality_issues': 'í’ˆì§ˆë¬¸ì œ'
        }
        
        for col in result_df.columns:
            korean_col_name = korean_column_names.get(col, col)
            processed_df[f"{refined_column}_{korean_col_name}"] = result_df[col]
        
        # ìˆ˜í•™ì  ë¶„ì„ ê²°ê³¼ë¥¼ ë³„ë„ ì‹œíŠ¸ë¡œ ì €ì¥
        with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
            # ë©”ì¸ ë°ì´í„°
            processed_df.to_excel(writer, sheet_name='ë¶„ì„ê²°ê³¼', index=False)
            
            # ë²¡í„° ë¶„ì„ ê²°ê³¼
            if vector_analysis:
                vector_df = pd.DataFrame([vector_analysis])
                vector_df.to_excel(writer, sheet_name='ë²¡í„°ë¶„ì„', index=False)
            
            # í†µê³„ ë¶„ì„ ê²°ê³¼
            if statistical_results:
                # ê¸°ìˆ í†µê³„ëŸ‰
                desc_stats = pd.DataFrame(statistical_results.get('descriptive_stats', {}))
                desc_stats.to_excel(writer, sheet_name='ê¸°ìˆ í†µê³„ëŸ‰', index=True)
                
                # ì •ê·œì„± ê²€ì •
                normality_df = pd.DataFrame(statistical_results.get('normality_tests', {}))
                normality_df.to_excel(writer, sheet_name='ì •ê·œì„±ê²€ì •', index=True)
                
                # ìƒê´€ê´€ê³„
                corr_df = pd.DataFrame([statistical_results.get('correlations', {})])
                corr_df.to_excel(writer, sheet_name='ìƒê´€ê´€ê³„', index=False)
                
                # ì‹ ë¢°êµ¬ê°„
                ci_df = pd.DataFrame([statistical_results.get('confidence_intervals', {})])
                ci_df.to_excel(writer, sheet_name='ì‹ ë¢°êµ¬ê°„', index=False)
            
            # ê³ ê¸‰ ë©”íŠ¸ë¦­
            if advanced_metrics:
                metrics_df = pd.DataFrame([advanced_metrics])
                metrics_df.to_excel(writer, sheet_name='ê³ ê¸‰ë©”íŠ¸ë¦­', index=False)
            
            # ìƒì„¸ ì„¤ëª… ì‹œíŠ¸ ì¶”ê°€
            self._create_explanation_sheets(writer, vector_analysis, statistical_results, advanced_metrics)
        
        print(f"\në¶„ì„ ì™„ë£Œ! ê²°ê³¼ê°€ '{output_file}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("ğŸ“Š ë‹¤ì¤‘ ì‹œíŠ¸ ì—‘ì…€ íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤:")
        print("  - ë¶„ì„ê²°ê³¼: ê¸°ë³¸ ê°ì • ë¶„ì„ ë°ì´í„°")
        print("  - ë²¡í„°ë¶„ì„: ê°ì • ë²¡í„° ê³µê°„ ë¶„ì„")
        print("  - ê¸°ìˆ í†µê³„ëŸ‰: ê¸°ë³¸ í†µê³„ ì •ë³´")
        print("  - ì •ê·œì„±ê²€ì •: ë¶„í¬ ì •ê·œì„± ê²€ì • ê²°ê³¼")
        print("  - ìƒê´€ê´€ê³„: ê°ì •-ê°•ë„ ìƒê´€ë¶„ì„")
        print("  - ì‹ ë¢°êµ¬ê°„: 95% ì‹ ë¢°êµ¬ê°„")
        print("  - ê³ ê¸‰ë©”íŠ¸ë¦­: ê°ì • ë³µì¡ë„, ì¼ê´€ì„± ë“±")
        print("  - ë¶„ì„ë°©ë²•_ì„¤ëª…: ìˆ˜í•™/í†µê³„ ë°©ë²•ë¡  ì„¤ëª…")
        print("  - ì§€í‘œí•´ì„_ê°€ì´ë“œ: ê° ì§€í‘œì˜ ì˜ë¯¸ì™€ ê¸°ì¤€")
        print("  - í™œìš©ë°©ì•ˆ_ì•ˆë‚´: ì‹¤ë¬´ í™œìš© ê°€ì´ë“œ")
        print("  - ê²°ê³¼í•´ì„_ì˜ˆì‹œ: í˜„ì¬ ê²°ê³¼ í•´ì„ ì˜ˆì‹œ")
        
        # ë‚®ì€ í’ˆì§ˆ í•­ëª© ì•ˆë‚´
        if low_quality_indices:
            print(f"\nâš ï¸  {len(low_quality_indices)}ê°œ í•­ëª©ì´ ì¬ê²€í† ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            print("ì„¸ë¶€ ì‚¬í•­ì€ 'quality_issues' ì»¬ëŸ¼ì„ í™•ì¸í•˜ì„¸ìš”.")
        
        # ê³ ë„í™”ëœ ê°ì • ë¶„ì„ í†µê³„ ì¶œë ¥
        print(f"\n=== '{refined_column}' ê³ ë„í™”ëœ ë¶„ì„ ê²°ê³¼ ====")
        
        # ê¸°ì¡´ 3ë¶„ë¥˜ ê°ì • í†µê³„
        sentiment_counts = result_df['sentiment'].value_counts()
        print(f"ê¸°ë³¸ ê°ì • ë¶„ë¥˜:")
        for sentiment, count in sentiment_counts.items():
            percentage = (count / len(result_df)) * 100
            print(f"  {sentiment}: {count}ê°œ ({percentage:.1f}%)")
        
        # 8ê°€ì§€ ì„¸ë¶„í™”ëœ ê°ì • í†µê³„
        if 'primary_emotion' in result_df.columns:
            primary_emotion_counts = result_df['primary_emotion'].value_counts()
            print(f"\nì„¸ë¶„í™”ëœ ì£¼ìš” ê°ì •:")
            for emotion, count in primary_emotion_counts.items():
                percentage = (count / len(result_df)) * 100
                print(f"  {emotion}: {count}ê°œ ({percentage:.1f}%)")
        
        # ë³µí•© ê°ì • í†µê³„
        if 'emotional_complexity' in result_df.columns:
            complexity_counts = result_df['emotional_complexity'].value_counts()
            print(f"\nê°ì • ë³µí•©ì„±:")
            for complexity, count in complexity_counts.items():
                percentage = (count / len(result_df)) * 100
                print(f"  {complexity}: {count}ê°œ ({percentage:.1f}%)")
        
        # ì˜ë£Œ ë§¥ë½ í†µê³„
        if 'medical_context' in result_df.columns:
            all_contexts = []
            for contexts in result_df['medical_context']:
                if isinstance(contexts, list):
                    all_contexts.extend(contexts)
            if all_contexts:
                from collections import Counter
                context_counts = Counter(all_contexts)
                print(f"\nì˜ë£Œ í˜‘ì—… ë§¥ë½:")
                for context, count in context_counts.most_common():
                    print(f"  {context}: {count}ê±´")
        
        # ê¸°ì¡´ í†µê³„
        intensity_mean = result_df['emotion_intensity'].mean() if 'emotion_intensity' in result_df.columns else result_df['sentiment_intensity'].mean()
        confidence_mean = result_df['quality_score'].mean() if 'quality_score' in result_df.columns else result_df['confidence_score'].mean()
        
        print(f"\ní‰ê·  ê°ì • ê°•ë„: {intensity_mean:.1f}/10")
        print(f"í‰ê·  ì‹ ë¢°ë„: {confidence_mean:.1f}/10")
        
        # ì¶”ê°€ í†µê³„
        if 'is_anonymized' in result_df.columns:
            anonymized_count = result_df['is_anonymized'].sum()
            print(f"ë¹„ì‹ë³„ ì²˜ë¦¬ëœ í•­ëª©: {anonymized_count}ê°œ")
        
        # í’ˆì§ˆ í†µê³„
        needs_review_count = sum(1 for q in quality_results if q['needs_review'])
        reliable_count = sum(1 for q in quality_results if q['is_reliable'])
        print(f"ì¬ê²€í†  í•„ìš” í•­ëª©: {needs_review_count}ê°œ")
        print(f"ì‹ ë¢°ë„ ë†’ì€ í•­ëª©: {reliable_count}ê°œ")
        
        # ì£¼ìš” í‚¤ì›Œë“œ ì¶œë ¥
        print(f"\n=== ì£¼ìš” í‚¤ì›Œë“œ TOP 10 ====")
        for keyword, freq in list(keyword_freq.items())[:10]:
            print(f"  {keyword}: {freq}ë²ˆ")
        
        # ìˆ˜í•™ì /í†µê³„ì  ë¶„ì„ ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        if vector_analysis:
            print(f"\n=== ê°ì • ë²¡í„° ë¶„ì„ ê²°ê³¼ ====")
            print(f"ê°ì • ë‹¤ì–‘ì„± ì§€ìˆ˜: {vector_analysis.get('emotion_diversity', 0):.3f}")
            print(f"í‰ê·  ê°ì • ê·¹ì„±: {vector_analysis.get('avg_polarity', 0):.3f}")
            print(f"ê°ì • ì‘ì§‘ë„: {vector_analysis.get('emotion_cohesion', 0):.3f}")
            print(f"ê°•ë„ ì•ˆì •ì„±: {vector_analysis.get('intensity_stability', 0):.3f}")
            
            dist = vector_analysis.get('emotion_distribution', {})
            print(f"ê°ì • ë¶„í¬ - ê¸ì •: {dist.get('positive_ratio', 0):.1%}, "
                  f"ë¶€ì •: {dist.get('negative_ratio', 0):.1%}, "
                  f"ì¤‘ë¦½: {dist.get('neutral_ratio', 0):.1%}")
        
        if statistical_results:
            print(f"\n=== í†µê³„ì  ë¶„ì„ ê²°ê³¼ ====")
            desc_stats = statistical_results.get('descriptive_stats', {})
            intensity_stats = desc_stats.get('intensity', {})
            
            print(f"ê°ì • ê°•ë„ í†µê³„:")
            print(f"  í‰ê· : {intensity_stats.get('mean', 0):.2f} Â± {intensity_stats.get('std', 0):.2f}")
            print(f"  ì¤‘ì•™ê°’: {intensity_stats.get('median', 0):.2f}")
            print(f"  ì™œë„: {intensity_stats.get('skewness', 0):.3f}")
            print(f"  ì²¨ë„: {intensity_stats.get('kurtosis', 0):.3f}")
            
            # ì •ê·œì„± ê²€ì • ê²°ê³¼
            normality = statistical_results.get('normality_tests', {})
            intensity_norm = normality.get('intensity_shapiro', {})
            print(f"ì •ê·œì„± ê²€ì • (Shapiro-Wilk): p={intensity_norm.get('p_value', 0):.4f} "
                  f"({'ì •ê·œë¶„í¬' if intensity_norm.get('is_normal', False) else 'ë¹„ì •ê·œë¶„í¬'})")
            
            # ìƒê´€ê´€ê³„
            corr = statistical_results.get('correlations', {})
            print(f"ê°ì •-ê°•ë„ ìƒê´€ê³„ìˆ˜: r={corr.get('pearson_r', 0):.3f} "
                  f"({'ìœ ì˜' if corr.get('significant', False) else 'ë¹„ìœ ì˜'})")
            
            # ì‹ ë¢°êµ¬ê°„
            ci = statistical_results.get('confidence_intervals', {})
            intensity_ci = ci.get('intensity_95ci', [0, 0])
            print(f"ê°ì • ê°•ë„ 95% ì‹ ë¢°êµ¬ê°„: [{intensity_ci[0]:.2f}, {intensity_ci[1]:.2f}]")
            
            # ì •ë³´ ì´ë¡ 
            info_theory = statistical_results.get('information_theory', {})
            print(f"ê°ì • ì—”íŠ¸ë¡œí”¼: {info_theory.get('emotion_entropy', 0):.3f} "
                  f"(ìµœëŒ€: {info_theory.get('max_entropy', 0):.3f})")
            
        if advanced_metrics:
            print(f"\n=== ê³ ê¸‰ ê°ì • ë©”íŠ¸ë¦­ ====")
            print(f"ê°ì • ì¼ê´€ì„± ì§€ìˆ˜: {advanced_metrics.get('emotion_consistency_index', 0):.3f}")
            print(f"ê°ì • ë³µì¡ë„ ì§€ìˆ˜: {advanced_metrics.get('emotion_complexity_index', 0):.3f}")
            print(f"ì˜ë£Œ ë§¥ë½ ë‹¤ì–‘ì„±: {advanced_metrics.get('medical_context_diversity', 0):.3f}")
            print(f"ê°ì • ë³€ë™ì„± ì§€ìˆ˜: {advanced_metrics.get('emotional_volatility_index', 0):.3f}")
            print(f"ê°ì • ì§‘ì¤‘ë„ ì§€ìˆ˜: {advanced_metrics.get('emotion_concentration_index', 0):.3f}")
            print(f"ì•ˆì •ì„± ì ìˆ˜: {advanced_metrics.get('stability_score', 0):.3f}")
            print(f"ê· í˜•ì„± ì ìˆ˜: {advanced_metrics.get('balance_score', 0):.3f}")
            print(f"ì „ì²´ ì •êµì„±: {advanced_metrics.get('overall_sophistication', 0):.3f}")

    def process_xlsx_with_column(self, input_file: str, column_name: str, output_file: str = None, delay: float = 0.1, max_rows: int = None, use_batch: bool = True, batch_size: int = 10, enable_quality_retry: bool = True):
        """
        ì—‘ì…€ íŒŒì¼ì˜ íŠ¹ì • ì»¬ëŸ¼ì— ìˆëŠ” í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
        
        Args:
            input_file: ì…ë ¥ ì—‘ì…€ íŒŒì¼ ê²½ë¡œ
            column_name: ë¶„ì„í•  í…ìŠ¤íŠ¸ê°€ í¬í•¨ëœ ì»¬ëŸ¼ëª…
            output_file: ê²°ê³¼ë¥¼ ì €ì¥í•  íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: ì…ë ¥íŒŒì¼ëª…_processed.xlsx)
            delay: API í˜¸ì¶œ ê°„ ëŒ€ê¸° ì‹œê°„ (ì´ˆ) - ë°°ì¹˜ ì²˜ë¦¬ ì‹œ ë¬´ì‹œë¨
            max_rows: í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì²˜ë¦¬í•  ìµœëŒ€ í–‰ ìˆ˜
            use_batch: ë°°ì¹˜ ì²˜ë¦¬ ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
            batch_size: ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 10)
            enable_quality_retry: ë‚®ì€ í’ˆì§ˆ í•­ëª© ì¬ë¶„ì„ ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
        """
        # ì—‘ì…€ íŒŒì¼ ì½ê¸°
        df = pd.read_excel(input_file)
        
        # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì¼ë¶€ ë°ì´í„°ë§Œ ì²˜ë¦¬í•˜ëŠ” ê²½ìš° (ìƒë‹¨ë¶€í„° ìˆœì°¨ ì¶”ì¶œ)
        if max_rows:
            df = df.head(max_rows)
            print(f"ì „ì²´ ë°ì´í„°ì—ì„œ ìƒë‹¨ {len(df)}ê°œ ì¶”ì¶œ")
        
        # ì§€ì •í•œ ì»¬ëŸ¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        if column_name not in df.columns:
            available_columns = list(df.columns)
            raise ValueError(f"'{column_name}' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {available_columns}")
        
        # ì¶œë ¥ íŒŒì¼ëª… ì„¤ì •
        if output_file is None:
            output_file = str(Path(input_file).stem) + "_processed.xlsx"
        
        total_rows = len(df)
        print(f"ì´ {total_rows}ê°œì˜ '{column_name}' í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤...")
        
        # í…ìŠ¤íŠ¸ ë°ì´í„° ì¶”ì¶œ
        texts = []
        for _, row in df.iterrows():
            original_text = str(row[column_name]) if pd.notna(row[column_name]) else ""
            texts.append(original_text)
        
        # ë°°ì¹˜ ì²˜ë¦¬ ë˜ëŠ” ìˆœì°¨ ì²˜ë¦¬ ì„ íƒ
        if use_batch:
            print(f"ë°°ì¹˜ ì²˜ë¦¬ ëª¨ë“œ (ë°°ì¹˜ í¬ê¸°: {batch_size})")
            print(f"ì˜ˆìƒ ì†Œìš”ì‹œê°„: ì•½ {math.ceil(total_rows / batch_size) * 2}ë¶„ (ë°°ì¹˜ ì²˜ë¦¬ ê¸°ì¤€)")
            
            # ì§„í–‰ë¥  í‘œì‹œì™€ í•¨ê»˜ ë°°ì¹˜ ì²˜ë¦¬
            results = []
            with tqdm(total=total_rows, desc="ë¶„ì„ ì§„í–‰ë¥ ", unit="ê±´") as pbar:
                for i in range(0, len(texts), batch_size):
                    batch_texts = texts[i:i+batch_size]
                    batch_results = self.analyze_batch(batch_texts, len(batch_texts))
                    results.extend(batch_results)
                    pbar.update(len(batch_texts))
        else:
            print(f"ìˆœì°¨ ì²˜ë¦¬ ëª¨ë“œ")
            est_sec = total_rows * delay
            est_time = str(datetime.timedelta(seconds=math.ceil(est_sec)))
            print(f"ì˜ˆìƒ ì†Œìš”ì‹œê°„: {est_time} (ì§€ì—° {delay}ì´ˆ/ê±´ ê¸°ì¤€)")
            
            # ìˆœì°¨ ì²˜ë¦¬ì™€ ì§„í–‰ë¥  í‘œì‹œ
            results = []
            with tqdm(total=total_rows, desc="ë¶„ì„ ì§„í–‰ë¥ ", unit="ê±´") as pbar:
                for text in texts:
                    result = self.analyze_review(text)
                    # ì›ë³¸ í…ìŠ¤íŠ¸ëŠ” ê²°ê³¼ì—ì„œ ì œê±°
                    if "original_text" in result:
                        del result["original_text"]
                    results.append(result)
                    pbar.update(1)
                    
                    # API í˜¸ì¶œ ì œí•œì„ ìœ„í•œ ëŒ€ê¸°
                    if delay > 0:
                        time.sleep(delay)
        
        # ê²°ê³¼ì—ì„œ ì›ë³¸ í…ìŠ¤íŠ¸ ì œê±° ë° ì •ë¦¬
        for result in results:
            if "original_text" in result:
                del result["original_text"]
        
        # í’ˆì§ˆ ê²€ì¦ ìˆ˜í–‰
        quality_results = []
        print("\ní’ˆì§ˆ ê²€ì¦ ìˆ˜í–‰ ì¤‘...")
        for idx, (result, original_text) in enumerate(zip(results, texts)):
            quality_check = self.validate_analysis_quality(result, original_text)
            quality_results.append(quality_check)
        
        # ë‚®ì€ í’ˆì§ˆ í•­ëª© ì¬ë¶„ì„ (í™œì„±í™”ëœ ê²½ìš°ì—ë§Œ)
        if enable_quality_retry:
            results, quality_results = self.retry_low_quality_analysis(texts, results, quality_results, max_retries=3)
        
        # ìµœì¢… ë‚®ì€ í’ˆì§ˆ í•­ëª© ì°¾ê¸°
        low_quality_indices = [i for i, q in enumerate(quality_results) if q['needs_review']]
        
        # í’ˆì§ˆ ê²€ì¦ ê²°ê³¼ë¥¼ ê²°ê³¼ì— ì¶”ê°€
        for i, result in enumerate(results):
            result['quality_score'] = quality_results[i]['quality_score']
            result['needs_review'] = quality_results[i]['needs_review']
            result['quality_issues'] = '; '.join(quality_results[i]['issues']) if quality_results[i]['issues'] else ''
        
        # í‚¤ì›Œë“œ ë¶„ì„
        print("\ní‚¤ì›Œë“œ ë¶„ì„ ìˆ˜í–‰ ì¤‘...")
        all_refined_texts = [r.get('refined_text', '') for r in results]
        keyword_freq = self.extract_keywords(all_refined_texts)
        
        # ìˆ˜í•™ì /í†µê³„ì  ë¶„ì„ ìˆ˜í–‰
        print("\nìˆ˜í•™ì  ë¶„ì„ ìˆ˜í–‰ ì¤‘...")
        vector_analysis = self.calculate_emotion_vectors(results)
        
        print("í†µê³„ì  ë¶„ì„ ìˆ˜í–‰ ì¤‘...")
        statistical_results = self.statistical_analysis(results)
        
        print("ê³ ê¸‰ ê°ì • ë©”íŠ¸ë¦­ ê³„ì‚° ì¤‘...")
        advanced_metrics = self.advanced_emotion_metrics(results)
        
        # ë¶„ì„ ê²°ê³¼ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
        result_df = pd.DataFrame(results)
        
        # ì›ë³¸ ë°ì´í„°ì™€ ë¶„ì„ ê²°ê³¼ë¥¼ í•©ì¹œ ìƒˆë¡œìš´ ë°ì´í„°í”„ë ˆì„ ìƒì„±
        processed_df = df.copy()
        
        # ì»¬ëŸ¼ëª…ì„ í•œêµ­ì–´ë¡œ ë§¤í•‘ (ê³ ë„í™” ë²„ì „)
        korean_column_names = {
            'refined_text': 'ì •ì œí…ìŠ¤íŠ¸',
            'is_anonymized': 'ë¹„ì‹ë³„ì²˜ë¦¬ì—¬ë¶€',
            'primary_emotion': 'ì£¼ìš”ê°ì •',
            'secondary_emotion': 'ë³´ì¡°ê°ì •', 
            'emotion_intensity': 'ê°ì •ê°•ë„',
            'emotional_complexity': 'ê°ì •ë³µí•©ì„±',
            'emotion_mix': 'ê°ì •ë¹„ìœ¨',
            'medical_context': 'ì˜ë£Œë§¥ë½',
            'context_weight': 'ë§¥ë½ì¤‘ìš”ë„',
            'sentiment': 'ê°ì •ë¶„ì„',
            'sentiment_intensity': 'ê°ì •ê°•ë„ì ìˆ˜',
            'confidence_score': 'AIì‹ ë¢°ë„',
            'key_terms': 'í•µì‹¬í‚¤ì›Œë“œ',
            'labels': 'ë¶„ë¥˜ë¼ë²¨',
            'quality_score': 'í’ˆì§ˆì ìˆ˜',
            'needs_review': 'ì¬ê²€í† í•„ìš”',
            'quality_issues': 'í’ˆì§ˆë¬¸ì œ'
        }
        
        for col in result_df.columns:
            korean_col_name = korean_column_names.get(col, col)
            processed_df[f"{column_name}_{korean_col_name}"] = result_df[col]
        
        # ìˆ˜í•™ì  ë¶„ì„ ê²°ê³¼ë¥¼ ë³„ë„ ì‹œíŠ¸ë¡œ ì €ì¥
        with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
            # ë©”ì¸ ë°ì´í„°
            processed_df.to_excel(writer, sheet_name='ë¶„ì„ê²°ê³¼', index=False)
            
            # ë²¡í„° ë¶„ì„ ê²°ê³¼
            if vector_analysis:
                vector_df = pd.DataFrame([vector_analysis])
                vector_df.to_excel(writer, sheet_name='ë²¡í„°ë¶„ì„', index=False)
            
            # í†µê³„ ë¶„ì„ ê²°ê³¼
            if statistical_results:
                # ê¸°ìˆ í†µê³„ëŸ‰
                desc_stats = pd.DataFrame(statistical_results.get('descriptive_stats', {}))
                desc_stats.to_excel(writer, sheet_name='ê¸°ìˆ í†µê³„ëŸ‰', index=True)
                
                # ì •ê·œì„± ê²€ì •
                normality_df = pd.DataFrame(statistical_results.get('normality_tests', {}))
                normality_df.to_excel(writer, sheet_name='ì •ê·œì„±ê²€ì •', index=True)
                
                # ìƒê´€ê´€ê³„
                corr_df = pd.DataFrame([statistical_results.get('correlations', {})])
                corr_df.to_excel(writer, sheet_name='ìƒê´€ê´€ê³„', index=False)
                
                # ì‹ ë¢°êµ¬ê°„
                ci_df = pd.DataFrame([statistical_results.get('confidence_intervals', {})])
                ci_df.to_excel(writer, sheet_name='ì‹ ë¢°êµ¬ê°„', index=False)
            
            # ê³ ê¸‰ ë©”íŠ¸ë¦­
            if advanced_metrics:
                metrics_df = pd.DataFrame([advanced_metrics])
                metrics_df.to_excel(writer, sheet_name='ê³ ê¸‰ë©”íŠ¸ë¦­', index=False)
            
            # ìƒì„¸ ì„¤ëª… ì‹œíŠ¸ ì¶”ê°€
            self._create_explanation_sheets(writer, vector_analysis, statistical_results, advanced_metrics)
        
        print(f"\në¶„ì„ ì™„ë£Œ! ê²°ê³¼ê°€ '{output_file}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("ğŸ“Š ë‹¤ì¤‘ ì‹œíŠ¸ ì—‘ì…€ íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤:")
        print("  - ë¶„ì„ê²°ê³¼: ê¸°ë³¸ ê°ì • ë¶„ì„ ë°ì´í„°")
        print("  - ë²¡í„°ë¶„ì„: ê°ì • ë²¡í„° ê³µê°„ ë¶„ì„")
        print("  - ê¸°ìˆ í†µê³„ëŸ‰: ê¸°ë³¸ í†µê³„ ì •ë³´")
        print("  - ì •ê·œì„±ê²€ì •: ë¶„í¬ ì •ê·œì„± ê²€ì • ê²°ê³¼")
        print("  - ìƒê´€ê´€ê³„: ê°ì •-ê°•ë„ ìƒê´€ë¶„ì„")
        print("  - ì‹ ë¢°êµ¬ê°„: 95% ì‹ ë¢°êµ¬ê°„")
        print("  - ê³ ê¸‰ë©”íŠ¸ë¦­: ê°ì • ë³µì¡ë„, ì¼ê´€ì„± ë“±")
        print("  - ë¶„ì„ë°©ë²•_ì„¤ëª…: ìˆ˜í•™/í†µê³„ ë°©ë²•ë¡  ì„¤ëª…")
        print("  - ì§€í‘œí•´ì„_ê°€ì´ë“œ: ê° ì§€í‘œì˜ ì˜ë¯¸ì™€ ê¸°ì¤€")
        print("  - í™œìš©ë°©ì•ˆ_ì•ˆë‚´: ì‹¤ë¬´ í™œìš© ê°€ì´ë“œ")
        print("  - ê²°ê³¼í•´ì„_ì˜ˆì‹œ: í˜„ì¬ ê²°ê³¼ í•´ì„ ì˜ˆì‹œ")
        
        # ë‚®ì€ í’ˆì§ˆ í•­ëª© ì•ˆë‚´
        if low_quality_indices:
            print(f"\nâš ï¸  {len(low_quality_indices)}ê°œ í•­ëª©ì´ ì¬ê²€í† ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            print("ì„¸ë¶€ ì‚¬í•­ì€ 'quality_issues' ì»¬ëŸ¼ì„ í™•ì¸í•˜ì„¸ìš”.")
        
        # ê³ ë„í™”ëœ ê°ì • ë¶„ì„ í†µê³„ ì¶œë ¥
        print(f"\n=== '{column_name}' ê³ ë„í™”ëœ ë¶„ì„ ê²°ê³¼ ====")
        
        # ê¸°ì¡´ 3ë¶„ë¥˜ ê°ì • í†µê³„
        sentiment_counts = result_df['sentiment'].value_counts()
        print(f"ê¸°ë³¸ ê°ì • ë¶„ë¥˜:")
        for sentiment, count in sentiment_counts.items():
            percentage = (count / len(result_df)) * 100
            print(f"  {sentiment}: {count}ê°œ ({percentage:.1f}%)")
        
        # 8ê°€ì§€ ì„¸ë¶„í™”ëœ ê°ì • í†µê³„
        if 'primary_emotion' in result_df.columns:
            primary_emotion_counts = result_df['primary_emotion'].value_counts()
            print(f"\nì„¸ë¶„í™”ëœ ì£¼ìš” ê°ì •:")
            for emotion, count in primary_emotion_counts.items():
                percentage = (count / len(result_df)) * 100
                print(f"  {emotion}: {count}ê°œ ({percentage:.1f}%)")
        
        # ë³µí•© ê°ì • í†µê³„
        if 'emotional_complexity' in result_df.columns:
            complexity_counts = result_df['emotional_complexity'].value_counts()
            print(f"\nê°ì • ë³µí•©ì„±:")
            for complexity, count in complexity_counts.items():
                percentage = (count / len(result_df)) * 100
                print(f"  {complexity}: {count}ê°œ ({percentage:.1f}%)")
        
        # ì˜ë£Œ ë§¥ë½ í†µê³„
        if 'medical_context' in result_df.columns:
            all_contexts = []
            for contexts in result_df['medical_context']:
                if isinstance(contexts, list):
                    all_contexts.extend(contexts)
            if all_contexts:
                from collections import Counter
                context_counts = Counter(all_contexts)
                print(f"\nì˜ë£Œ í˜‘ì—… ë§¥ë½:")
                for context, count in context_counts.most_common():
                    print(f"  {context}: {count}ê±´")
        
        # ê¸°ì¡´ í†µê³„
        intensity_mean = result_df['emotion_intensity'].mean() if 'emotion_intensity' in result_df.columns else result_df['sentiment_intensity'].mean()
        confidence_mean = result_df['quality_score'].mean() if 'quality_score' in result_df.columns else result_df['confidence_score'].mean()
        
        print(f"\ní‰ê·  ê°ì • ê°•ë„: {intensity_mean:.1f}/10")
        print(f"í‰ê·  ì‹ ë¢°ë„: {confidence_mean:.1f}/10")
        
        # ì¶”ê°€ í†µê³„
        if 'is_anonymized' in result_df.columns:
            anonymized_count = result_df['is_anonymized'].sum()
            print(f"ë¹„ì‹ë³„ ì²˜ë¦¬ëœ í•­ëª©: {anonymized_count}ê°œ")
        
        # í’ˆì§ˆ í†µê³„
        needs_review_count = sum(1 for q in quality_results if q['needs_review'])
        reliable_count = sum(1 for q in quality_results if q['is_reliable'])
        print(f"ì¬ê²€í†  í•„ìš” í•­ëª©: {needs_review_count}ê°œ")
        print(f"ì‹ ë¢°ë„ ë†’ì€ í•­ëª©: {reliable_count}ê°œ")
        
        # ì£¼ìš” í‚¤ì›Œë“œ ì¶œë ¥
        print(f"\n=== ì£¼ìš” í‚¤ì›Œë“œ TOP 10 ====")
        for keyword, freq in list(keyword_freq.items())[:10]:
            print(f"  {keyword}: {freq}ë²ˆ")
        
        # ìˆ˜í•™ì /í†µê³„ì  ë¶„ì„ ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        if vector_analysis:
            print(f"\n=== ê°ì • ë²¡í„° ë¶„ì„ ê²°ê³¼ ====")
            print(f"ê°ì • ë‹¤ì–‘ì„± ì§€ìˆ˜: {vector_analysis.get('emotion_diversity', 0):.3f}")
            print(f"í‰ê·  ê°ì • ê·¹ì„±: {vector_analysis.get('avg_polarity', 0):.3f}")
            print(f"ê°ì • ì‘ì§‘ë„: {vector_analysis.get('emotion_cohesion', 0):.3f}")
            print(f"ê°•ë„ ì•ˆì •ì„±: {vector_analysis.get('intensity_stability', 0):.3f}")
            
            dist = vector_analysis.get('emotion_distribution', {})
            print(f"ê°ì • ë¶„í¬ - ê¸ì •: {dist.get('positive_ratio', 0):.1%}, "
                  f"ë¶€ì •: {dist.get('negative_ratio', 0):.1%}, "
                  f"ì¤‘ë¦½: {dist.get('neutral_ratio', 0):.1%}")
        
        if statistical_results:
            print(f"\n=== í†µê³„ì  ë¶„ì„ ê²°ê³¼ ====")
            desc_stats = statistical_results.get('descriptive_stats', {})
            intensity_stats = desc_stats.get('intensity', {})
            
            print(f"ê°ì • ê°•ë„ í†µê³„:")
            print(f"  í‰ê· : {intensity_stats.get('mean', 0):.2f} Â± {intensity_stats.get('std', 0):.2f}")
            print(f"  ì¤‘ì•™ê°’: {intensity_stats.get('median', 0):.2f}")
            print(f"  ì™œë„: {intensity_stats.get('skewness', 0):.3f}")
            print(f"  ì²¨ë„: {intensity_stats.get('kurtosis', 0):.3f}")
            
            # ì •ê·œì„± ê²€ì • ê²°ê³¼
            normality = statistical_results.get('normality_tests', {})
            intensity_norm = normality.get('intensity_shapiro', {})
            print(f"ì •ê·œì„± ê²€ì • (Shapiro-Wilk): p={intensity_norm.get('p_value', 0):.4f} "
                  f"({'ì •ê·œë¶„í¬' if intensity_norm.get('is_normal', False) else 'ë¹„ì •ê·œë¶„í¬'})")
            
            # ìƒê´€ê´€ê³„
            corr = statistical_results.get('correlations', {})
            print(f"ê°ì •-ê°•ë„ ìƒê´€ê³„ìˆ˜: r={corr.get('pearson_r', 0):.3f} "
                  f"({'ìœ ì˜' if corr.get('significant', False) else 'ë¹„ìœ ì˜'})")
            
            # ì‹ ë¢°êµ¬ê°„
            ci = statistical_results.get('confidence_intervals', {})
            intensity_ci = ci.get('intensity_95ci', [0, 0])
            print(f"ê°ì • ê°•ë„ 95% ì‹ ë¢°êµ¬ê°„: [{intensity_ci[0]:.2f}, {intensity_ci[1]:.2f}]")
            
            # ì •ë³´ ì´ë¡ 
            info_theory = statistical_results.get('information_theory', {})
            print(f"ê°ì • ì—”íŠ¸ë¡œí”¼: {info_theory.get('emotion_entropy', 0):.3f} "
                  f"(ìµœëŒ€: {info_theory.get('max_entropy', 0):.3f})")
            
        if advanced_metrics:
            print(f"\n=== ê³ ê¸‰ ê°ì • ë©”íŠ¸ë¦­ ====")
            print(f"ê°ì • ì¼ê´€ì„± ì§€ìˆ˜: {advanced_metrics.get('emotion_consistency_index', 0):.3f}")
            print(f"ê°ì • ë³µì¡ë„ ì§€ìˆ˜: {advanced_metrics.get('emotion_complexity_index', 0):.3f}")
            print(f"ì˜ë£Œ ë§¥ë½ ë‹¤ì–‘ì„±: {advanced_metrics.get('medical_context_diversity', 0):.3f}")
            print(f"ê°ì • ë³€ë™ì„± ì§€ìˆ˜: {advanced_metrics.get('emotional_volatility_index', 0):.3f}")
            print(f"ê°ì • ì§‘ì¤‘ë„ ì§€ìˆ˜: {advanced_metrics.get('emotion_concentration_index', 0):.3f}")
            print(f"ì•ˆì •ì„± ì ìˆ˜: {advanced_metrics.get('stability_score', 0):.3f}")
            print(f"ê· í˜•ì„± ì ìˆ˜: {advanced_metrics.get('balance_score', 0):.3f}")
            print(f"ì „ì²´ ì •êµì„±: {advanced_metrics.get('overall_sophistication', 0):.3f}")

def get_user_input():
    """
    ì‚¬ìš©ìë¡œë¶€í„° íŒŒì¼ ê²½ë¡œì™€ ì„¤ì •ì„ ì…ë ¥ë°›ìŠµë‹ˆë‹¤.
    """
    print("=== ê³ ë„í™”ëœ í…ìŠ¤íŠ¸ ë¶„ì„ ì‹œìŠ¤í…œ ====")
    print("- ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì„±ëŠ¥ í–¥ìƒ")
    print("- ê°ì • ê°•ë„ ì ìˆ˜ (1-10 ìŠ¤ì¼€ì¼)")
    print("- ì‹¤ì‹œê°„ ì§„í–‰ë¥  ëª¨ë‹ˆí„°ë§")
    print("- í‚¤ì›Œë“œ ì¶”ì¶œ ë° ë¹ˆë„ ë¶„ì„")
    print("- AI ë¶„ì„ í’ˆì§ˆ ê²€ì¦ ë° ì‹ ë¢°ë„ í‰ê°€")
    print()
    
    # ì…ë ¥ íŒŒì¼ ê²½ë¡œ
    while True:
        input_file = input("ë¶„ì„í•  ì—‘ì…€ íŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip()
        if not input_file:
            print("íŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            continue
        if not os.path.exists(input_file):
            print(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_file}")
            continue
        if not input_file.lower().endswith(('.xlsx', '.xls')):
            print("ì—‘ì…€ íŒŒì¼(.xlsx, .xls)ë§Œ ì§€ì›ë©ë‹ˆë‹¤.")
            continue
        break
    
    # ì»¬ëŸ¼ëª… ì…ë ¥
    try:
        df = pd.read_excel(input_file)
        print(f"\nì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {list(df.columns)}")
    except Exception as e:
        print(f"íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
        sys.exit(1)
    
    while True:
        column_name = input("ë¶„ì„í•  ì»¬ëŸ¼ëª…ì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()
        if not column_name:
            print("ì»¬ëŸ¼ëª…ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            continue
        if column_name not in df.columns:
            print(f"'{column_name}' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            continue
        break
    
    # ì¶œë ¥ íŒŒì¼ ê²½ë¡œ (ì„ íƒì‚¬í•­)
    output_file = input("ê²°ê³¼ íŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì—”í„°: ìë™ ìƒì„±): ").strip()
    if not output_file:
        output_file = str(Path(input_file).stem) + "_processed.xlsx"
    
    # ìµœëŒ€ ì²˜ë¦¬ í–‰ ìˆ˜ (ì„ íƒì‚¬í•­)
    max_rows = None
    max_rows_input = input("ìµœëŒ€ ì²˜ë¦¬ í–‰ ìˆ˜ ì…ë ¥ (ì—”í„°: ì „ì²´ ì²˜ë¦¬): ").strip()
    if max_rows_input.isdigit():
        max_rows = int(max_rows_input)
    
    return input_file, column_name, output_file, max_rows

def parse_arguments():
    """
    ëª…ë ¹í–‰ ì¸ìë¥¼ íŒŒì‹±í•©ë‹ˆë‹¤.
    """
    parser = argparse.ArgumentParser(
        description="ê³ ë„í™”ëœ í…ìŠ¤íŠ¸ ë¶„ì„ ì‹œìŠ¤í…œ - AIë¥¼ í™œìš©í•œ ê°ì • ë¶„ì„ ë° í…ìŠ¤íŠ¸ ì •ì œ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  1. ëŒ€í™”í˜• ëª¨ë“œ (ê¶Œì¥):
     python main.py
     
  2. ëª…ë ¹í–‰ ì¸ì ì‚¬ìš©:
     python main.py -i data.xlsx -c "í”¼ë“œë°±" -o result.xlsx
     
  3. ìƒì„¸ ì„¤ì •:
     python main.py -i data.xlsx -c "í”¼ë“œë°±" -m 1000 -b 20 --no-retry
     
  4. í”„ë¡œì íŠ¸ ID ë³€ê²½:
     python main.py -i data.xlsx -c "í”¼ë“œë°±" -p your-project-id

ê¸°ëŠ¥:
  - AI ê¸°ë°˜ ê°ì • ë¶„ì„ (ê¸ì •/ë¶€ì •/ì¤‘ë¦½)
  - í…ìŠ¤íŠ¸ ì •ì œ ë° ë¹„ì‹ë³„ ì²˜ë¦¬
  - í‚¤ì›Œë“œ ì¶”ì¶œ ë° ë¹ˆë„ ë¶„ì„
  - í’ˆì§ˆ ê²€ì¦ ë° ìë™ ì¬ê²€í† 
  - ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì„±ëŠ¥ ìµœì í™”
        """
    )
    
    parser.add_argument("--input", "-i", type=str, help="ì…ë ¥ ì—‘ì…€ íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--column", "-c", type=str, help="ë¶„ì„í•  ì»¬ëŸ¼ëª…")
    parser.add_argument("--output", "-o", type=str, help="ê²°ê³¼ íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: ì…ë ¥íŒŒì¼ëª…_processed.xlsx)")
    parser.add_argument("--max-rows", "-m", type=int, help="ìµœëŒ€ ì²˜ë¦¬ í–‰ ìˆ˜ (ê¸°ë³¸ê°’: ì „ì²´)")
    parser.add_argument("--project-id", "-p", type=str, default="mindmap-462708", help="Google Cloud í”„ë¡œì íŠ¸ ID (ê¸°ë³¸ê°’: mindmap-462708)")
    parser.add_argument("--batch-size", "-b", type=int, default=10, help="ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 10)")
    parser.add_argument("--no-batch", action="store_true", help="ë°°ì¹˜ ì²˜ë¦¬ ë¹„í™œì„±í™” (ìˆœì°¨ ì²˜ë¦¬)")
    parser.add_argument("--no-retry", action="store_true", help="í’ˆì§ˆ ì¬ê²€í†  ë¹„í™œì„±í™”")
    
    return parser.parse_args()

def main():
    """
    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ - í…ìŠ¤íŠ¸ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    # ëª…ë ¹í–‰ ì¸ì íŒŒì‹±
    args = parse_arguments()
    
    try:
        # ëª…ë ¹í–‰ ì¸ìê°€ ì œê³µëœ ê²½ìš°
        if args.input and args.column:
            input_file = args.input
            column_name = args.column
            output_file = args.output or str(Path(input_file).stem) + "_processed.xlsx"
            max_rows = args.max_rows
            
            # íŒŒì¼ ì¡´ì¬ í™•ì¸
            if not os.path.exists(input_file):
                print(f"ì˜¤ë¥˜: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ - {input_file}")
                sys.exit(1)
            
            # ì»¬ëŸ¼ ì¡´ì¬ í™•ì¸
            try:
                df = pd.read_excel(input_file)
                if column_name not in df.columns:
                    print(f"ì˜¤ë¥˜: '{column_name}' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    print(f"ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {list(df.columns)}")
                    sys.exit(1)
            except Exception as e:
                print(f"íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
                sys.exit(1)
        else:
            # ëŒ€í™”í˜• ì…ë ¥
            input_file, column_name, output_file, max_rows = get_user_input()
        
        print(f"\nì„¤ì • í™•ì¸:")
        print(f"- ì…ë ¥ íŒŒì¼: {input_file}")
        print(f"- ë¶„ì„ ì»¬ëŸ¼: {column_name}")
        print(f"- ì¶œë ¥ íŒŒì¼: {output_file}")
        print(f"- ìµœëŒ€ ì²˜ë¦¬ í–‰: {max_rows or 'ì „ì²´'}")
        print(f"- í”„ë¡œì íŠ¸ ID: {args.project_id}")
        print()
        
        # ë¶„ì„ê¸° ìƒì„± ë° ì‹¤í–‰
        analyzer = ReviewAnalyzer(project_id=args.project_id)
        
        # ì •ì œëœ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ í™•ì¸
        if "_refined_text" in column_name:
            # ì´ë¯¸ ì •ì œëœ í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ê²½ìš°
            anonymized_column = column_name.replace("_refined_text", "_is_anonymized")
            print(f"ì •ì œëœ í…ìŠ¤íŠ¸ ë¶„ì„ ëª¨ë“œ")
            print(f"ì •ì œ í…ìŠ¤íŠ¸ ì»¬ëŸ¼: {column_name}")
            print(f"ë¹„ì‹ë³„ ì²˜ë¦¬ ì»¬ëŸ¼: {anonymized_column}")
            
            analyzer.process_xlsx_with_refined_text(
                input_file,
                column_name,
                anonymized_column,
                output_file,
                max_rows=max_rows,
                use_batch=not args.no_batch,
                batch_size=args.batch_size,
                enable_quality_retry=not args.no_retry
            )
        else:
            # ì›ë³¸ í…ìŠ¤íŠ¸ ë¶„ì„ (ê¸°ì¡´ ë°©ì‹)
            print(f"ì›ë³¸ í…ìŠ¤íŠ¸ ë¶„ì„ ëª¨ë“œ")
            analyzer.process_xlsx_with_column(
                input_file, 
                column_name, 
                output_file, 
                max_rows=max_rows,
                use_batch=not args.no_batch,
                batch_size=args.batch_size,
                enable_quality_retry=not args.no_retry
            )
        
    except KeyboardInterrupt:
        print("\n\nì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        sys.exit(0)
    except Exception as e:
        import traceback
        print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
        print(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        print("\ní•´ê²° ë°©ë²•:")
        print("1. Google Cloud í”„ë¡œì íŠ¸ ID í™•ì¸")
        print("2. Vertex AI API í™œì„±í™” í™•ì¸")
        print("3. ì¸ì¦ ì„¤ì • í™•ì¸ (gcloud auth application-default login)")
        print("4. ì—‘ì…€ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸")
        print("5. í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜ í™•ì¸ (pandas, openpyxl, tqdm)")
        sys.exit(1)

# í”„ë¡œê·¸ë¨ì´ ì§ì ‘ ì‹¤í–‰ë  ë•Œë§Œ main() í•¨ìˆ˜ í˜¸ì¶œ
if __name__ == "__main__":
    main() 