import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

def apply_clustering_to_analysis():
    """ì‹¤ì œë¡œ í´ëŸ¬ìŠ¤í„°ë§ì„ ì ìš©í•˜ëŠ” í•¨ìˆ˜"""
    
    print("ğŸ”§ í˜‘ì—… í›„ê¸° í´ëŸ¬ìŠ¤í„°ë§ ì ìš© ì‹œì‘")
    print("=" * 50)
    
    try:
        # 1. ë¶„ì„ëœ íŒŒì¼ ì½ê¸°
        print("ğŸ“ ë¶„ì„ íŒŒì¼ ë¡œë“œ ì¤‘...")
        df = pd.read_excel('í˜‘ì—…í›„ê¸°_ë¶„ì„ê²°ê³¼_ìƒìœ„200ê±´.xlsx', engine='openpyxl')
        print(f"âœ… {len(df)}ê±´ ë¡œë“œ ì™„ë£Œ")
        
        # 2. ìœ íš¨í•œ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ (ì˜ë£Œìš©ì–´ ì œì™¸)
        print("\nğŸ“Š í´ëŸ¬ìŠ¤í„°ë§ ëŒ€ìƒ í…ìŠ¤íŠ¸ ì¤€ë¹„...")
        valid_mask = df['í˜‘ì—…í›„ê¸°_ì •ì œí…ìŠ¤íŠ¸'].notna() & (df['í˜‘ì—…í›„ê¸°_ì •ì œí…ìŠ¤íŠ¸'].str.len() > 3)
        valid_df = df[valid_mask].copy()
        
        texts = valid_df['í˜‘ì—…í›„ê¸°_ì •ì œí…ìŠ¤íŠ¸'].tolist()
        print(f"âœ… ìœ íš¨í•œ í…ìŠ¤íŠ¸: {len(texts)}ê±´")
        
        if len(texts) < 5:
            print("âŒ í´ëŸ¬ìŠ¤í„°ë§ì„ ìœ„í•œ í…ìŠ¤íŠ¸ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
            return
        
        # 3. TF-IDF ë²¡í„°í™” (ì˜ë£Œìš©ì–´ ì œì™¸ ì²˜ë¦¬)
        print("\nğŸ”¤ TF-IDF ë²¡í„°í™” ì¤‘...")
        
        # í•œêµ­ì–´ ë¶ˆìš©ì–´ ë° ì˜ë¯¸ì—†ëŠ” ë‹¨ì–´ë“¤
        stop_words = [
            'ì—†ìŠµë‹ˆë‹¤', 'ê°ì‚¬í•©ë‹ˆë‹¤', 'ë§Œì¡±', 'í•­ìƒ', 'ë§¤ìš°', 'ì •ë§', 'ë„ˆë¬´', 'ì•„ì£¼',
            'ì—†ìŒ', 'í•´ë‹¹ì—†ìŒ', 'ë¬´', '...', '....', '.', 'x000d'
        ]
        
        vectorizer = TfidfVectorizer(
            max_features=80,           # íŠ¹ì„± ìˆ˜ ì¡°ì •
            min_df=2,                  # ìµœì†Œ 2ë²ˆ ì´ìƒ ë“±ì¥
            max_df=0.8,                # 80% ì´ìƒ ë¬¸ì„œì— ë“±ì¥í•˜ëŠ” ë‹¨ì–´ ì œì™¸
            ngram_range=(1, 2),        # 1-2 ë‹¨ì–´ ì¡°í•©
            stop_words=stop_words      # ë¶ˆìš©ì–´ ì œì™¸
        )
        
        tfidf_matrix = vectorizer.fit_transform(texts)
        print(f"âœ… ë²¡í„° í¬ê¸°: {tfidf_matrix.shape}")
        
        # 4. ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ ê²°ì •
        n_clusters = min(8, max(3, len(texts) // 20))  # 3~8ê°œ í´ëŸ¬ìŠ¤í„°
        print(f"\nğŸ¯ {n_clusters}ê°œ í´ëŸ¬ìŠ¤í„°ë¡œ K-means ì‹¤í–‰...")
        
        # 5. K-means í´ëŸ¬ìŠ¤í„°ë§
        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        clusters = kmeans.fit_predict(tfidf_matrix)
        
        # 6. í´ëŸ¬ìŠ¤í„° ë¶„í¬ í™•ì¸
        cluster_counts = Counter(clusters)
        print(f"ğŸ“Š í´ëŸ¬ìŠ¤í„° ë¶„í¬: {dict(cluster_counts)}")
        
        # 7. ì›ë³¸ DataFrameì— í´ëŸ¬ìŠ¤í„° ê²°ê³¼ ì¶”ê°€
        print("\nğŸ“‹ í´ëŸ¬ìŠ¤í„° ê²°ê³¼ë¥¼ ì›ë³¸ ë°ì´í„°ì— ì¶”ê°€...")
        df['í˜‘ì—…í›„ê¸°_í´ëŸ¬ìŠ¤í„°'] = np.nan
        df.loc[valid_mask, 'í˜‘ì—…í›„ê¸°_í´ëŸ¬ìŠ¤í„°'] = clusters
        
        # í´ëŸ¬ìŠ¤í„° ë²ˆí˜¸ë¥¼ ì˜ë¯¸ìˆëŠ” ì´ë¦„ìœ¼ë¡œ ë³€ê²½
        cluster_names = {}
        for i in range(n_clusters):
            cluster_names[i] = f"ê·¸ë£¹{i+1}"
        
        df['í˜‘ì—…í›„ê¸°_í´ëŸ¬ìŠ¤í„°ê·¸ë£¹'] = df['í˜‘ì—…í›„ê¸°_í´ëŸ¬ìŠ¤í„°'].map(cluster_names)
        
        # 8. í´ëŸ¬ìŠ¤í„°ë³„ ìƒì„¸ ë¶„ì„
        print("\nğŸ“ í´ëŸ¬ìŠ¤í„°ë³„ íŠ¹ì„± ë¶„ì„...")
        cluster_summary = []
        feature_names = vectorizer.get_feature_names_out()
        
        for i in range(n_clusters):
            cluster_mask = (df['í˜‘ì—…í›„ê¸°_í´ëŸ¬ìŠ¤í„°'] == i)
            cluster_data = df[cluster_mask]
            
            # í´ëŸ¬ìŠ¤í„° ëŒ€í‘œ í‚¤ì›Œë“œ (ìƒìœ„ 5ê°œ)
            cluster_center = kmeans.cluster_centers_[i]
            top_indices = cluster_center.argsort()[-5:][::-1]
            keywords = [feature_names[idx] for idx in top_indices]
            
            # ê°ì • ë¶„í¬
            sentiments = cluster_data['í˜‘ì—…í›„ê¸°_ì£¼ê°ì •'].value_counts()
            main_sentiment = sentiments.index[0] if len(sentiments) > 0 else "ì¤‘ë¦½"
            sentiment_dist = dict(sentiments)
            
            # ì„¸ë¶€ê°ì • ë¶„í¬
            detailed_sentiments = cluster_data['í˜‘ì—…í›„ê¸°_ì„¸ë¶€ê°ì •'].value_counts()
            main_detailed = detailed_sentiments.index[0] if len(detailed_sentiments) > 0 else "ê¸°íƒ€"
            
            # í‰ê·  ê°ì • ê°•ë„
            avg_intensity = cluster_data['í˜‘ì—…í›„ê¸°_ê°ì •ê°•ë„'].mean() if len(cluster_data) > 0 else 5.0
            
            # ëŒ€í‘œ ì˜ˆì‹œ (ê°€ì¥ ì„¼í„°ì— ê°€ê¹Œìš´ í…ìŠ¤íŠ¸)
            if len(cluster_data) > 0:
                cluster_texts = cluster_data['í˜‘ì—…í›„ê¸°_ì •ì œí…ìŠ¤íŠ¸'].tolist()
                # í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ì— ê°€ì¥ ê°€ê¹Œìš´ í…ìŠ¤íŠ¸ ì°¾ê¸°
                cluster_indices = cluster_data.index[cluster_data.index.isin(valid_df.index)]
                if len(cluster_indices) > 0:
                    cluster_tfidf = tfidf_matrix[[valid_df.index.get_loc(idx) for idx in cluster_indices]]
                    similarities = cosine_similarity(cluster_tfidf, [cluster_center])
                    best_idx = similarities.flatten().argmax()
                    representative_text = cluster_texts[best_idx]
                else:
                    representative_text = cluster_texts[0]
            else:
                representative_text = ""
            
            cluster_summary.append({
                'í´ëŸ¬ìŠ¤í„°ê·¸ë£¹': f"ê·¸ë£¹{i+1}",
                'í”¼ë“œë°±ìˆ˜': len(cluster_data),
                'ë¹„ìœ¨': f"{len(cluster_data)/len(df)*100:.1f}%",
                'ì£¼ìš”í‚¤ì›Œë“œ': ' | '.join(keywords[:3]),  # ìƒìœ„ 3ê°œë§Œ
                'ì „ì²´í‚¤ì›Œë“œ': ' | '.join(keywords),
                'ì£¼ê°ì •': main_sentiment,
                'ì„¸ë¶€ê°ì •': main_detailed,
                'í‰ê· ê°ì •ê°•ë„': f"{avg_intensity:.1f}",
                'ê°ì •ë¶„í¬': str(sentiment_dist),
                'ëŒ€í‘œì˜ˆì‹œ': representative_text[:80] + "..." if len(representative_text) > 80 else representative_text
            })
            
            print(f"  ê·¸ë£¹{i+1}: {len(cluster_data)}ê±´ ({len(cluster_data)/len(df)*100:.1f}%) - {' | '.join(keywords[:2])}")
        
        # 9. ìœ ì‚¬ë„ê°€ ë†’ì€ í”¼ë“œë°± ìŒ ì°¾ê¸°
        print("\nğŸ“ ìœ ì‚¬ë„ ë†’ì€ í”¼ë“œë°± ìŒ ë¶„ì„...")
        similarity_pairs = []
        
        # ìƒ˜í”Œë¡œ ìƒìœ„ 50ê±´ë§Œ ê³„ì‚° (ê³„ì‚° ì‹œê°„ ë‹¨ì¶•)
        sample_size = min(50, len(texts))
        sample_matrix = tfidf_matrix[:sample_size]
        similarity_matrix = cosine_similarity(sample_matrix)
        
        for i in range(sample_size):
            for j in range(i+1, sample_size):
                similarity = similarity_matrix[i][j]
                if similarity > 0.3:  # 30% ì´ìƒ ìœ ì‚¬í•œ ê²½ìš°
                    similarity_pairs.append({
                        'í…ìŠ¤íŠ¸1_ë²ˆí˜¸': i+1,
                        'í…ìŠ¤íŠ¸2_ë²ˆí˜¸': j+1,
                        'ìœ ì‚¬ë„': f"{similarity:.3f}",
                        'í…ìŠ¤íŠ¸1': texts[i][:50] + "...",
                        'í…ìŠ¤íŠ¸2': texts[j][:50] + "...",
                        'í´ëŸ¬ìŠ¤í„°1': f"ê·¸ë£¹{clusters[i]+1}",
                        'í´ëŸ¬ìŠ¤í„°2': f"ê·¸ë£¹{clusters[j]+1}"
                    })
        
        print(f"âœ… ìœ ì‚¬ë„ 30% ì´ìƒì¸ í”¼ë“œë°± ìŒ: {len(similarity_pairs)}ê°œ ë°œê²¬")
        
        # 10. ê²°ê³¼ë¥¼ Excel íŒŒì¼ë¡œ ì €ì¥
        print("\nğŸ’¾ í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ì €ì¥...")
        output_file = 'í˜‘ì—…í›„ê¸°_ë¶„ì„ê²°ê³¼_í´ëŸ¬ìŠ¤í„°ë§í¬í•¨_ìƒìœ„200ê±´.xlsx'
        
        with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
            # ë©”ì¸ ì‹œíŠ¸: ê¸°ì¡´ ë¶„ì„ + í´ëŸ¬ìŠ¤í„° ê²°ê³¼
            df.to_excel(writer, sheet_name='ë¶„ì„ê²°ê³¼_í´ëŸ¬ìŠ¤í„°í¬í•¨', index=False)
            
            # í´ëŸ¬ìŠ¤í„° ìš”ì•½ ì‹œíŠ¸
            pd.DataFrame(cluster_summary).to_excel(writer, sheet_name='í´ëŸ¬ìŠ¤í„°ë³„ìš”ì•½', index=False)
            
            # ìœ ì‚¬ í”¼ë“œë°± ìŒ ì‹œíŠ¸
            if similarity_pairs:
                pd.DataFrame(similarity_pairs).to_excel(writer, sheet_name='ìœ ì‚¬í”¼ë“œë°±ìŒ', index=False)
        
        print(f"âœ… ì €ì¥ ì™„ë£Œ: {output_file}")
        
        # 11. ìµœì¢… ìš”ì•½ ì¶œë ¥
        print(f"\nğŸ‰ í´ëŸ¬ìŠ¤í„°ë§ ì ìš© ì™„ë£Œ!")
        print(f"ğŸ“Š ì´ {n_clusters}ê°œ ê·¸ë£¹ìœ¼ë¡œ ë¶„ë¥˜")
        print(f"ğŸ“‹ ì¶”ê°€ëœ ì»¬ëŸ¼: 'í˜‘ì—…í›„ê¸°_í´ëŸ¬ìŠ¤í„°', 'í˜‘ì—…í›„ê¸°_í´ëŸ¬ìŠ¤í„°ê·¸ë£¹'")
        print(f"ğŸ“‘ ìƒì„±ëœ ì‹œíŠ¸: 'í´ëŸ¬ìŠ¤í„°ë³„ìš”ì•½', 'ìœ ì‚¬í”¼ë“œë°±ìŒ'")
        
        print(f"\nğŸ“ˆ ê·¸ë£¹ë³„ ë¶„í¬:")
        for summary in cluster_summary:
            print(f"  {summary['í´ëŸ¬ìŠ¤í„°ê·¸ë£¹']}: {summary['í”¼ë“œë°±ìˆ˜']}ê±´ ({summary['ë¹„ìœ¨']}) - {summary['ì£¼ìš”í‚¤ì›Œë“œ']}")
        
        return output_file, cluster_summary
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return None, None

if __name__ == "__main__":
    result_file, summary = apply_clustering_to_analysis()
    
    if result_file:
        print(f"\n{'='*60}")
        print("ğŸ¯ í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ í™œìš© ê°€ì´ë“œ:")
        print("1. Excel íŒŒì¼ì„ ì—´ì–´ 'í´ëŸ¬ìŠ¤í„°ë³„ìš”ì•½' ì‹œíŠ¸ í™•ì¸")
        print("2. 'ìœ ì‚¬í”¼ë“œë°±ìŒ' ì‹œíŠ¸ì—ì„œ ì¤‘ë³µ ì´ìŠˆ í™•ì¸")
        print("3. ë©”ì¸ ì‹œíŠ¸ì—ì„œ ê·¸ë£¹ë³„ í•„í„°ë§í•˜ì—¬ íŒ¨í„´ ë¶„ì„")
        print("4. ê° ê·¸ë£¹ì˜ ëŒ€í‘œ ì´ìŠˆë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°œì„  ê³„íš ìˆ˜ë¦½")