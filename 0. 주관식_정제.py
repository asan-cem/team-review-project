# ë°ì´í„° ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤
import pandas as pd  # ì—‘ì…€, CSV íŒŒì¼ ì²˜ë¦¬
import json  # JSON ë°ì´í„° ì²˜ë¦¬
import time  # ëŒ€ê¸° ì‹œê°„ ì²˜ë¦¬
from pathlib import Path  # íŒŒì¼ ê²½ë¡œ ì²˜ë¦¬
from tqdm import tqdm  # ì§„í–‰ë¥  í‘œì‹œ
from concurrent.futures import ThreadPoolExecutor, as_completed  # ë³‘ë ¬ ì²˜ë¦¬
import re  # ì •ê·œí‘œí˜„ì‹
import sys  # ëª…ë ¹í–‰ ì¸ì ì²˜ë¦¬
import warnings  # ê²½ê³  ë©”ì‹œì§€ ì œì–´
import pickle  # ì²´í¬í¬ì¸íŠ¸ ì§ë ¬í™”
import logging  # ë¡œê¹…
from multiprocessing import Process, Queue  # ë°±ê·¸ë¼ìš´ë“œ í”„ë¡œì„¸ì‹±
from collections import Counter  # í‚¤ì›Œë“œ ì¹´ìš´íŒ…
warnings.filterwarnings('ignore')

# Google Gemini API ë¼ì´ë¸ŒëŸ¬ë¦¬
import google.generativeai as genai  # Google Gemini API

# ê°ì • ë¶„ë¥˜ ìƒìˆ˜ ì •ì˜
EMOTION_CATEGORIES = {
    "ê¸ì •êµ°": ["ê¸°ì¨", "ê°ì‚¬", "ì‹ ë¢°", "ë§Œì¡±"],
    "ë¶€ì •êµ°": ["ë¶„ë…¸", "ìŠ¬í””", "ë‘ë ¤ì›€", "ì‹¤ë§"], 
    "ì¤‘ë¦½êµ°": ["í‰ì˜¨", "ë¬´ê´€ì‹¬"]
}

MEDICAL_CONTEXT_CATEGORIES = {
    "ì˜ë£Œ_ì„œë¹„ìŠ¤": ["ì‘ê¸‰ìƒí™©", "íˆ¬ì•½ì˜¤ë¥˜", "ìˆ˜ìˆ í˜‘ë ¥", "ì•ˆì „ì‚¬ê³ ", "ì˜ë£Œì‚¬ê³ "],
    "ì—…ë¬´_íš¨ìœ¨": ["ì¼ì •ì¡°ìœ¨", "ì •ë³´ê³µìœ ", "í”„ë¡œì„¸ìŠ¤", "ì—…ë¬´ë¶„ë‹´", "íš¨ìœ¨ì„±"],
    "ì¡´ì¤‘_ì†Œí†µ": ["ì¡´ì¤‘", "ì†Œí†µ", "ë°°ë ¤", "ì˜ˆì˜", "ì¹œì ˆ"],
    "ì „ë¬¸ì„±": ["ì§€ì‹", "ê¸°ìˆ ", "ê²½í—˜", "ì—­ëŸ‰", "ì „ë¬¸ì„±"]
}

# AIì—ê²Œ ë³´ë‚¼ ë¶„ì„ ì§€ì‹œì‚¬í•­ í…œí”Œë¦¿ (ê³ ë„í™” ë²„ì „)
PROMPT_TEMPLATE = """
[í˜ë¥´ì†Œë‚˜]
ë‹¹ì‹ ì€ ì˜ë£Œì§„ ê°„ í˜‘ì—… í”¼ë“œë°±ì„ ë¶„ì„í•˜ëŠ” ì „ë¬¸ AI ë¶„ì„ê°€ì…ë‹ˆë‹¤. 8ê°€ì§€ ì„¸ë¶„í™”ëœ ê°ì •ê³¼ ë³µí•© ê°ì • ë¶„ì„ì„ í†µí•´ ì •í™•í•˜ê³  ê¹Šì´ ìˆëŠ” ê°ì • ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

[ì§€ì‹œì‚¬í•­]
1. **í…ìŠ¤íŠ¸ ì •ì œ ë° ì˜ë¯¸ íŒë‹¨**:
   - ë¨¼ì € ì›ë³¸ í…ìŠ¤íŠ¸ê°€ ì˜ë¯¸ìˆëŠ” ë‚´ìš©ì¸ì§€ íŒë‹¨í•©ë‹ˆë‹¤.
   - 'ì—†ìŠµë‹ˆë‹¤', 'ì—†ìŒ', 'íŠ¹ë³„íˆ ì—†ìŒ', 'í•´ë‹¹ ì—†ìŒ', 'ì—†ë‹¤', 'ì—†ì–´ìš”' ë“±ì˜ í‘œí˜„ë§Œ ìˆëŠ” ê²½ìš° ì •ì œëœ_í…ìŠ¤íŠ¸ë¥¼ ë¹ˆ ë¬¸ìì—´("")ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
   - ë¶ˆí•„ìš”í•œ ê¸°í˜¸ë‚˜ êµ¬ë¶„ìë§Œ ìˆëŠ” ê²½ìš° (ì˜ˆ: "---", "...", "//", "ã…¡ã…¡", "ë¬´" ë“±) ì •ì œëœ_í…ìŠ¤íŠ¸ë¥¼ ë¹ˆ ë¬¸ìì—´("")ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
   - ì˜ë¯¸ ì—†ëŠ” ë‹¨ìˆœ ë°˜ë³µ ë¬¸ìë‚˜ ìˆ«ìë§Œ ìˆëŠ” ê²½ìš° ì •ì œëœ_í…ìŠ¤íŠ¸ë¥¼ ë¹ˆ ë¬¸ìì—´("")ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
   - ì˜ë¯¸ìˆëŠ” ë‚´ìš©ì´ ìˆëŠ” ê²½ìš°ì—ë§Œ í•µì‹¬ ì˜ë¯¸ë¥¼ ë³´ì¡´í•˜ë©´ì„œ ì˜¤íƒ€ì™€ ë¬¸ë²•ì„ êµì •í•˜ì—¬ ì •ì œëœ_í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

2. **í‘œí˜„ ìˆœí™”**:
   - ì†ì–´ë‚˜ ê³µê²©ì ì¸ í‘œí˜„ì€ ì „ë¬¸ì ì´ê³  ì •ì¤‘í•œ í‘œí˜„ìœ¼ë¡œ ìˆœí™”í•©ë‹ˆë‹¤.

3. **ë§¤ìš° ì¤‘ìš” - ë¹„ì‹ë³„ ì²˜ë¦¬ ê·œì¹™**: 
   **3-1. ê¸ì •ì /ì¤‘ë¦½ì  í”¼ë“œë°± ì²˜ë¦¬ ê·œì¹™:**
   - ê¸ì •ì ì´ê±°ë‚˜ ì¤‘ë¦½ì  í”¼ë“œë°±ì€ ì‹¤ëª…ì´ í¬í•¨ë˜ì–´ ìˆì–´ë„ ì ˆëŒ€ ë¹„ì‹ë³„ ì²˜ë¦¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
   - ë¹„ì‹ë³„_ì²˜ë¦¬ë¥¼ ë°˜ë“œì‹œ falseë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
   
   **3-2. ë¶€ì •ì  í”¼ë“œë°± ì²˜ë¦¬ ê·œì¹™:**
   - ë¶€ì •ì  í”¼ë“œë°±ì´ë©´ì„œ ì‹¤ëª…ì´ë‚˜ ë§¤ìš° êµ¬ì²´ì ì¸ ê°œì¸ ì‹ë³„ ì •ë³´ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ë¹„ì‹ë³„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
   - ì¼ë°˜ì ì¸ í˜¸ì¹­("ì„ ìƒë‹˜", "ì§ì›ë¶„" ë“±)ì€ ë¹„ì‹ë³„ ì²˜ë¦¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

4. **ê°ì • ë¶„ì„**:
   - ê°ì •_ë¶„ë¥˜: ê¸ì •, ë¶€ì •, ì¤‘ë¦½ ì¤‘ 1ê°œ (ë¬´ì˜ë¯¸í•œ í…ìŠ¤íŠ¸ëŠ” ë¹ˆ ë¬¸ìì—´ "")
   - ê°ì •_ê°•ë„_ì ìˆ˜: 1-10 (ë¬´ì˜ë¯¸í•œ í…ìŠ¤íŠ¸ëŠ” ë¹ˆ ë¬¸ìì—´ "")

5. **ì˜ë£Œ í˜‘ì—… ë§¥ë½ ë¶„ì„**:
   - ì˜ë£Œ_ë§¥ë½: ì˜ë£Œ_ì„œë¹„ìŠ¤, ì—…ë¬´_íš¨ìœ¨, ì¡´ì¤‘_ì†Œí†µ, ì „ë¬¸ì„± ì¤‘ í•´ë‹¹í•˜ëŠ” ëª¨ë“  í•­ëª© (ë¬´ì˜ë¯¸í•œ í…ìŠ¤íŠ¸ëŠ” ë¹ˆ ë°°ì—´ [])

6. **ì‹ ë¢°ë„ í‰ê°€**:
   - ì‹ ë¢°ë„_ì ìˆ˜ë¥¼ 1-10 ìŠ¤ì¼€ì¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤ (ë¬´ì˜ë¯¸í•œ í…ìŠ¤íŠ¸ëŠ” ë¹ˆ ë¬¸ìì—´ "").

7. **í‚¤ì›Œë“œ ì¶”ì¶œ**:
   - í•µì‹¬_í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ì œê³µí•©ë‹ˆë‹¤ (ë¬´ì˜ë¯¸í•œ í…ìŠ¤íŠ¸ëŠ” ë¹ˆ ë°°ì—´ [], ì˜ë¯¸ìˆëŠ” í…ìŠ¤íŠ¸ëŠ” ì£¼ìš” í‚¤ì›Œë“œ 3-5ê°œ).

[ê°ì • ë¶„ë¥˜ ê°€ì´ë“œ]
**ê¸ì •êµ°:**
- ê¸°ì¨: "ê¸°ë»ìš”", "ì¦ê±°ì›Œìš”", "í–‰ë³µí•´ìš”"
- ê°ì‚¬: "ê°ì‚¬í•©ë‹ˆë‹¤", "ê³ ë§ˆì›Œìš”", "ë„ì›€ì´ ë˜ì—ˆì–´ìš”"
- ì‹ ë¢°: "ë¯¿ì„ ìˆ˜ ìˆì–´ìš”", "ì „ë¬¸ì ì´ì—ìš”", "ì•ˆì‹¬ì´ ë¼ìš”"
- ë§Œì¡±: "ë§Œì¡±í•´ìš”", "ì¢‹ì•˜ì–´ìš”", "í›Œë¥­í•´ìš”"

**ë¶€ì •êµ°:**
- ë¶„ë…¸: "í™”ë‚˜ìš”", "ì§œì¦ë‚˜ìš”", "ë¶„í•´ìš”"
- ìŠ¬í””: "ìŠ¬í¼ìš”", "ìš°ìš¸í•´ìš”", "ë§ˆìŒì´ ì•„íŒŒìš”"
- ë‘ë ¤ì›€: "ë¬´ì„œì›Œìš”", "ê±±ì •ë¼ìš”", "ë¶ˆì•ˆí•´ìš”"
- ì‹¤ë§: "ì‹¤ë§í•´ìš”", "ì•„ì‰¬ì›Œìš”", "ê¸°ëŒ€ì— ëª» ë¯¸ì³ìš”"

**ì¤‘ë¦½êµ°:**
- í‰ì˜¨: "ê´œì°®ì•„ìš”", "ë³´í†µì´ì—ìš”", "í‰ë²”í•´ìš”"
- ë¬´ê´€ì‹¬: "ìƒê´€ì—†ì–´ìš”", "ê·¸ëƒ¥ ê·¸ë˜ìš”"

[ì¶œë ¥ í˜•ì‹]
- JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.
- ì•„ë˜ í‚¤ë“¤ì„ ëª¨ë‘ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.

ì˜ˆì‹œ í˜•ì‹:
{{
  "ì •ì œëœ_í…ìŠ¤íŠ¸": "ìµœì¢… ì •ì œ ë° ë¹„ì‹ë³„ ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸",
  "ë¹„ì‹ë³„_ì²˜ë¦¬": false,
  "ê°ì •_ë¶„ë¥˜": "ê¸ì •",
  "ê°ì •_ê°•ë„_ì ìˆ˜": 7,
  "í•µì‹¬_í‚¤ì›Œë“œ": ["ê°ì‚¬", "ì „ë¬¸ì ", "ë„ì›€"],
  "ì˜ë£Œ_ë§¥ë½": ["ì¡´ì¤‘_ì†Œí†µ", "ì „ë¬¸ì„±"],
  "ì‹ ë¢°ë„_ì ìˆ˜": 8
}}


[ì˜ˆì‹œ]
- ì›ë³¸ í…ìŠ¤íŠ¸: "ê¹€ì² ìˆ˜ íŒ€ì¥ ì¼ì²˜ë¦¬ ë„ˆë¬´ ë‹µë‹µí•˜ê³  ì†Œí†µë„ ì•ˆë¨. ê°œì„ ì´ ì‹œê¸‰í•¨"
- JSON ì¶œë ¥:
{{"ì •ì œëœ_í…ìŠ¤íŠ¸": "ë‹´ë‹¹ìì˜ ì¼ ì²˜ë¦¬ê°€ ë‹¤ì†Œ ì•„ì‰½ê³ , ì†Œí†µ ë°©ì‹ì˜ ê°œì„ ì´ í•„ìš”í•´ ë³´ì…ë‹ˆë‹¤.", "ë¹„ì‹ë³„_ì²˜ë¦¬": true, "ê°ì •_ë¶„ë¥˜": "ë¶€ì •", "ê°ì •_ê°•ë„_ì ìˆ˜": 8, "í•µì‹¬_í‚¤ì›Œë“œ": ["ì¼ì²˜ë¦¬", "ì†Œí†µ", "ê°œì„ "], "ì˜ë£Œ_ë§¥ë½": ["ì—…ë¬´_íš¨ìœ¨", "ì¡´ì¤‘_ì†Œí†µ"], "ì‹ ë¢°ë„_ì ìˆ˜": 9}}

- ì›ë³¸ í…ìŠ¤íŠ¸: "ì„ ìƒë‹˜ë“¤ì´ ì—…ë¬´ ì²˜ë¦¬ê°€ ë„ˆë¬´ ëŠë ¤ì„œ ë‹µë‹µí•©ë‹ˆë‹¤"
- JSON ì¶œë ¥:
{{"ì •ì œëœ_í…ìŠ¤íŠ¸": "ì„ ìƒë‹˜ë“¤ì˜ ì—…ë¬´ ì²˜ë¦¬ ì†ë„ê°€ ë‹¤ì†Œ ì•„ì‰½ìŠµë‹ˆë‹¤.", "ë¹„ì‹ë³„_ì²˜ë¦¬": false, "ê°ì •_ë¶„ë¥˜": "ë¶€ì •", "ê°ì •_ê°•ë„_ì ìˆ˜": 6, "í•µì‹¬_í‚¤ì›Œë“œ": ["ì—…ë¬´", "ì²˜ë¦¬", "ì†ë„"], "ì˜ë£Œ_ë§¥ë½": ["ì—…ë¬´_íš¨ìœ¨"], "ì‹ ë¢°ë„_ì ìˆ˜": 7}}

- ì›ë³¸ í…ìŠ¤íŠ¸: "ë°•ì˜í¬ ì„ ìƒë‹˜ì€ í•­ìƒ ë™ë£Œë“¤ì„ ë¨¼ì € ì±™ê¸°ê³  ë°°ë ¤í•˜ëŠ” ëª¨ìŠµì´ ë³´ê¸° ì¢‹ìŠµë‹ˆë‹¤"
- JSON ì¶œë ¥:
{{"ì •ì œëœ_í…ìŠ¤íŠ¸": "ë°•ì˜í¬ ì„ ìƒë‹˜ì€ í•­ìƒ ë™ë£Œë“¤ì„ ë¨¼ì € ì±™ê¸°ê³  ë°°ë ¤í•˜ëŠ” ëª¨ìŠµì´ ë³´ê¸° ì¢‹ìŠµë‹ˆë‹¤.", "ë¹„ì‹ë³„_ì²˜ë¦¬": false, "ê°ì •_ë¶„ë¥˜": "ê¸ì •", "ê°ì •_ê°•ë„_ì ìˆ˜": 8, "í•µì‹¬_í‚¤ì›Œë“œ": ["ë°°ë ¤", "ë™ë£Œ", "ì±™ê¹€"], "ì˜ë£Œ_ë§¥ë½": ["ì¡´ì¤‘_ì†Œí†µ"], "ì‹ ë¢°ë„_ì ìˆ˜": 9}}

- ì›ë³¸ í…ìŠ¤íŠ¸: "ìˆ˜ìˆ  ì¤‘ ì‘ê¸‰ìƒí™©ì—ì„œ ë¹ ë¥¸ ëŒ€ì‘ì´ ì¸ìƒì ì´ì—ˆìŠµë‹ˆë‹¤"
- JSON ì¶œë ¥:
{{"ì •ì œëœ_í…ìŠ¤íŠ¸": "ìˆ˜ìˆ  ì¤‘ ì‘ê¸‰ìƒí™©ì—ì„œ ë¹ ë¥¸ ëŒ€ì‘ì´ ì¸ìƒì ì´ì—ˆìŠµë‹ˆë‹¤.", "ë¹„ì‹ë³„_ì²˜ë¦¬": false, "ê°ì •_ë¶„ë¥˜": "ê¸ì •", "ê°ì •_ê°•ë„_ì ìˆ˜": 8, "í•µì‹¬_í‚¤ì›Œë“œ": ["ì‘ê¸‰ìƒí™©", "ë¹ ë¥¸ëŒ€ì‘", "ìˆ˜ìˆ "], "ì˜ë£Œ_ë§¥ë½": ["ì˜ë£Œ_ì„œë¹„ìŠ¤", "ì „ë¬¸ì„±"], "ì‹ ë¢°ë„_ì ìˆ˜": 9}}

- ì›ë³¸ í…ìŠ¤íŠ¸: "ì—†ìŠµë‹ˆë‹¤"
- JSON ì¶œë ¥:
{{"ì •ì œëœ_í…ìŠ¤íŠ¸": "", "ë¹„ì‹ë³„_ì²˜ë¦¬": false, "ê°ì •_ë¶„ë¥˜": "", "ê°ì •_ê°•ë„_ì ìˆ˜": "", "í•µì‹¬_í‚¤ì›Œë“œ": [], "ì˜ë£Œ_ë§¥ë½": [], "ì‹ ë¢°ë„_ì ìˆ˜": ""}}

- ì›ë³¸ í…ìŠ¤íŠ¸: "íŠ¹ë³„íˆ ì—†ìŒ"
- JSON ì¶œë ¥:
{{"ì •ì œëœ_í…ìŠ¤íŠ¸": "", "ë¹„ì‹ë³„_ì²˜ë¦¬": false, "ê°ì •_ë¶„ë¥˜": "", "ê°ì •_ê°•ë„_ì ìˆ˜": "", "í•µì‹¬_í‚¤ì›Œë“œ": [], "ì˜ë£Œ_ë§¥ë½": [], "ì‹ ë¢°ë„_ì ìˆ˜": ""}}

- ì›ë³¸ í…ìŠ¤íŠ¸: "---"
- JSON ì¶œë ¥:
{{"ì •ì œëœ_í…ìŠ¤íŠ¸": "", "ë¹„ì‹ë³„_ì²˜ë¦¬": false, "ê°ì •_ë¶„ë¥˜": "", "ê°ì •_ê°•ë„_ì ìˆ˜": "", "í•µì‹¬_í‚¤ì›Œë“œ": [], "ì˜ë£Œ_ë§¥ë½": [], "ì‹ ë¢°ë„_ì ìˆ˜": ""}}

ì›ë³¸ í…ìŠ¤íŠ¸: "{original_text}"
"""

class CheckpointManager:
    """
    ì²´í¬í¬ì¸íŠ¸ ì €ì¥/ë¡œë“œë¥¼ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤
    """
    
    def __init__(self, checkpoint_dir: str = "checkpoints"):
        self.checkpoint_dir = Path(checkpoint_dir)
        self.checkpoint_dir.mkdir(exist_ok=True)
    
    def save_checkpoint(self, session_id: str, data: dict):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        checkpoint_file = self.checkpoint_dir / f"checkpoint_{session_id}.pkl"
        with open(checkpoint_file, 'wb') as f:
            pickle.dump(data, f)
        return checkpoint_file
    
    def load_checkpoint(self, session_id: str) -> dict:
        """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
        checkpoint_file = self.checkpoint_dir / f"checkpoint_{session_id}.pkl"
        if checkpoint_file.exists():
            with open(checkpoint_file, 'rb') as f:
                return pickle.load(f)
        return None
    
    def cleanup_old_checkpoints(self, keep_days: int = 7):
        """ì˜¤ë˜ëœ ì²´í¬í¬ì¸íŠ¸ ì •ë¦¬"""
        cutoff_time = time.time() - (keep_days * 24 * 3600)
        for checkpoint_file in self.checkpoint_dir.glob("checkpoint_*.pkl"):
            if checkpoint_file.stat().st_mtime < cutoff_time:
                checkpoint_file.unlink()

class ProgressMonitor:
    """
    í–¥ìƒëœ ì§„í–‰ë¥  ëª¨ë‹ˆí„°ë§ í´ë˜ìŠ¤
    """
    
    def __init__(self, total_items: int, session_id: str = None):
        self.total_items = total_items
        self.processed_items = 0
        self.start_time = time.time()
        self.session_id = session_id or f"session_{int(time.time())}"
        self.error_count = 0
        self.retry_count = 0
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        self.processing_times = []
        self.last_checkpoint_time = time.time()
    
    def update(self, increment: int = 1, processing_time: float = None):
        """ì§„í–‰ë¥  ì—…ë°ì´íŠ¸"""
        self.processed_items += increment
        if processing_time:
            self.processing_times.append(processing_time)
    
    def add_error(self):
        """ì—ëŸ¬ ì¹´ìš´íŠ¸ ì¦ê°€"""
        self.error_count += 1
    
    def add_retry(self):
        """ì¬ì‹œë„ ì¹´ìš´íŠ¸ ì¦ê°€"""
        self.retry_count += 1
    
    def get_statistics(self) -> dict:
        """í˜„ì¬ í†µê³„ ë°˜í™˜"""
        elapsed_time = time.time() - self.start_time
        
        if self.processed_items > 0:
            avg_time_per_item = elapsed_time / self.processed_items
            remaining_items = self.total_items - self.processed_items
            eta = remaining_items * avg_time_per_item
            items_per_second = self.processed_items / elapsed_time
        else:
            avg_time_per_item = 0
            eta = 0
            items_per_second = 0
        
        return {
            'processed': self.processed_items,
            'total': self.total_items,
            'progress_percent': (self.processed_items / self.total_items) * 100,
            'elapsed_time': elapsed_time,
            'eta_seconds': eta,
            'items_per_second': items_per_second,
            'error_count': self.error_count,
            'retry_count': self.retry_count,
            'avg_processing_time': sum(self.processing_times) / len(self.processing_times) if self.processing_times else 0
        }
    
    def should_checkpoint(self, interval: int = 100) -> bool:
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì‹œì  í™•ì¸"""
        return self.processed_items % interval == 0 and self.processed_items > 0

class BackgroundWorker:
    """
    ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ê´€ë¦¬ í´ë˜ìŠ¤
    """
    
    def __init__(self, timeout_seconds: int = 60):
        self.timeout_seconds = timeout_seconds
        self.is_running = False
        self.heartbeat_interval = 30  # 30ì´ˆë§ˆë‹¤ í•˜íŠ¸ë¹„íŠ¸
    
    def run_with_timeout(self, func, *args, **kwargs):
        """íƒ€ì„ì•„ì›ƒê³¼ í•¨ê»˜ í•¨ìˆ˜ ì‹¤í–‰"""
        result_queue = Queue()
        error_queue = Queue()
        
        def worker():
            try:
                result = func(*args, **kwargs)
                result_queue.put(result)
            except Exception as e:
                error_queue.put(e)
        
        process = Process(target=worker)
        process.start()
        
        try:
            process.join(timeout=self.timeout_seconds)
            
            if process.is_alive():
                process.terminate()
                process.join()
                raise TimeoutError(f"ì‘ì—…ì´ {self.timeout_seconds}ì´ˆ ë‚´ì— ì™„ë£Œë˜ì§€ ì•ŠìŒ")
            
            if not error_queue.empty():
                raise error_queue.get()
            
            if not result_queue.empty():
                return result_queue.get()
            else:
                raise RuntimeError("ì‘ì—… ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŒ")
        
        except TimeoutError:
            raise
        except Exception as e:
            raise e

class ReviewAnalyzer:
    """
    í…ìŠ¤íŠ¸ ë¦¬ë·°ë¥¼ AIë¡œ ë¶„ì„í•˜ëŠ” í´ë˜ìŠ¤
    Googleì˜ Gemini APIë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ì˜ ê°ì •, ê°œì„ ëœ í‘œí˜„, ë¶„ë¥˜ ë¼ë²¨ ë“±ì„ ë¶„ì„í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, api_key_file: str = "Gemini API.json", enable_background: bool = True):
        """
        ë¶„ì„ê¸°ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

        Args:
            api_key_file: Gemini API í‚¤ JSON íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: "Gemini API.json")
            enable_background: ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬ í™œì„±í™” ì—¬ë¶€
        """
        # Gemini API í‚¤ ë¡œë“œ
        with open(api_key_file, 'r') as f:
            api_config = json.load(f)
            api_key = api_config.get('apikey') or api_config.get('api_key')
            if not api_key:
                raise KeyError("JSON íŒŒì¼ì— 'apikey' ë˜ëŠ” 'api_key'ê°€ ì—†ìŠµë‹ˆë‹¤.")

        # Google Gemini API ì´ˆê¸°í™”
        genai.configure(api_key=api_key)

        # ì‚¬ìš©í•  AI ëª¨ë¸ ì„¤ì • (Gemini 2.5 Flash ëª¨ë¸)
        self.model = genai.GenerativeModel("gemini-2.5-flash")
        
        # ìƒˆë¡œìš´ ê¸°ëŠ¥ë“¤ ì´ˆê¸°í™”
        self.checkpoint_manager = CheckpointManager()
        self.background_worker = BackgroundWorker() if enable_background else None
        
        # ë¡œê¹… ì„¤ì •
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('analysis.log'),
                logging.StreamHandler()
            ]
        )
    
    
    def analyze_review(self, original_text: str, use_background: bool = False) -> dict:
        """
        í…ìŠ¤íŠ¸ë¥¼ AIë¡œ ë¶„ì„í•˜ì—¬ ê°ì •, ê°œì„ ëœ í…ìŠ¤íŠ¸ ë“±ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        Args:
            original_text: ë¶„ì„í•  ì›ë³¸ í…ìŠ¤íŠ¸
            use_background: ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬ ì‚¬ìš© ì—¬ë¶€
            
        Returns:
            dict: ë¶„ì„ ê²°ê³¼ (ê°ì •, ê°œì„ ëœ í…ìŠ¤íŠ¸, ë¼ë²¨ ë“±)
        """
        start_time = time.time()
        
        # ë¹ˆ í…ìŠ¤íŠ¸ ì²˜ë¦¬
        if not original_text or original_text.strip() == "":
            return {
                "original_text": original_text,
                "ì •ì œëœ_í…ìŠ¤íŠ¸": "",
                "ë¹„ì‹ë³„_ì²˜ë¦¬": False,
                "ê°ì •_ë¶„ë¥˜": "",
                "ê°ì •_ê°•ë„_ì ìˆ˜": "",
                "í•µì‹¬_í‚¤ì›Œë“œ": [],
                "ì˜ë£Œ_ë§¥ë½": [],
                "ì‹ ë¢°ë„_ì ìˆ˜": ""
            }
        
        try:
            # ê°„ì†Œí™”ëœ ì§ì ‘ ì²˜ë¦¬ (ë°±ê·¸ë¼ìš´ë“œ ì›Œì»¤ ì œê±°)
            result = self._analyze_text_internal(original_text)
            
            # ì²˜ë¦¬ ì‹œê°„ ë¡œê¹…
            processing_time = time.time() - start_time
            logging.info(f"í…ìŠ¤íŠ¸ ë¶„ì„ ì™„ë£Œ: {processing_time:.2f}ì´ˆ")
            
            return result
            
        except Exception as e:
            logging.error(f"í…ìŠ¤íŠ¸ ë¶„ì„ ì˜¤ë¥˜: {e} - {original_text[:50]}...")
            return self._get_fallback_result(original_text)
    
    def _analyze_text_internal(self, original_text: str) -> dict:
        """
        ë‚´ë¶€ í…ìŠ¤íŠ¸ ë¶„ì„ ë¡œì§ (ë°±ê·¸ë¼ìš´ë“œ í”„ë¡œì„¸ìŠ¤ì—ì„œ ì‹¤í–‰)
        """
        # AI ë¶„ì„ í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = PROMPT_TEMPLATE.format(original_text=original_text)
        
        # API í•œê³„ ëŒ€ì‘ì„ ìœ„í•œ ì¬ì‹œë„ ë¡œì§ (ê· í˜• ì¡°ì •)
        max_retries = 3  # ì¬ì‹œë„ íšŸìˆ˜ ì ì •
        base_wait_time = 1.0  # ê¸°ë³¸ ëŒ€ê¸° ì‹œê°„ ì ì •
        
        for attempt in range(max_retries):
            try:
                # AI ëª¨ë¸ì— ë¶„ì„ ìš”ì²­ (íƒ€ì„ì•„ì›ƒ ì„¤ì •)
                response = self.model.generate_content(prompt)
                response_text = response.text.strip()
                
                # ì„±ê³µ ì‹œ ê²°ê³¼ íŒŒì‹± ë° ë°˜í™˜
                return self._parse_ai_response(response_text, original_text)
                
            except Exception as e:
                if "429" in str(e) or "Resource exhausted" in str(e) or "quota" in str(e).lower():
                    if attempt < max_retries - 1:
                        # ì§€ìˆ˜ ë°±ì˜¤í”„ with ì§€í„°: (2^attempt) * base_wait_time + random jitter
                        wait_time = (2 ** attempt) * base_wait_time + (attempt * 0.5)
                        logging.warning(f"API í•œê³„ ë„ë‹¬, {wait_time:.1f}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„... ({attempt + 1}/{max_retries})")
                        time.sleep(wait_time)
                        continue
                    else:
                        # ìµœëŒ€ ì¬ì‹œë„ í›„ì—ë„ ì‹¤íŒ¨í•˜ë©´ ê¸°ë³¸ê°’ ë°˜í™˜
                        logging.error(f"API í•œê³„ë¡œ ì¸í•œ ìµœì¢… ì‹¤íŒ¨: {original_text[:50]}...")
                        return self._get_fallback_result(original_text)
                else:
                    # ë‹¤ë¥¸ ì¢…ë¥˜ì˜ ì˜¤ë¥˜ëŠ” ì¬ì‹œë„
                    if attempt < max_retries - 1:
                        wait_time = base_wait_time * (attempt + 1)
                        logging.warning(f"ì¼ë°˜ ì˜¤ë¥˜ ì¬ì‹œë„: {e} - {wait_time}ì´ˆ ëŒ€ê¸°")
                        time.sleep(wait_time)
                        continue
                    else:
                        raise e
        
        return self._get_fallback_result(original_text)
    
    def _parse_ai_response(self, response_text: str, original_text: str) -> dict:
        """AI ì‘ë‹µ íŒŒì‹±"""
        try:
            json_start = response_text.find('{')
            json_end = response_text.rfind('}') + 1
            if json_start != -1 and json_end > json_start:
                json_text = response_text[json_start:json_end]
                result = json.loads(json_text)
                result["original_text"] = original_text
                return result
            else:
                raise json.JSONDecodeError("No JSON found", response_text, 0)
        except json.JSONDecodeError:
            logging.warning(f"AI ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {original_text[:50]}...")
            return self._get_fallback_result(original_text)
    
    def _get_fallback_result(self, original_text: str) -> dict:
        """ê¸°ë³¸ ê²°ê³¼ ë°˜í™˜"""
        return {
            "original_text": original_text,
            "ì •ì œëœ_í…ìŠ¤íŠ¸": original_text,
            "ë¹„ì‹ë³„_ì²˜ë¦¬": False,
            "ê°ì •_ë¶„ë¥˜": "ì¤‘ë¦½",
            "ê°ì •_ê°•ë„_ì ìˆ˜": 5,
            "í•µì‹¬_í‚¤ì›Œë“œ": [],
            "ì˜ë£Œ_ë§¥ë½": [],
            "ì‹ ë¢°ë„_ì ìˆ˜": 1
        }
    
    def _analyze_batch_with_monitoring(self, texts: list, batch_size: int, progress_monitor: ProgressMonitor) -> list:
        """
        ì§„í–‰ë¥  ëª¨ë‹ˆí„°ë§ê³¼ í•¨ê»˜ ë°°ì¹˜ ì²˜ë¦¬
        """
        results = []
        
        # ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ThreadPoolExecutor ì‚¬ìš© (ì ë‹¹í•œ ë³‘ë ¬ ì²˜ë¦¬)
        with ThreadPoolExecutor(max_workers=min(20, len(texts))) as executor:
            # ê° í…ìŠ¤íŠ¸ì— ëŒ€í•´ ë¹„ë™ê¸° ì‘ì—… ì œì¶œ
            future_to_index = {}
            for idx, text in enumerate(texts):
                future = executor.submit(self.analyze_review, text, False)  # ì§ì ‘ ì²˜ë¦¬
                future_to_index[future] = (idx, text)
            
            # ë°°ì¹˜ í¬ê¸°ë§Œí¼ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
            batch_results = [None] * len(texts)
            
            # ê²°ê³¼ ìˆ˜ì§‘ (ì™„ë£Œ ìˆœì„œì™€ ê´€ê³„ì—†ì´ ì›ë˜ ìˆœì„œ ìœ ì§€)
            for future in as_completed(future_to_index):
                idx, original_text = future_to_index[future]
                start_time = time.time()
                try:
                    result = future.result(timeout=60)  # 1ë¶„ íƒ€ì„ì•„ì›ƒìœ¼ë¡œ ë‹¨ì¶•
                    batch_results[idx] = result
                    processing_time = time.time() - start_time
                    progress_monitor.update(1, processing_time)
                except Exception as e:
                    logging.error(f"ë°°ì¹˜ ì²˜ë¦¬ ì˜¤ë¥˜: {e} - {original_text[:50]}...")
                    progress_monitor.add_error()
                    # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
                    batch_results[idx] = self._get_fallback_result(original_text)
        
        return batch_results

    def analyze_batch(self, texts: list, batch_size: int = 10) -> list:
        """
        ì—¬ëŸ¬ í…ìŠ¤íŠ¸ë¥¼ ë°°ì¹˜ë¡œ ì²˜ë¦¬í•˜ì—¬ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.
        
        Args:
            texts: ë¶„ì„í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            batch_size: í•œ ë²ˆì— ì²˜ë¦¬í•  í…ìŠ¤íŠ¸ ìˆ˜
            
        Returns:
            list: ë¶„ì„ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        results = []
        
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬ (ìˆœì„œ ë³´ì¥)
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i+batch_size]
            batch_results = []
            
            # ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ThreadPoolExecutor ì‚¬ìš©
            with ThreadPoolExecutor(max_workers=min(10, len(batch))) as executor:
                # ê° í…ìŠ¤íŠ¸ì— ëŒ€í•´ ë¹„ë™ê¸° ì‘ì—… ì œì¶œ (ì¸ë±ìŠ¤ì™€ í•¨ê»˜)
                future_to_index = {}
                for idx, text in enumerate(batch):
                    future = executor.submit(self.analyze_review, text)
                    future_to_index[future] = (idx, text)
                
                # ë°°ì¹˜ í¬ê¸°ë§Œí¼ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
                batch_results = [None] * len(batch)
                
                # ê²°ê³¼ ìˆ˜ì§‘ (ì™„ë£Œ ìˆœì„œì™€ ê´€ê³„ì—†ì´ ì›ë˜ ìˆœì„œ ìœ ì§€)
                for future in as_completed(future_to_index):
                    idx, original_text = future_to_index[future]
                    try:
                        result = future.result()
                        batch_results[idx] = result
                    except Exception as e:
                        print(f"ë°°ì¹˜ ì²˜ë¦¬ ì˜¤ë¥˜: {e} - {original_text[:50]}...")
                        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
                        batch_results[idx] = {
                            "original_text": original_text,
                            "ì •ì œëœ_í…ìŠ¤íŠ¸": original_text,
                            "ë¹„ì‹ë³„_ì²˜ë¦¬": False,
                            "ê°ì •_ë¶„ë¥˜": "ì¤‘ë¦½",
                            "ê°ì •_ê°•ë„_ì ìˆ˜": 5,
                            "í•µì‹¬_í‚¤ì›Œë“œ": [],
                            "ì˜ë£Œ_ë§¥ë½": [],
                            "ì‹ ë¢°ë„_ì ìˆ˜": 1
                        }
            
            # ë°°ì¹˜ ê²°ê³¼ë¥¼ ì „ì²´ ê²°ê³¼ì— ìˆœì„œëŒ€ë¡œ ì¶”ê°€
            results.extend(batch_results)
        
        return results
    
    def analyze_refined_batch(self, texts_and_flags: list, batch_size: int) -> list:
        """
        ì—¬ëŸ¬ ì •ì œëœ í…ìŠ¤íŠ¸ë¥¼ ë°°ì¹˜ë¡œ ì²˜ë¦¬í•˜ì—¬ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.
        
        Args:
            texts_and_flags: (ì •ì œëœ_í…ìŠ¤íŠ¸, ë¹„ì‹ë³„_ì—¬ë¶€) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
            batch_size: ë°°ì¹˜ í¬ê¸°
            
        Returns:
            list: ë¶„ì„ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        results = []
        
        # ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ThreadPoolExecutor ì‚¬ìš© (ì•ˆì •ì  ì²˜ë¦¬)
        with ThreadPoolExecutor(max_workers=min(3, len(texts_and_flags))) as executor:
            # ê° í…ìŠ¤íŠ¸ì— ëŒ€í•´ ë¹„ë™ê¸° ì‘ì—… ì œì¶œ (ì¸ë±ìŠ¤ì™€ í•¨ê»˜)
            future_to_index = {}
            for idx, (refined_text, is_anonymized) in enumerate(texts_and_flags):
                future = executor.submit(self.analyze_refined_text, refined_text, is_anonymized)
                future_to_index[future] = (idx, refined_text, is_anonymized)
            
            # ë°°ì¹˜ í¬ê¸°ë§Œí¼ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
            batch_results = [None] * len(texts_and_flags)
            
            # ê²°ê³¼ ìˆ˜ì§‘ (ì™„ë£Œ ìˆœì„œì™€ ê´€ê³„ì—†ì´ ì›ë˜ ìˆœì„œ ìœ ì§€)
            for future in as_completed(future_to_index):
                idx, refined_text, is_anonymized = future_to_index[future]
                try:
                    result = future.result()
                    batch_results[idx] = result
                except Exception as e:
                    print(f"ë°°ì¹˜ ì²˜ë¦¬ ì˜¤ë¥˜: {e} - {refined_text[:50]}...")
                    # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
                    batch_results[idx] = {
                        "original_text": refined_text,
                        "ì •ì œëœ_í…ìŠ¤íŠ¸": refined_text,
                        "ë¹„ì‹ë³„_ì²˜ë¦¬": is_anonymized,
                        "ê°ì •_ë¶„ë¥˜": "ì¤‘ë¦½",
                        "ê°ì •_ê°•ë„_ì ìˆ˜": 5,
                        "í•µì‹¬_í‚¤ì›Œë“œ": [],
                        "ì˜ë£Œ_ë§¥ë½": [],
                        "ì‹ ë¢°ë„_ì ìˆ˜": 1
                    }
        
        return batch_results
    
    def retry_low_quality_analysis_refined(self, texts_and_flags: list, results: list, quality_results: list, max_retries: int = 2) -> tuple:
        """
        ì •ì œëœ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ë‚®ì€ í’ˆì§ˆì˜ ë¶„ì„ ê²°ê³¼ë¥¼ ì¬ë¶„ì„í•©ë‹ˆë‹¤.
        
        Args:
            texts_and_flags: (ì •ì œëœ_í…ìŠ¤íŠ¸, ë¹„ì‹ë³„_ì—¬ë¶€) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
            results: ë¶„ì„ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            quality_results: í’ˆì§ˆ ê²€ì¦ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
            
        Returns:
            tuple: (ê°œì„ ëœ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸, ê°œì„ ëœ í’ˆì§ˆ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸)
        """
        improved_results = results.copy()
        improved_quality = quality_results.copy()
        
        # ì¬ê²€í† ê°€ í•„ìš”í•œ í•­ëª©ë“¤ ì°¾ê¸°
        retry_indices = [i for i, q in enumerate(quality_results) if q['needs_review']]
        
        if not retry_indices:
            print("ì¬ê²€í† ê°€ í•„ìš”í•œ í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
            return improved_results, improved_quality
        
        print(f"\n{len(retry_indices)}ê°œ í•­ëª©ì„ ì¬ë¶„ì„í•©ë‹ˆë‹¤...")
        
        retry_count = 0
        total_improved = 0
        
        with tqdm(total=len(retry_indices), desc="ì¬ë¶„ì„ ì§„í–‰ë¥ ", unit="ê±´") as pbar:
            for idx in retry_indices:
                if retry_count >= max_retries:
                    break
                
                refined_text, is_anonymized = texts_and_flags[idx]
                original_quality = quality_results[idx]['quality_score']
                
                # ì¬ë¶„ì„ ìˆ˜í–‰
                new_result = self.analyze_refined_text(refined_text, is_anonymized)
                new_quality = self.validate_analysis_quality(new_result, refined_text)
                
                # ê°œì„ ëœ ê²½ìš°ì—ë§Œ ê²°ê³¼ ë°˜ì˜
                if new_quality['quality_score'] > original_quality:
                    # original_text ì œê±°
                    if "original_text" in new_result:
                        del new_result["original_text"]
                    
                    improved_results[idx] = new_result
                    improved_quality[idx] = new_quality
                    total_improved += 1
                
                retry_count += 1
                pbar.update(1)
                
                # API ì œí•œì„ ìœ„í•œ ì§§ì€ ëŒ€ê¸°
                time.sleep(0.5)
        
        print(f"ì¬ë¶„ì„ ì™„ë£Œ: {total_improved}ê°œ í•­ëª©ì´ ê°œì„ ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        return improved_results, improved_quality
    
    def validate_analysis_quality(self, result: dict, original_text: str) -> dict:
        """
        AI ë¶„ì„ ê²°ê³¼ì˜ í’ˆì§ˆì„ ê²€ì¦í•˜ê³  ì‹ ë¢°ë„ë¥¼ í‰ê°€í•©ë‹ˆë‹¤.
        
        Args:
            result: AI ë¶„ì„ ê²°ê³¼
            original_text: ì›ë³¸ í…ìŠ¤íŠ¸
            
        Returns:
            dict: í’ˆì§ˆ ê²€ì¦ ê²°ê³¼
        """
        quality_issues = []
        confidence_score = result.get('ì‹ ë¢°ë„_ì ìˆ˜', 5)
        
        # ì‹ ë¢°ë„ ì ìˆ˜ê°€ ë¬¸ìì—´ì¸ ê²½ìš° (ë¹ˆ ë¬¸ìì—´ ë“±) ê¸°ë³¸ê°’ìœ¼ë¡œ ì²˜ë¦¬
        if isinstance(confidence_score, str):
            if confidence_score == "":
                overall_confidence = 5  # ë¹ˆ ë¬¸ìì—´ì¸ ê²½ìš° ê¸°ë³¸ê°’
            else:
                try:
                    overall_confidence = float(confidence_score)
                except (ValueError, TypeError):
                    overall_confidence = 5
        else:
            overall_confidence = confidence_score
        
        # 1. ê¸°ë³¸ í•„ë“œ ì™„ì„±ë„ ê²€ì‚¬
        required_fields = ['ì •ì œëœ_í…ìŠ¤íŠ¸', 'ê°ì •_ë¶„ë¥˜', 'ê°ì •_ê°•ë„_ì ìˆ˜', 'í•µì‹¬_í‚¤ì›Œë“œ']
        missing_fields = [field for field in required_fields if field not in result or not result[field]]
        
        if missing_fields:
            # í•„ìˆ˜ í•„ë“œ ëˆ„ë½ ì •ë³´ëŠ” ì¶”ê°€í•˜ì§€ ì•ŠìŒ (ê³µë€ ì²˜ë¦¬)
            overall_confidence -= 3
        
        # 2. ê°ì • ê°•ë„ì™€ ê°ì • ë¶„ë¥˜ ì¼ì¹˜ì„± ê²€ì‚¬
        sentiment = result.get('ê°ì •_ë¶„ë¥˜', '')
        intensity_raw = result.get('ê°ì •_ê°•ë„_ì ìˆ˜', 5)
        
        # ê°ì • ê°•ë„ê°€ ë¬¸ìì—´ì¸ ê²½ìš° ì²˜ë¦¬
        if isinstance(intensity_raw, str):
            if intensity_raw == "":
                intensity = 5  # ë¹ˆ ë¬¸ìì—´ì¸ ê²½ìš° ê¸°ë³¸ê°’
            else:
                try:
                    intensity = float(intensity_raw)
                except (ValueError, TypeError):
                    intensity = 5
        else:
            intensity = intensity_raw
        
        if sentiment == 'ê¸ì •' and intensity < 6:
            quality_issues.append("ê¸ì • ê°ì •ì¸ë° ê°•ë„ê°€ ë‚®ìŒ")
            overall_confidence -= 2
        elif sentiment == 'ë¶€ì •' and intensity > 5:
            quality_issues.append("ë¶€ì • ê°ì •ì¸ë° ê°•ë„ê°€ ë†’ìŒ")
            overall_confidence -= 2
        
        # 3. í…ìŠ¤íŠ¸ ê¸¸ì´ ë¹„êµ
        original_len = len(str(original_text).strip()) if original_text and pd.notna(original_text) else 0
        refined_len = len(str(result.get('ì •ì œëœ_í…ìŠ¤íŠ¸', '')).strip())
        
        if original_len > 10 and refined_len < original_len * 0.3:
            quality_issues.append("ì •ì œëœ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìŒ")
            overall_confidence -= 2
        elif refined_len > original_len * 2:
            quality_issues.append("ì •ì œëœ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¹€")
            overall_confidence -= 1
        
        # 4. í‚¤ì›Œë“œ ì¶”ì¶œ í’ˆì§ˆ ê²€ì‚¬
        key_terms = result.get('í•µì‹¬_í‚¤ì›Œë“œ', [])
        if original_len > 20 and len(key_terms) == 0:
            quality_issues.append("í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨")
            overall_confidence -= 2
        
        # 5. ë¹„ì‹ë³„ ì²˜ë¦¬ ì¼ì¹˜ì„± ê²€ì‚¬
        is_anonymized = result.get('ë¹„ì‹ë³„_ì²˜ë¦¬', False)
        refined_text = result.get('ì •ì œëœ_í…ìŠ¤íŠ¸', '')
        
        # ì‹¤ëª… íŒ¨í„´ ê²€ì‚¬ (ê°„ë‹¨í•œ í•œê¸€ ì´ë¦„ íŒ¨í„´)
        name_pattern = r'[\uac00-\ud7a3]{2,3}\s*ì„ ìƒë‹˜|[\uac00-\ud7a3]{2,3}\s*ê³¼ì¥|[\uac00-\ud7a3]{2,3}\s*íŒ€ì¥|[\uac00-\ud7a3]{2,3}\s*ëŒ€ë¦¬'
        
        if is_anonymized and re.search(name_pattern, refined_text):
            quality_issues.append("ë¹„ì‹ë³„ ì²˜ë¦¬ ë¶ˆì™„ì „ - ì‹¤ëª… ì”ì¡´")
            overall_confidence -= 3
        
        # ìµœì¢… ì‹ ë¢°ë„ ì¡°ì •
        overall_confidence = max(1, min(10, overall_confidence))
        
        return {
            'quality_score': overall_confidence,
            'issues': quality_issues,
            'needs_review': overall_confidence < 6 or len(quality_issues) > 2,
            'is_reliable': overall_confidence >= 7 and len(quality_issues) <= 1
        }
    
    def retry_low_quality_analysis(self, texts: list, results: list, quality_results: list, max_retries: int = 2) -> tuple:
        """
        ë‚®ì€ í’ˆì§ˆì˜ ë¶„ì„ ê²°ê³¼ë¥¼ ì¬ë¶„ì„í•©ë‹ˆë‹¤.
        
        Args:
            texts: ì›ë³¸ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            results: ë¶„ì„ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            quality_results: í’ˆì§ˆ ê²€ì¦ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
            
        Returns:
            tuple: (ê°œì„ ëœ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸, ê°œì„ ëœ í’ˆì§ˆ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸)
        """
        improved_results = results.copy()
        improved_quality = quality_results.copy()
        
        # ì¬ê²€í† ê°€ í•„ìš”í•œ í•­ëª©ë“¤ ì°¾ê¸°
        retry_indices = [i for i, q in enumerate(quality_results) if q['needs_review']]
        
        if not retry_indices:
            print("ì¬ê²€í† ê°€ í•„ìš”í•œ í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
            return improved_results, improved_quality
        
        print(f"\n{len(retry_indices)}ê°œ í•­ëª©ì„ ì¬ë¶„ì„í•©ë‹ˆë‹¤...")
        
        retry_count = 0
        total_improved = 0
        
        with tqdm(total=len(retry_indices), desc="ì¬ë¶„ì„ ì§„í–‰ë¥ ", unit="ê±´") as pbar:
            for idx in retry_indices:
                if retry_count >= max_retries:
                    break
                
                original_text = texts[idx]
                original_quality = quality_results[idx]['quality_score']
                
                # ì¬ë¶„ì„ ìˆ˜í–‰
                new_result = self.analyze_review(original_text)
                new_quality = self.validate_analysis_quality(new_result, original_text)
                
                # ê°œì„ ëœ ê²½ìš°ì—ë§Œ ê²°ê³¼ ë°˜ì˜
                if new_quality['quality_score'] > original_quality:
                    # original_text ì œê±°
                    if "original_text" in new_result:
                        del new_result["original_text"]
                    
                    improved_results[idx] = new_result
                    improved_quality[idx] = new_quality
                    total_improved += 1
                
                retry_count += 1
                pbar.update(1)
                
                # API ì œí•œì„ ìœ„í•œ ì§§ì€ ëŒ€ê¸°
                time.sleep(0.5)
        
        print(f"ì¬ë¶„ì„ ì™„ë£Œ: {total_improved}ê°œ í•­ëª©ì´ ê°œì„ ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        return improved_results, improved_quality
    
    def process_xlsx_with_column(self, input_file: str, column_name: str, output_file: str = None, 
                                max_rows: int = None, use_batch: bool = True, batch_size: int = 10,
                                enable_quality_retry: bool = True, checkpoint_interval: int = 100,
                                resume_from_checkpoint: bool = True):
        """
        ì—‘ì…€ íŒŒì¼ì˜ íŠ¹ì • ì»¬ëŸ¼ì„ ë¶„ì„í•˜ì—¬ ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
        
        Args:
            input_file: ë¶„ì„í•  ì—‘ì…€ íŒŒì¼ ê²½ë¡œ
            column_name: ë¶„ì„í•  ì»¬ëŸ¼ëª…
            output_file: ê²°ê³¼ë¥¼ ì €ì¥í•  íŒŒì¼ ê²½ë¡œ (ìë™ ìƒì„± ê°€ëŠ¥)
            max_rows: í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì²˜ë¦¬í•  ìµœëŒ€ í–‰ ìˆ˜
            use_batch: ë°°ì¹˜ ì²˜ë¦¬ ì‚¬ìš© ì—¬ë¶€
            batch_size: ë°°ì¹˜ í¬ê¸°
            enable_quality_retry: í’ˆì§ˆ ì¬ê²€í†  í™œì„±í™” ì—¬ë¶€
            checkpoint_interval: ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ê°„ê²©
            resume_from_checkpoint: ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ê°œ ì—¬ë¶€
        """
        # ì„¸ì…˜ ID ìƒì„± (ì²´í¬í¬ì¸íŠ¸ ì‹ë³„ìš©)
        session_id = f"{Path(input_file).stem}_{column_name}_{int(time.time())}"
        
        # ì¶œë ¥ íŒŒì¼ëª… ìë™ ìƒì„±
        if output_file is None:
            output_file = "rawdata/" + str(Path(input_file).stem) + "_processed.xlsx"
        
        # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ê°œ í™•ì¸
        checkpoint_data = None
        if resume_from_checkpoint:
            checkpoint_data = self.checkpoint_manager.load_checkpoint(session_id)
            if checkpoint_data:
                logging.info(f"ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ê°œ: {checkpoint_data['processed_count']}/{checkpoint_data['total_count']} ì™„ë£Œ")
        
        # ì—‘ì…€ íŒŒì¼ì„ ì½ì–´ì„œ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
        df = pd.read_excel(input_file)
        
        # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì¼ë¶€ ë°ì´í„°ë§Œ ì²˜ë¦¬í•˜ëŠ” ê²½ìš°
        if max_rows:
            df = df.head(max_rows)
            print(f"í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ìƒìœ„ {max_rows}ê°œ í–‰ë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
        
        # ì§€ì •ëœ ì»¬ëŸ¼ì—ì„œ í…ìŠ¤íŠ¸ ë°ì´í„° ì¶”ì¶œ ë° ë¹ˆ ê°’ ì²´í¬
        texts = df[column_name].tolist()
        total_texts = len(texts)
        
        # ìœ íš¨í•œ í…ìŠ¤íŠ¸ë§Œ í•„í„°ë§ (ì‚¬ì „ í•„í„°ë§ ê°•í™”)
        valid_texts = []
        valid_indices = []
        
        # ë¬´ì˜ë¯¸í•œ í…ìŠ¤íŠ¸ íŒ¨í„´ ì •ì˜
        meaningless_patterns = [
            r'^ì—†[ìŠµë‹¤ìŒì–´ìš”]*$', r'^íŠ¹ë³„íˆ\s*ì—†[ìŒë‹¤ìŠµë‹ˆìš”]*$', r'^í•´ë‹¹\s*ì—†[ìŒë‹¤ìŠµë‹ˆìš”]*$',
            r'^[-\s]*$', r'^[.\s]*$', r'^[/\s]*$', r'^[ã…¡\s]*$', r'^ë¬´\s*$',
            r'^[\d\s]*$', r'^[a-zA-Z\s]*$', r'^[!@#$%^&*()_+\-=\[\]{};:"\\|,.<>?/\s]*$'
        ]
        
        for i, text in enumerate(texts):
            if pd.notna(text) and str(text).strip() != '' and str(text).strip() != 'nan':
                text_clean = str(text).strip()
                
                # ë¬´ì˜ë¯¸í•œ íŒ¨í„´ ê²€ì‚¬
                is_meaningless = any(re.match(pattern, text_clean, re.IGNORECASE) for pattern in meaningless_patterns)
                
                # ê¸¸ì´ê°€ ë„ˆë¬´ ì§§ì€ ê²½ìš°ë„ í•„í„°ë§ (3ê¸€ì ë¯¸ë§Œ)
                if len(text_clean) < 3:
                    is_meaningless = True
                
                if not is_meaningless:
                    valid_texts.append(text_clean)
                    valid_indices.append(i)
        
        print(f"ì´ {total_texts}ê°œ ì¤‘ {len(valid_texts)}ê°œì˜ ìœ íš¨í•œ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤...")
        
        # ì§„í–‰ë¥  ëª¨ë‹ˆí„° ì´ˆê¸°í™”
        progress_monitor = ProgressMonitor(len(valid_texts), session_id)
        
        # ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
        if checkpoint_data:
            # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ê°œ
            results = checkpoint_data['results']
            progress_monitor.processed_items = checkpoint_data['processed_count']
            print(f"ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ê°œ: {checkpoint_data['processed_count']}ê°œ ì•„ì´í…œ ì™„ë£Œ")
        else:
            # ìƒˆë¡œ ì‹œì‘
            results = [None] * total_texts
        
        if len(valid_texts) > 0 and not checkpoint_data:
            # ë°°ì¹˜ ì²˜ë¦¬ ê°•ì œ í™œì„±í™” (ì†ë„ ìµœì í™”)
            if use_batch and len(valid_texts) > 0:
                print(f"ë°°ì¹˜ ì²˜ë¦¬ ëª¨ë“œ (ë°°ì¹˜ í¬ê¸°: {batch_size})")
                valid_results = []
                
                # ì§„í–‰ë¥  í‘œì‹œë¥¼ ìœ„í•œ tqdm ì‚¬ìš©
                with tqdm(total=len(valid_texts), desc="ë¶„ì„ ì§„í–‰ë¥ ", unit="ê±´") as pbar:
                    for i in range(0, len(valid_texts), batch_size):
                        batch_texts = valid_texts[i:i+batch_size]
                        
                        # ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬ë¡œ ë°°ì¹˜ ë¶„ì„
                        batch_results = self._analyze_batch_with_monitoring(
                            batch_texts, len(batch_texts), progress_monitor
                        )
                        valid_results.extend(batch_results)
                        
                        # ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€
                        print(f"ğŸ” ë°°ì¹˜ ì™„ë£Œ: {len(valid_results)}ê°œ ì²˜ë¦¬ë¨")
                        
                        # ìœ íš¨í•œ ê²°ê³¼ë¥¼ ì›ë˜ ìœ„ì¹˜ì— ë°°ì¹˜ (ì²´í¬í¬ì¸íŠ¸ìš©)
                        temp_results = results.copy()
                        for j, result in enumerate(valid_results):
                            if j < len(valid_indices):
                                temp_results[valid_indices[j]] = result
                        
                        # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ í™•ì¸ (ë§¤ ë°°ì¹˜ë§ˆë‹¤ ê°•ì œ ì €ì¥)
                        print(f"ğŸ” ì²´í¬í¬ì¸íŠ¸ í™•ì¸: {len(valid_results)} % 10 = {len(valid_results) % 10}")
                        if len(valid_results) > 0 and len(valid_results) % 10 == 0:
                            checkpoint_data = {
                                'session_id': session_id,
                                'input_file': input_file,
                                'column_name': column_name,
                                'total_count': len(valid_texts),
                                'processed_count': len(valid_results),
                                'results': temp_results,
                                'valid_indices': valid_indices,
                                'timestamp': time.time()
                            }
                            self.checkpoint_manager.save_checkpoint(session_id, checkpoint_data)
                            print(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ë¨: {len(valid_results)}/{len(valid_texts)}")
                            logging.info(f"ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {len(valid_results)}/{len(valid_texts)}")
                            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ë¨
                        
                        pbar.update(len(batch_texts))
                        
                        # API ëŒ€ê¸° ì‹œê°„ ì œê±° (ì¬ì‹œë„ ë¡œì§ì—ì„œ ìë™ ì²˜ë¦¬)
            else:
                print("ìˆœì°¨ ì²˜ë¦¬ ëª¨ë“œ")
                valid_results = []
                
                # ê° í…ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì”© ë¶„ì„
                with tqdm(total=len(valid_texts), desc="ë¶„ì„ ì§„í–‰ë¥ ", unit="ê±´") as pbar:
                    for idx, text in enumerate(valid_texts):
                        start_time = time.time()
                        result = self.analyze_review(text, use_background=True)
                        processing_time = time.time() - start_time
                        
                        valid_results.append(result)
                        progress_monitor.update(1, processing_time)
                        
                        # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ í™•ì¸
                        if progress_monitor.should_checkpoint(checkpoint_interval):
                            # ì§€ê¸ˆê¹Œì§€ì˜ ê²°ê³¼ë¥¼ ì›ë˜ ìœ„ì¹˜ì— ë°°ì¹˜
                            temp_results = results.copy()
                            for i, res in enumerate(valid_results):
                                if i < len(valid_indices):
                                    temp_results[valid_indices[i]] = res
                            
                            checkpoint_data = {
                                'session_id': session_id,
                                'input_file': input_file,
                                'column_name': column_name,
                                'total_count': len(valid_texts),
                                'processed_count': len(valid_results),
                                'results': temp_results,
                                'valid_indices': valid_indices,
                                'timestamp': time.time()
                            }
                            self.checkpoint_manager.save_checkpoint(session_id, checkpoint_data)
                            logging.info(f"ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {len(valid_results)}/{len(valid_texts)}")
                        
                        pbar.update(1)
            
            # ìœ íš¨í•œ ê²°ê³¼ë¥¼ ì›ë˜ ìœ„ì¹˜ì— ë°°ì¹˜
            for i, result in enumerate(valid_results):
                if i < len(valid_indices):
                    original_index = valid_indices[i]
                    results[original_index] = result
        
        # ë¹ˆ ê°’ì— ëŒ€í•œ ê¸°ë³¸ ê²°ê³¼ ìƒì„±
        for i, result in enumerate(results):
            if result is None:
                results[i] = {
                    "ì •ì œëœ_í…ìŠ¤íŠ¸": "",
                    "ë¹„ì‹ë³„_ì²˜ë¦¬": False,
                    "ê°ì •_ë¶„ë¥˜": "",
                    "ê°ì •_ê°•ë„_ì ìˆ˜": "",
                    "í•µì‹¬_í‚¤ì›Œë“œ": [],
                    "ì˜ë£Œ_ë§¥ë½": [],
                    "ì‹ ë¢°ë„_ì ìˆ˜": ""
                }
        
        # í’ˆì§ˆ ê²€ì¦ì„ ìœ„í•œ ê¸°ë³¸ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
        quality_results = [None] * total_texts
        
        # original_text í•„ë“œ ì œê±° (ì¤‘ë³µ ë°©ì§€) ë° í•œêµ­ì–´ ì»¬ëŸ¼ëª… ë³€ê²½
        for result in results:
            if "original_text" in result:
                del result["original_text"]
            
            # ì˜ì–´ ì»¬ëŸ¼ëª…ì„ í•œêµ­ì–´ë¡œ ë³€ê²½ (í•„ìš”í•œ ì»¬ëŸ¼ë§Œ)
            if "refined_text" in result:
                result["ì •ì œëœ_í…ìŠ¤íŠ¸"] = result.pop("refined_text")
            if "is_anonymized" in result:
                result["ë¹„ì‹ë³„_ì²˜ë¦¬"] = result.pop("is_anonymized")
            if "sentiment" in result:
                result["ê°ì •_ë¶„ë¥˜"] = result.pop("sentiment")
            if "sentiment_intensity" in result:
                result["ê°ì •_ê°•ë„_ì ìˆ˜"] = result.pop("sentiment_intensity")
            if "key_terms" in result:
                result["í•µì‹¬_í‚¤ì›Œë“œ"] = result.pop("key_terms")
            if "medical_context" in result:
                result["ì˜ë£Œ_ë§¥ë½"] = result.pop("medical_context")
            if "confidence_score" in result:
                result["ì‹ ë¢°ë„_ì ìˆ˜"] = result.pop("confidence_score")
        
        # í’ˆì§ˆ ê²€ì¦
        if enable_quality_retry and len(valid_texts) > 0:
            print("\në¶„ì„ í’ˆì§ˆ ê²€ì¦ ì¤‘...")
            
            with tqdm(total=len(valid_texts), desc="í’ˆì§ˆ ê²€ì¦", unit="ê±´") as pbar:
                for i, valid_index in enumerate(valid_indices):
                    result = results[valid_index]
                    original_text = texts[valid_index]
                    quality = self.validate_analysis_quality(result, original_text)
                    quality_results[valid_index] = quality
                    pbar.update(1)
            
            # í’ˆì§ˆ ì¬ê²€í†  (ìœ íš¨í•œ í…ìŠ¤íŠ¸ë§Œ)
            if len(valid_texts) > 0:
                # ìœ íš¨í•œ í…ìŠ¤íŠ¸ì™€ ê²°ê³¼ë§Œ ì¶”ì¶œ
                valid_results_for_retry = [results[i] for i in valid_indices]
                valid_quality_for_retry = [quality_results[i] for i in valid_indices]
                
                improved_results, improved_quality = self.retry_low_quality_analysis(valid_texts, valid_results_for_retry, valid_quality_for_retry)
                
                # ê°œì„ ëœ ê²°ê³¼ë¥¼ ì›ë˜ ìœ„ì¹˜ì— ë°˜ì˜
                for i, improved_result in enumerate(improved_results):
                    original_index = valid_indices[i]
                    # original_text ì œê±°
                    if "original_text" in improved_result:
                        del improved_result["original_text"]
                    results[original_index] = improved_result
                    quality_results[original_index] = improved_quality[i]
            
            # í’ˆì§ˆ í†µê³„ ì¶œë ¥ (ìœ íš¨í•œ ë¶„ì„ ê²°ê³¼ë§Œ)
            valid_quality_results = [q for q in quality_results if q is not None]
            if len(valid_quality_results) > 0:
                reliable_count = sum(1 for q in valid_quality_results if q['is_reliable'])
                avg_quality = sum(q['quality_score'] for q in valid_quality_results) / len(valid_quality_results)
                
                print(f"\ní’ˆì§ˆ ê²€ì¦ ê²°ê³¼:")
                print(f"- ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë¶„ì„: {reliable_count}/{len(valid_quality_results)} ({reliable_count/len(valid_quality_results)*100:.1f}%)")
                print(f"- í‰ê·  í’ˆì§ˆ ì ìˆ˜: {avg_quality:.2f}/10")
        
        # ë¶„ì„ ê²°ê³¼ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
        result_df = pd.DataFrame(results)
        
        # ì»¬ëŸ¼ ë¶„ë¥˜ ë° ìµœì í™”
        final_df = self._optimize_columns_for_dashboard(df, result_df)
        
        # ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥ (ì—¬ëŸ¬ ì‹œíŠ¸ë¡œ êµ¬ë¶„)
        self._save_to_excel_with_sheets(final_df, output_file)
        
        print(f"\në¶„ì„ ì™„ë£Œ! ê²°ê³¼ê°€ '{output_file}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # ìµœì¢… í†µê³„ ë° ì²´í¬í¬ì¸íŠ¸ ì •ë¦¬
        final_stats = progress_monitor.get_statistics()
        logging.info(f"ë¶„ì„ ì™„ë£Œ - ì²˜ë¦¬ ì‹œê°„: {final_stats['elapsed_time']:.2f}ì´ˆ, í‰ê·  ì†ë„: {final_stats['items_per_second']:.2f}ê±´/ì´ˆ")
        
        # ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì‚­ì œ (ì™„ë£Œ í›„)
        try:
            checkpoint_file = self.checkpoint_manager.checkpoint_dir / f"checkpoint_{session_id}.pkl"
            if checkpoint_file.exists():
                checkpoint_file.unlink()
                logging.info("ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì‚­ì œ ì™„ë£Œ")
        except Exception as e:
            logging.warning(f"ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")
        
        print(f"ìµœì¢… í†µê³„: {final_stats['processed']}ê°œ ì²˜ë¦¬, ì—ëŸ¬ {final_stats['error_count']}ê°œ, ì¬ì‹œë„ {final_stats['retry_count']}ê°œ")
        
        # ê°ì • ë¶„ì„ í†µê³„ ì¶œë ¥
        self._print_analysis_statistics(result_df)
    
    def _print_analysis_statistics(self, result_df):
        """ë¶„ì„ ê²°ê³¼ í†µê³„ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤."""
        print("\n=== ë¶„ì„ ê²°ê³¼ í†µê³„ ===")
        
        # ê°ì • ë¶„ì„ í†µê³„ (í•œêµ­ì–´ ì»¬ëŸ¼ëª… ì‚¬ìš©)
        if 'ê°ì •_ë¶„ë¥˜' in result_df.columns:
            # ë¹ˆ ê°’ì„ ì œì™¸í•˜ê³  í†µê³„ ê³„ì‚°
            valid_sentiments = result_df['ê°ì •_ë¶„ë¥˜'][result_df['ê°ì •_ë¶„ë¥˜'] != '']
            if len(valid_sentiments) > 0:
                sentiment_counts = valid_sentiments.value_counts()
                print(f"\nê°ì • ë¶„ë¥˜ (ìœ íš¨í•œ {len(valid_sentiments)}ê°œ ë¶„ì„):")
                for sentiment, count in sentiment_counts.items():
                    percentage = (count / len(valid_sentiments)) * 100
                    print(f"  {sentiment}: {count}ê°œ ({percentage:.1f}%)")
        
        # ê°•ë„ í†µê³„
        if 'ê°ì •_ê°•ë„_ì ìˆ˜' in result_df.columns:
            # ë¹ˆ ê°’ê³¼ ë¬¸ìì—´ì„ ì œì™¸í•˜ê³  ìˆ«ìë§Œ ê³„ì‚°
            valid_intensities = pd.to_numeric(result_df['ê°ì •_ê°•ë„_ì ìˆ˜'], errors='coerce').dropna()
            if len(valid_intensities) > 0:
                avg_intensity = valid_intensities.mean()
                print(f"\ní‰ê·  ê°ì • ê°•ë„: {avg_intensity:.2f}/10")
        
        
        # í‚¤ì›Œë“œ í†µê³„
        if 'í•µì‹¬_í‚¤ì›Œë“œ' in result_df.columns:
            all_keywords = []
            for keywords in result_df['í•µì‹¬_í‚¤ì›Œë“œ']:
                if isinstance(keywords, list) and len(keywords) > 0:
                    all_keywords.extend(keywords)
            
            if all_keywords:
                keyword_counts = Counter(all_keywords)
                print("\nì£¼ìš” í‚¤ì›Œë“œ:")
                for keyword, count in keyword_counts.most_common(10):
                    print(f"  {keyword}: {count}íšŒ")
    
    def _optimize_columns_for_dashboard(self, original_df, result_df):
        """ì›ë³¸ ë°ì´í„° ì™„ì „ì„±ì„ ìœ ì§€í•˜ë©´ì„œ ë¶„ì„ ê²°ê³¼ë¥¼ í†µí•©í•©ë‹ˆë‹¤."""
        
        # ì§€ì •ëœ í…ìŠ¤íŠ¸ ë¶„ì„ ì»¬ëŸ¼ (7ê°œ)
        analysis_columns = [
            'ì •ì œëœ_í…ìŠ¤íŠ¸',
            'ë¹„ì‹ë³„_ì²˜ë¦¬',
            'ê°ì •_ë¶„ë¥˜',
            'ê°ì •_ê°•ë„_ì ìˆ˜',
            'í•µì‹¬_í‚¤ì›Œë“œ',
            'ì˜ë£Œ_ë§¥ë½',
            'ì‹ ë¢°ë„_ì ìˆ˜'
        ]
        
        # ì›ë³¸ ë°ì´í„° ì „ì²´ ìœ ì§€ + ë¶„ì„ ê²°ê³¼ ì¶”ê°€
        analysis_data = pd.concat([
            original_df,
            result_df[[col for col in analysis_columns if col in result_df.columns]]
        ], axis=1)
        
        return {
            'analysis': analysis_data
        }
    
    def _save_to_excel_with_sheets(self, data_dict, output_file):
        """ë¶„ì„ ì‹œíŠ¸ í•˜ë‚˜ë¡œ ì—‘ì…€ íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤."""
        with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
            # ë¶„ì„ ì‹œíŠ¸ (ì›ë³¸ ë°ì´í„° + ë¶„ì„ ê²°ê³¼)
            data_dict['analysis'].to_excel(
                writer, 
                sheet_name='ë¶„ì„ ì‹œíŠ¸', 
                index=False
            )
        
        print(f"\n=== ë°ì´í„° ì‹œíŠ¸ êµ¬ì„± ===")
        print(f"ë¶„ì„ ì‹œíŠ¸: {len(data_dict['analysis'].columns)}ê°œ ì»¬ëŸ¼")
        print(f"- ì›ë³¸ ë°ì´í„°: {len(data_dict['analysis'].columns) - 7}ê°œ ì»¬ëŸ¼")
        print(f"- ë¶„ì„ ê²°ê³¼: 7ê°œ ì»¬ëŸ¼")



# ê¸€ë¡œë²Œ ë³€ìˆ˜ ì„ ì–¸ (KeyboardInterruptì—ì„œ ì‚¬ìš©)
current_session_info = None

def main():
    """
    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ - í…ìŠ¤íŠ¸ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    try:
        # rawdata í´ë”ì—ì„œ ìµœì‹  data_processor ê²°ê³¼ íŒŒì¼ ì°¾ê¸°
        rawdata_path = Path("rawdata")
        pattern = "1. data_processor_ê²°ê³¼_*.xlsx"

        # íŒ¨í„´ì— ë§ëŠ” ëª¨ë“  íŒŒì¼ ì°¾ê¸°
        matching_files = list(rawdata_path.glob(pattern))

        if not matching_files:
            print(f"âŒ '{pattern}' íŒ¨í„´ì— ë§ëŠ” íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print(f"rawdata í´ë”ë¥¼ í™•ì¸í•˜ì„¸ìš”: {rawdata_path.absolute()}")
            sys.exit(1)

        # ê°€ì¥ ìµœê·¼ì— ìˆ˜ì •ëœ íŒŒì¼ ì„ íƒ (íŒŒì¼ëª…ì˜ íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ì¤€)
        input_file = max(matching_files, key=lambda x: x.stat().st_mtime)

        print(f"ğŸ“‚ ìµœì‹  íŒŒì¼ ìë™ ì„ íƒ: {input_file.name}")

        # ì¶œë ¥ íŒŒì¼ëª… ìƒì„± (íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ)
        input_filename = input_file.stem
        timestamp = input_filename.split('_')[-2] + '_' + input_filename.split('_')[-1]
        output_file = f"rawdata/2. text_processor_ê²°ê³¼_{timestamp}.xlsx"
        
        column_name = "í˜‘ì—… í›„ê¸°"
        max_rows = None  # ì „ì²´ ë°ì´í„° ì²˜ë¦¬ (ìˆ«ìë¡œ ì„¤ì •í•˜ë©´ í•´ë‹¹ í–‰ ìˆ˜ë§Œ ì²˜ë¦¬)
        api_key_file = "Gemini API.json"

        print(f"\nì„¤ì • í™•ì¸:")
        print(f"- ì…ë ¥ íŒŒì¼: {input_file.name}")
        print(f"- ë¶„ì„ ì»¬ëŸ¼: {column_name}")
        print(f"- ì¶œë ¥ íŒŒì¼: {output_file}")
        print(f"- ìµœëŒ€ ì²˜ë¦¬ í–‰: {max_rows or 'ì „ì²´'}")
        print(f"- API í‚¤ íŒŒì¼: {api_key_file}")
        print()

        # ë¶„ì„ê¸° ìƒì„± ë° ì‹¤í–‰ (ë°±ê·¸ë¼ìš´ë“œ ì›Œì»¤ ì œê±°ë¡œ íš¨ìœ¨ì„± í–¥ìƒ)
        analyzer = ReviewAnalyzer(api_key_file=api_key_file, enable_background=False)

        # ê¸€ë¡œë²Œ ë³€ìˆ˜ë¡œ ì„¸ì…˜ ì •ë³´ ì €ì¥ (KeyboardInterruptì—ì„œ ì‚¬ìš©)
        global current_session_info
        current_session_info = {
            'input_file': str(input_file),
            'column_name': column_name,
            'output_file': output_file
        }
        
        # ì›ë³¸ í…ìŠ¤íŠ¸ ë¶„ì„
        print(f"ì›ë³¸ í…ìŠ¤íŠ¸ ë¶„ì„ ëª¨ë“œ")
        analyzer.process_xlsx_with_column(
            str(input_file),
            column_name,
            output_file,
            max_rows=max_rows,
            use_batch=True,
            batch_size=10,  # í…ŒìŠ¤íŠ¸ìš©: ì‘ì€ ë°°ì¹˜ í¬ê¸°
            enable_quality_retry=False,  # ì†ë„ ìµœì í™”ë¥¼ ìœ„í•´ ë¹„í™œì„±í™”
            checkpoint_interval=5,  # í…ŒìŠ¤íŠ¸ìš©: 5ê°œë§ˆë‹¤ ì²´í¬í¬ì¸íŠ¸
            resume_from_checkpoint=True  # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ê°œ í™œì„±í™”
        )
        
    except KeyboardInterrupt:
        print("\n\nì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("ì¤‘ë‹¨ëœ ìƒíƒœê¹Œì§€ ê²°ê³¼ë¥¼ ì €ì¥í•˜ëŠ” ì¤‘...")
        
        try:
            # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ë¶€ë¶„ ê²°ê³¼ ë¡œë“œ (ê°€ì¥ ìµœê·¼ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì°¾ê¸°)
            checkpoint_manager = CheckpointManager()
            
            # í˜„ì¬ ì…ë ¥ íŒŒì¼ê³¼ ì»¬ëŸ¼ëª…ì— í•´ë‹¹í•˜ëŠ” ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ë“¤ ì°¾ê¸°
            file_stem = Path(current_session_info['input_file']).stem
            checkpoint_pattern = f"checkpoint_{file_stem}_{current_session_info['column_name']}_*.pkl"
            
            # ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²€ìƒ‰
            checkpoint_files = list(checkpoint_manager.checkpoint_dir.glob(checkpoint_pattern))
            
            if not checkpoint_files:
                print(f"âŒ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒ¨í„´: {checkpoint_pattern}")
                print(f"ì²´í¬í¬ì¸íŠ¸ í´ë”: {checkpoint_manager.checkpoint_dir}")
                print("ì‚¬ìš© ê°€ëŠ¥í•œ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼:")
                for f in checkpoint_manager.checkpoint_dir.glob("*.pkl"):
                    print(f"  - {f.name}")
                checkpoint_data = None
            else:
                # ê°€ì¥ ìµœê·¼ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì„ íƒ
                latest_checkpoint = max(checkpoint_files, key=lambda x: x.stat().st_mtime)
                print(f"ê°€ì¥ ìµœê·¼ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼: {latest_checkpoint.name}")
                
                # ì„¸ì…˜ ID ì¶”ì¶œ
                session_id = latest_checkpoint.stem.replace('checkpoint_', '')
                checkpoint_data = checkpoint_manager.load_checkpoint(session_id)
            
            if checkpoint_data and 'results' in checkpoint_data:
                # ì›ë³¸ ë°ì´í„° ë¡œë“œ
                df = pd.read_excel(current_session_info['input_file'])
                
                # ë¶€ë¶„ ê²°ê³¼ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
                partial_results = checkpoint_data['results']
                result_df = pd.DataFrame(partial_results)
                
                # ë¶„ì„ ê²°ê³¼ ì»¬ëŸ¼ ì •ë¦¬
                analysis_columns = [
                    'ì •ì œëœ_í…ìŠ¤íŠ¸', 'ë¹„ì‹ë³„_ì²˜ë¦¬', 'ê°ì •_ë¶„ë¥˜', 'ê°ì •_ê°•ë„_ì ìˆ˜',
                    'í•µì‹¬_í‚¤ì›Œë“œ', 'ì˜ë£Œ_ë§¥ë½', 'ì‹ ë¢°ë„_ì ìˆ˜'
                ]
                
                # ë¹ˆ ê²°ê³¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ì±„ìš°ê¸°
                for i, result in enumerate(partial_results):
                    if result is None:
                        partial_results[i] = {
                            "ì •ì œëœ_í…ìŠ¤íŠ¸": "",
                            "ë¹„ì‹ë³„_ì²˜ë¦¬": False,
                            "ê°ì •_ë¶„ë¥˜": "",
                            "ê°ì •_ê°•ë„_ì ìˆ˜": "",
                            "í•µì‹¬_í‚¤ì›Œë“œ": [],
                            "ì˜ë£Œ_ë§¥ë½": [],
                            "ì‹ ë¢°ë„_ì ìˆ˜": ""
                        }
                
                # original_text í•„ë“œ ì œê±°
                for result in partial_results:
                    if "original_text" in result:
                        del result["original_text"]
                
                # ê²°ê³¼ ë°ì´í„°í”„ë ˆì„ ìƒì„±
                result_df = pd.DataFrame(partial_results)
                
                # ì›ë³¸ ë°ì´í„°ì™€ ê²°í•©
                final_df = pd.concat([
                    df,
                    result_df[[col for col in analysis_columns if col in result_df.columns]]
                ], axis=1)
                
                # ë¶€ë¶„ ê²°ê³¼ íŒŒì¼ëª… ìƒì„±
                partial_output_file = current_session_info['output_file'].replace('.xlsx', '_partial.xlsx')
                
                # ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥
                with pd.ExcelWriter(partial_output_file, engine='openpyxl') as writer:
                    final_df.to_excel(writer, sheet_name='ë¶„ì„ ì‹œíŠ¸', index=False)
                
                processed_count = checkpoint_data.get('processed_count', 0)
                total_count = checkpoint_data.get('total_count', len(df))
                
                print(f"âœ… ë¶€ë¶„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {partial_output_file}")
                print(f"ğŸ“Š ì²˜ë¦¬ í˜„í™©: {processed_count}/{total_count} ({processed_count/total_count*100:.1f}%)")
                
            else:
                print("âŒ ì €ì¥í•  ì²´í¬í¬ì¸íŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                
        except Exception as save_error:
            print(f"âŒ ë¶€ë¶„ ê²°ê³¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {save_error}")
        
        sys.exit(0)
    except Exception as e:
        import traceback
        print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
        print(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        print("\ní•´ê²° ë°©ë²•:")
        print("1. Gemini API í‚¤ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ (Gemini API.json)")
        print("2. API í‚¤ ìœ íš¨ì„± í™•ì¸")
        print("3. ì—‘ì…€ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸")
        print("4. í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜ í™•ì¸ (pandas, openpyxl, tqdm, google-generativeai)")
        sys.exit(1)

# í”„ë¡œê·¸ë¨ì´ ì§ì ‘ ì‹¤í–‰ë  ë•Œë§Œ main() í•¨ìˆ˜ í˜¸ì¶œ
if __name__ == "__main__":
    main() 