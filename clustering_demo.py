import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

def demo_clustering_process():
    """í´ëŸ¬ìŠ¤í„°ë§ ê³¼ì • ë°ëª¨"""
    
    print("ğŸ” í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ë° í´ëŸ¬ìŠ¤í„°ë§ êµ¬í˜„ ë°©ì‹ ì„¤ëª…")
    print("=" * 60)
    
    # 1. í˜„ì¬ ë¶„ì„ëœ íŒŒì¼ ì½ê¸°
    try:
        df = pd.read_excel('í˜‘ì—…í›„ê¸°_ë¶„ì„ê²°ê³¼_ìƒìœ„200ê±´.xlsx', engine='openpyxl')
        print(f"âœ… ë¶„ì„ íŒŒì¼ ë¡œë“œ: {len(df)}ê±´")
    except:
        print("âŒ ë¶„ì„ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € í…ìŠ¤íŠ¸ ë¶„ì„ì„ ì™„ë£Œí•˜ì„¸ìš”.")
        return
    
    # 2. í…ìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
    texts = df['í˜‘ì—…í›„ê¸°_ì •ì œí…ìŠ¤íŠ¸'].fillna('').tolist()
    valid_texts = [t for t in texts if len(str(t).strip()) > 0]
    
    print(f"ğŸ“Š í´ëŸ¬ìŠ¤í„°ë§ ëŒ€ìƒ: {len(valid_texts)}ê±´")
    
    # 3. TF-IDF ë²¡í„°í™”
    print("\nğŸ”¤ 1ë‹¨ê³„: TF-IDF ë²¡í„°í™”")
    vectorizer = TfidfVectorizer(
        max_features=100,  # ìƒìœ„ 100ê°œ ë‹¨ì–´ë§Œ ì‚¬ìš©
        stop_words=None,   # í•œêµ­ì–´ ë¶ˆìš©ì–´ëŠ” ë³„ë„ ì²˜ë¦¬
        ngram_range=(1, 2) # 1-2 ë‹¨ì–´ ì¡°í•©
    )
    
    if len(valid_texts) > 5:  # ìµœì†Œ 5ê°œ ì´ìƒì˜ í…ìŠ¤íŠ¸ê°€ ìˆì„ ë•Œë§Œ
        tfidf_matrix = vectorizer.fit_transform(valid_texts)
        print(f"   ë²¡í„° í¬ê¸°: {tfidf_matrix.shape}")
        
        # 4. K-means í´ëŸ¬ìŠ¤í„°ë§
        print("\nğŸ¯ 2ë‹¨ê³„: K-means í´ëŸ¬ìŠ¤í„°ë§")
        n_clusters = min(5, len(valid_texts) // 10)  # í´ëŸ¬ìŠ¤í„° ìˆ˜ ìë™ ê²°ì •
        if n_clusters < 2:
            n_clusters = 2
            
        kmeans = KMeans(n_clusters=n_clusters, random_state=42)
        clusters = kmeans.fit_predict(tfidf_matrix)
        
        print(f"   í´ëŸ¬ìŠ¤í„° ìˆ˜: {n_clusters}")
        
        # 5. í´ëŸ¬ìŠ¤í„°ë³„ ë¶„í¬
        cluster_counts = Counter(clusters)
        print(f"   í´ëŸ¬ìŠ¤í„° ë¶„í¬: {dict(cluster_counts)}")
        
        # 6. ê° í´ëŸ¬ìŠ¤í„°ì˜ ëŒ€í‘œ í‚¤ì›Œë“œ
        print("\nğŸ“ 3ë‹¨ê³„: í´ëŸ¬ìŠ¤í„°ë³„ ëŒ€í‘œ í‚¤ì›Œë“œ")
        feature_names = vectorizer.get_feature_names_out()
        
        for i in range(n_clusters):
            cluster_center = kmeans.cluster_centers_[i]
            top_indices = cluster_center.argsort()[-5:][::-1]  # ìƒìœ„ 5ê°œ
            top_words = [feature_names[idx] for idx in top_indices]
            
            cluster_texts = [valid_texts[j] for j in range(len(valid_texts)) if clusters[j] == i]
            print(f"   í´ëŸ¬ìŠ¤í„° {i}: {', '.join(top_words)} ({len(cluster_texts)}ê±´)")
            if cluster_texts:
                print(f"     ì˜ˆì‹œ: {cluster_texts[0][:50]}...")
        
        # 7. ìœ ì‚¬ë„ ê³„ì‚° (ìƒ˜í”Œ)
        print("\nğŸ“ 4ë‹¨ê³„: í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚° (ìƒìœ„ 5ê±´)")
        similarity_matrix = cosine_similarity(tfidf_matrix[:5])
        
        for i in range(min(5, len(valid_texts))):
            for j in range(i+1, min(5, len(valid_texts))):
                similarity = similarity_matrix[i][j]
                if similarity > 0.1:  # ìœ ì‚¬ë„ê°€ 0.1 ì´ìƒì¸ ê²½ìš°ë§Œ
                    print(f"   í…ìŠ¤íŠ¸ {i+1} â†” í…ìŠ¤íŠ¸ {j+1}: {similarity:.3f}")
                    print(f"     1: {valid_texts[i][:30]}...")
                    print(f"     2: {valid_texts[j][:30]}...")
                    print()
    
    print("\n" + "=" * 60)
    print("ğŸ’¡ í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ í™œìš© ë°©ì•ˆ:")
    print("1. ì—‘ì…€ ì»¬ëŸ¼ ì¶”ê°€: ê° í”¼ë“œë°±ì˜ í´ëŸ¬ìŠ¤í„° ë²ˆí˜¸")
    print("2. ë³„ë„ ì‹œíŠ¸: í´ëŸ¬ìŠ¤í„°ë³„ ìš”ì•½ ë° ëŒ€í‘œ í”¼ë“œë°±")
    print("3. ì‹œê°í™”: í´ëŸ¬ìŠ¤í„° ë¶„í¬ë„ ë° ì›Œë“œí´ë¼ìš°ë“œ")
    print("4. ìœ ì‚¬ í”¼ë“œë°± ê·¸ë£¹í•‘: ì¤‘ë³µ ì´ìŠˆ ì‹ë³„")

def add_clustering_to_excel():
    """ì‹¤ì œë¡œ í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ë¥¼ ì—‘ì…€ì— ì¶”ê°€í•˜ëŠ” í•¨ìˆ˜"""
    
    print("\nğŸ”§ ì‹¤ì œ í´ëŸ¬ìŠ¤í„°ë§ ì ìš©")
    print("=" * 40)
    
    try:
        # ë¶„ì„ëœ íŒŒì¼ ì½ê¸°
        df = pd.read_excel('í˜‘ì—…í›„ê¸°_ë¶„ì„ê²°ê³¼_ìƒìœ„200ê±´.xlsx', engine='openpyxl')
        
        # ìœ íš¨í•œ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
        valid_indices = df['í˜‘ì—…í›„ê¸°_ì •ì œí…ìŠ¤íŠ¸'].notna() & (df['í˜‘ì—…í›„ê¸°_ì •ì œí…ìŠ¤íŠ¸'].str.len() > 0)
        valid_df = df[valid_indices].copy()
        
        if len(valid_df) < 5:
            print("âŒ í´ëŸ¬ìŠ¤í„°ë§ì„ ìœ„í•œ í…ìŠ¤íŠ¸ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
            return
        
        # TF-IDF ë²¡í„°í™”
        vectorizer = TfidfVectorizer(max_features=50, ngram_range=(1, 2))
        tfidf_matrix = vectorizer.fit_transform(valid_df['í˜‘ì—…í›„ê¸°_ì •ì œí…ìŠ¤íŠ¸'])
        
        # K-means í´ëŸ¬ìŠ¤í„°ë§
        n_clusters = min(8, len(valid_df) // 15)  # ì ì ˆí•œ í´ëŸ¬ìŠ¤í„° ìˆ˜
        if n_clusters < 2:
            n_clusters = 2
            
        kmeans = KMeans(n_clusters=n_clusters, random_state=42)
        clusters = kmeans.fit_predict(tfidf_matrix)
        
        # ì›ë³¸ DataFrameì— í´ëŸ¬ìŠ¤í„° ê²°ê³¼ ì¶”ê°€
        df['í˜‘ì—…í›„ê¸°_í´ëŸ¬ìŠ¤í„°'] = np.nan
        df.loc[valid_indices, 'í˜‘ì—…í›„ê¸°_í´ëŸ¬ìŠ¤í„°'] = clusters
        
        # í´ëŸ¬ìŠ¤í„°ë³„ ìš”ì•½ ìƒì„±
        cluster_summary = []
        feature_names = vectorizer.get_feature_names_out()
        
        for i in range(n_clusters):
            cluster_mask = (df['í˜‘ì—…í›„ê¸°_í´ëŸ¬ìŠ¤í„°'] == i)
            cluster_data = df[cluster_mask]
            
            # í´ëŸ¬ìŠ¤í„° ëŒ€í‘œ í‚¤ì›Œë“œ
            cluster_center = kmeans.cluster_centers_[i]
            top_indices = cluster_center.argsort()[-3:][::-1]
            keywords = [feature_names[idx] for idx in top_indices]
            
            # ê°ì • ë¶„í¬
            sentiments = cluster_data['í˜‘ì—…í›„ê¸°_ì£¼ê°ì •'].value_counts()
            main_sentiment = sentiments.index[0] if len(sentiments) > 0 else "ì¤‘ë¦½"
            
            cluster_summary.append({
                'í´ëŸ¬ìŠ¤í„°': f"ê·¸ë£¹ {i+1}",
                'ê±´ìˆ˜': len(cluster_data),
                'ì£¼ìš”í‚¤ì›Œë“œ': ', '.join(keywords),
                'ì£¼ê°ì •': main_sentiment,
                'ëŒ€í‘œì˜ˆì‹œ': cluster_data['í˜‘ì—…í›„ê¸°_ì •ì œí…ìŠ¤íŠ¸'].iloc[0][:50] + "..." if len(cluster_data) > 0 else ""
            })
        
        # ê²°ê³¼ ì €ì¥
        output_file = 'í˜‘ì—…í›„ê¸°_ë¶„ì„ê²°ê³¼_í´ëŸ¬ìŠ¤í„°ë§í¬í•¨_ìƒìœ„200ê±´.xlsx'
        
        with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
            # ë©”ì¸ ì‹œíŠ¸ (ê¸°ì¡´ ë¶„ì„ + í´ëŸ¬ìŠ¤í„°)
            df.to_excel(writer, sheet_name='ë¶„ì„ê²°ê³¼', index=False)
            
            # í´ëŸ¬ìŠ¤í„° ìš”ì•½ ì‹œíŠ¸
            pd.DataFrame(cluster_summary).to_excel(writer, sheet_name='í´ëŸ¬ìŠ¤í„°ìš”ì•½', index=False)
        
        print(f"âœ… í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ì €ì¥: {output_file}")
        print(f"ğŸ“Š {n_clusters}ê°œ í´ëŸ¬ìŠ¤í„°ë¡œ ë¶„ë¥˜")
        
        # í´ëŸ¬ìŠ¤í„°ë³„ ë¶„í¬ ì¶œë ¥
        print(f"\nğŸ“ˆ í´ëŸ¬ìŠ¤í„°ë³„ ë¶„í¬:")
        cluster_counts = df['í˜‘ì—…í›„ê¸°_í´ëŸ¬ìŠ¤í„°'].value_counts().sort_index()
        for cluster_id, count in cluster_counts.items():
            if pd.notna(cluster_id):
                print(f"  ê·¸ë£¹ {int(cluster_id)+1}: {count}ê±´")
        
        return output_file
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    # 1. í´ëŸ¬ìŠ¤í„°ë§ ê³¼ì • ì„¤ëª…
    demo_clustering_process()
    
    # 2. ì‹¤ì œ ì ìš© ì—¬ë¶€ í™•ì¸
    print(f"\n{'='*60}")
    print("ğŸ¤” ì‹¤ì œë¡œ í´ëŸ¬ìŠ¤í„°ë§ì„ ì ìš©í•˜ì‹œê² ìŠµë‹ˆê¹Œ?")
    print("- í˜„ì¬ ë¶„ì„ íŒŒì¼ì— 'í˜‘ì—…í›„ê¸°_í´ëŸ¬ìŠ¤í„°' ì»¬ëŸ¼ì´ ì¶”ê°€ë©ë‹ˆë‹¤")
    print("- ë³„ë„ 'í´ëŸ¬ìŠ¤í„°ìš”ì•½' ì‹œíŠ¸ê°€ ìƒì„±ë©ë‹ˆë‹¤")
    print("- ìœ ì‚¬í•œ í”¼ë“œë°±ë“¤ì´ ê·¸ë£¹ë³„ë¡œ ë¶„ë¥˜ë©ë‹ˆë‹¤")