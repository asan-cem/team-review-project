#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë¶€ì„œê°„ ê´€ê³„ í’ˆì§ˆ ë¶„ì„ê¸°
A, B ë¶€ì„œê°„ì˜ í‰ê°€ ì ìˆ˜, ì‘ë‹µìˆ˜, ê°ì •ë¶„ì„ ë“±ì„ ì¢…í•©í•˜ì—¬ ê´€ê³„ í’ˆì§ˆì„ í‰ê°€í•©ë‹ˆë‹¤.

ğŸ“‹ ì£¼ìš” ê¸°ëŠ¥:
1. ë¶€ì„œê°„ ìƒí˜¸í‰ê°€ ë°ì´í„° ë¶„ì„
2. ì •ëŸ‰ì /ì •ì„±ì  ì§€í‘œ ê³„ì‚°
3. ê´€ê³„ í’ˆì§ˆ ë“±ê¸‰ ë¶„ë¥˜ (S/A/B/C/D)
4. ê°œì„  í¬ì¸íŠ¸ ì œì•ˆ

ğŸ“Š ë¶„ì„ ì§€í‘œ:
- ì •ëŸ‰ì : ì ìˆ˜, ì‘ë‹µìˆ˜, í˜‘ì—… ê·œëª¨, ë°ì´í„° ì‹ ë¢°ì„±
- ì •ì„±ì : ê°ì • ë¶„ì„, í…ìŠ¤íŠ¸ í’ˆì§ˆ

ì‘ì„±ì: Claude AI
ë²„ì „: 1.0
ìƒì„±ì¼: 2025ë…„ 7ì›” 16ì¼
"""

import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# ============================================================================
# ğŸ”§ ì„¤ì • ë° ìƒìˆ˜ ì •ì˜
# ============================================================================

INPUT_DATA_FILE = "rawdata/2. text_processor_ê²°ê³¼_20250715_160846.xlsx"
OUTPUT_FILE = "ë¶€ì„œìŒë³„_ê´€ê³„ë¶„ì„_ê²°ê³¼.xlsx"

# ë¶„ì„ ê¸°ì¤€ ì„¤ì •
MIN_RESPONSES = 5      # ìµœì†Œ ì‘ë‹µìˆ˜
MIN_MUTUAL_RESPONSES = 3  # ìµœì†Œ ìƒí˜¸í‰ê°€ ì‘ë‹µìˆ˜

# ë“±ê¸‰ ê¸°ì¤€ì  (í…ìŠ¤íŠ¸ ì§€í‘œ ì œê±° í›„ ì¡°ì •)
GRADE_THRESHOLDS = {
    'S': {'score': 85, 'balance': 95, 'responses': 20, 'emotion': 1.6, 'continuity': 0.75},
    'A': {'score': 80, 'balance': 90, 'responses': 15, 'emotion': 1.4, 'continuity': 0.5},
    'B': {'score': 75, 'balance': 80, 'responses': 10, 'emotion': 1.0, 'continuity': 0.25},
    'C': {'score': 65, 'balance': 60, 'responses': 5, 'emotion': 0.6, 'continuity': 0},
}

# ì‹œê³„ì—´ ë¶„ì„ ê¸°ì¤€
TREND_THRESHOLDS = {
    'ê¸´ê¸‰': {'score_decline': -10, 'consecutive_years': 2},
    'ì£¼ì˜': {'score_decline': -5, 'recent_years': 1},
    'ê°œì„ ': {'score_improve': 5, 'consecutive_years': 2},
    'ëª¨ë²”': {'min_score': 80, 'stable_years': 4}
}

# ============================================================================
# ğŸ› ï¸ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
# ============================================================================

def load_and_preprocess_data():
    """
    ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ë¶„ì„ì— í•„ìš”í•œ ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    print("ğŸš€ ë¶€ì„œê°„ ê´€ê³„ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    try:
        df = pd.read_excel(INPUT_DATA_FILE)
        print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df):,}ê±´")
    except FileNotFoundError:
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {INPUT_DATA_FILE}")
        return None
    
    # ì»¬ëŸ¼ëª… ì •ë¦¬
    score_columns = [
        'â—‹â—‹ì€ íƒ€ ë¶€ì„œì˜ ì…ì¥ì„ ì¡´ì¤‘í•˜ê³  ë°°ë ¤í•˜ì—¬ í˜‘ë ¥í•´ì£¼ë©°. í˜‘ì—… ê´€ë ¨ ì˜ê²¬ì„ ê²½ì²­í•´ì¤€ë‹¤.',
        'â—‹â—‹ì€ ì—…ë¬´ìƒ í•„ìš”í•œ ì •ë³´ì— ëŒ€í•´ ê³µìœ ê°€ ì˜ ì´ë£¨ì–´ì§„ë‹¤.',
        'â—‹â—‹ì€ ì—…ë¬´ì— ëŒ€í•œ ëª…í™•í•œ ë‹´ë‹¹ìê°€ ìˆê³  ì—…ë¬´ë¥¼ ì¼ê´€ì„±ìˆê²Œ ì²˜ë¦¬í•´ì¤€ë‹¤.',
        'â—‹â—‹ì€ ì´ì „ë³´ë‹¤ ì—…ë¬´ í˜‘ë ¥ì— ëŒ€í•œ íƒœë„ë‚˜ ì˜ì§€ê°€ ê°œì„ ë˜ê³  ìˆë‹¤.',
        'ì „ë°˜ì ìœ¼ë¡œ â—‹â—‹ê³¼ì˜ í˜‘ì—…ì— ëŒ€í•´ ë§Œì¡±í•œë‹¤.'
    ]
    
    # ì§§ì€ ì»¬ëŸ¼ëª…ìœ¼ë¡œ ë§¤í•‘
    column_mapping = {
        score_columns[0]: 'ì¡´ì¤‘ë°°ë ¤',
        score_columns[1]: 'ì •ë³´ê³µìœ ', 
        score_columns[2]: 'ëª…í™•ì²˜ë¦¬',
        score_columns[3]: 'íƒœë„ê°œì„ ',
        score_columns[4]: 'ì „ë°˜ë§Œì¡±'
    }
    
    df = df.rename(columns=column_mapping)
    
    # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
    required_columns = ['ì„¤ë¬¸ì‹œí–‰ì—°ë„', 'í‰ê°€_ë¶€ì„œëª…', 'í”¼í‰ê°€ëŒ€ìƒ ë¶€ì„œëª…', 'ì¢…í•©ì ìˆ˜']
    missing_columns = [col for col in required_columns if col not in df.columns]
    
    if missing_columns:
        print(f"âŒ í•„ìˆ˜ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {missing_columns}")
        return None
    
    # ë°ì´í„° ì „ì²˜ë¦¬
    df['ì„¤ë¬¸ì‹œí–‰ì—°ë„'] = df['ì„¤ë¬¸ì‹œí–‰ì—°ë„'].astype(str)
    df = df.dropna(subset=['í‰ê°€_ë¶€ì„œëª…', 'í”¼í‰ê°€ëŒ€ìƒ ë¶€ì„œëª…', 'ì¢…í•©ì ìˆ˜'])
    
    # ê°ì • ë¶„ë¥˜ ê²°ì¸¡ê°’ ì²˜ë¦¬
    if 'ê°ì •_ë¶„ë¥˜' in df.columns:
        df['ê°ì •_ë¶„ë¥˜'] = df['ê°ì •_ë¶„ë¥˜'].fillna('ì¤‘ë¦½')
    
    print(f"ğŸ“Š ì „ì²˜ë¦¬ ì™„ë£Œ: {len(df):,}ê±´")
    print(f"ğŸ“… ë¶„ì„ ê¸°ê°„: {df['ì„¤ë¬¸ì‹œí–‰ì—°ë„'].min()} ~ {df['ì„¤ë¬¸ì‹œí–‰ì—°ë„'].max()}")
    
    return df

# ============================================================================
# ğŸ“Š ì •ëŸ‰ì  ì§€í‘œ ê³„ì‚°
# ============================================================================

def calculate_quantitative_metrics(df, dept_a, dept_b):
    """
    ë‘ ë¶€ì„œê°„ì˜ ì •ëŸ‰ì  ì§€í‘œë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    """
    # Aâ†’B í‰ê°€ ë°ì´í„°
    a_to_b = df[(df['í‰ê°€_ë¶€ì„œëª…'] == dept_a) & (df['í”¼í‰ê°€ëŒ€ìƒ ë¶€ì„œëª…'] == dept_b)]
    # Bâ†’A í‰ê°€ ë°ì´í„°  
    b_to_a = df[(df['í‰ê°€_ë¶€ì„œëª…'] == dept_b) & (df['í”¼í‰ê°€ëŒ€ìƒ ë¶€ì„œëª…'] == dept_a)]
    
    # ê¸°ë³¸ í†µê³„
    a_to_b_responses = len(a_to_b)
    b_to_a_responses = len(b_to_a)
    total_responses = a_to_b_responses + b_to_a_responses
    
    if total_responses == 0:
        return None
    
    # ì ìˆ˜ ê´€ë ¨ ì§€í‘œ
    a_to_b_score = a_to_b['ì¢…í•©ì ìˆ˜'].mean() if a_to_b_responses > 0 else 0
    b_to_a_score = b_to_a['ì¢…í•©ì ìˆ˜'].mean() if b_to_a_responses > 0 else 0
    
    mutual_avg_score = (a_to_b_score + b_to_a_score) / 2 if min(a_to_b_responses, b_to_a_responses) > 0 else max(a_to_b_score, b_to_a_score)
    score_balance = 100 - abs(a_to_b_score - b_to_a_score) if min(a_to_b_responses, b_to_a_responses) > 0 else 50
    
    # ì‘ë‹µ ê´€ë ¨ ì§€í‘œ
    response_balance = min(a_to_b_responses, b_to_a_responses) / max(a_to_b_responses, b_to_a_responses) * 100 if max(a_to_b_responses, b_to_a_responses) > 0 else 0
    
    # í˜‘ì—… ì§€ì†ì„± (ì—°ë„ë³„ ë¶„í¬)
    years_active = len(set(list(a_to_b['ì„¤ë¬¸ì‹œí–‰ì—°ë„']) + list(b_to_a['ì„¤ë¬¸ì‹œí–‰ì—°ë„'])))
    total_years = len(df['ì„¤ë¬¸ì‹œí–‰ì—°ë„'].unique())
    continuity = years_active / total_years
    
    # ì„¸ë¶€í•­ëª© ì¼ê´€ì„± (ë¶„ì‚°ê³„ìˆ˜)
    score_cols = ['ì¡´ì¤‘ë°°ë ¤', 'ì •ë³´ê³µìœ ', 'ëª…í™•ì²˜ë¦¬', 'íƒœë„ê°œì„ ', 'ì „ë°˜ë§Œì¡±']
    available_score_cols = [col for col in score_cols if col in df.columns]
    
    consistency_score = 0
    if available_score_cols and total_responses > 0:
        all_scores = pd.concat([a_to_b[available_score_cols], b_to_a[available_score_cols]])
        if len(all_scores) > 0:
            means = all_scores.mean()
            stds = all_scores.std()
            cv = stds / means  # ë³€ë™ê³„ìˆ˜
            consistency_score = max(0, 100 - cv.mean() * 10)  # ì¼ê´€ì„± ì ìˆ˜ë¡œ ë³€í™˜
    
    # ì‹ ë¢°ì„± ì§€í‘œ
    reliability_score = 100
    if 'ê·¹ë‹¨ê°’' in df.columns:
        # ê·¹ë‹¨ê°’ ì»¬ëŸ¼ì„ ìˆ«ìí˜•ìœ¼ë¡œ ë³€í™˜ (True/False -> 1/0)
        a_to_b_extreme = pd.to_numeric(a_to_b['ê·¹ë‹¨ê°’'], errors='coerce').fillna(0)
        b_to_a_extreme = pd.to_numeric(b_to_a['ê·¹ë‹¨ê°’'], errors='coerce').fillna(0)
        extreme_ratio = (a_to_b_extreme.sum() + b_to_a_extreme.sum()) / total_responses if total_responses > 0 else 0
        reliability_score *= (1 - extreme_ratio)
    
    if 'ì‹ ë¢°ë„_ì ìˆ˜' in df.columns:
        trust_scores = pd.concat([a_to_b['ì‹ ë¢°ë„_ì ìˆ˜'], b_to_a['ì‹ ë¢°ë„_ì ìˆ˜']]).dropna()
        if len(trust_scores) > 0:
            avg_trust = pd.to_numeric(trust_scores, errors='coerce').mean()
            if not pd.isna(avg_trust):
                reliability_score *= avg_trust / 100
    
    return {
        'a_to_b_score': round(a_to_b_score, 2),
        'b_to_a_score': round(b_to_a_score, 2),
        'mutual_avg_score': round(mutual_avg_score, 2),
        'score_balance': round(score_balance, 1),
        'a_to_b_responses': a_to_b_responses,
        'b_to_a_responses': b_to_a_responses,
        'total_responses': total_responses,
        'response_balance': round(response_balance, 1),
        'years_active': years_active,
        'continuity': round(continuity, 3),
        'consistency_score': round(consistency_score, 1),
        'reliability_score': round(reliability_score, 1)
    }

# ============================================================================
# ğŸ˜Š ì •ì„±ì  ì§€í‘œ ê³„ì‚°  
# ============================================================================

def calculate_qualitative_metrics(df, dept_a, dept_b):
    """
    ë‘ ë¶€ì„œê°„ì˜ ì •ì„±ì  ì§€í‘œë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    """
    # Aâ†’B, Bâ†’A í‰ê°€ ë°ì´í„°
    a_to_b = df[(df['í‰ê°€_ë¶€ì„œëª…'] == dept_a) & (df['í”¼í‰ê°€ëŒ€ìƒ ë¶€ì„œëª…'] == dept_b)]
    b_to_a = df[(df['í‰ê°€_ë¶€ì„œëª…'] == dept_b) & (df['í”¼í‰ê°€ëŒ€ìƒ ë¶€ì„œëª…'] == dept_a)]
    
    combined_data = pd.concat([a_to_b, b_to_a])
    
    if len(combined_data) == 0:
        return None
    
    # ê°ì • ë¶„ì„ ì§€í‘œ
    emotion_metrics = {'emotion_score': 0, 'emotion_intensity': 0, 'emotion_consistency': 0}
    
    if 'ê°ì •_ë¶„ë¥˜' in df.columns:
        emotions = combined_data['ê°ì •_ë¶„ë¥˜'].value_counts()
        total = len(combined_data)
        
        positive_ratio = emotions.get('ê¸ì •', 0) / total
        neutral_ratio = emotions.get('ì¤‘ë¦½', 0) / total  
        negative_ratio = emotions.get('ë¶€ì •', 0) / total
        
        # ê°ì • ì ìˆ˜ (0~2 ìŠ¤ì¼€ì¼)
        emotion_score = (positive_ratio * 2 + neutral_ratio * 1 + negative_ratio * 0)
        
        # ê°ì • ê°•ë„
        emotion_intensity = 0
        if 'ê°ì •_ê°•ë„_ì ìˆ˜' in df.columns:
            intensity_scores = combined_data['ê°ì •_ê°•ë„_ì ìˆ˜'].dropna()
            if len(intensity_scores) > 0:
                emotion_intensity = intensity_scores.mean() / 100
        
        # ê°ì • ì¼ê´€ì„± (ì—”íŠ¸ë¡œí”¼ ê¸°ë°˜)
        emotion_consistency = 0
        if total > 1:
            probabilities = [positive_ratio, neutral_ratio, negative_ratio]
            probabilities = [p for p in probabilities if p > 0]
            if len(probabilities) > 1:
                entropy = -sum(p * np.log2(p) for p in probabilities)
                max_entropy = np.log2(3)  # 3ê°€ì§€ ê°ì •ì˜ ìµœëŒ€ ì—”íŠ¸ë¡œí”¼
                emotion_consistency = max(0, 100 * (1 - entropy / max_entropy))
            else:
                emotion_consistency = 100  # í•˜ë‚˜ì˜ ê°ì •ë§Œ ìˆìœ¼ë©´ ì™„ì „ ì¼ê´€ì„±
        
        emotion_metrics = {
            'emotion_score': round(emotion_score, 3),
            'emotion_intensity': round(emotion_intensity, 3),
            'emotion_consistency': round(emotion_consistency, 1),
            'positive_ratio': round(positive_ratio * 100, 1),
            'negative_ratio': round(negative_ratio * 100, 1)
        }
    
    # í…ìŠ¤íŠ¸ ê´€ë ¨ ì§€í‘œëŠ” ë¶„ì„ì—ì„œ ì œì™¸
    # - í…ìŠ¤íŠ¸_í’ë¶€ë„: ë°ì´í„° í’ˆì§ˆ í¸ì°¨ í¼
    # - í‚¤ì›Œë“œ_ë‹¤ì–‘ì„±: í‘œì¤€í™” ì–´ë ¤ì›€
    # - ì˜ë£Œ_ë§¥ë½: ë°ì´í„° í˜•ì‹ ë¶ˆì¼ì¹˜
    
    return emotion_metrics

# ============================================================================
# ğŸ“ˆ ì‹œê³„ì—´ ë¶„ì„ í•¨ìˆ˜ë“¤
# ============================================================================

def calculate_yearly_metrics(df, dept_a, dept_b, year):
    """
    íŠ¹ì • ì—°ë„ì˜ ë¶€ì„œê°„ ê´€ê³„ ì§€í‘œë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    """
    year_data = df[df['ì„¤ë¬¸ì‹œí–‰ì—°ë„'] == year]
    
    quant_metrics = calculate_quantitative_metrics(year_data, dept_a, dept_b)
    qual_metrics = calculate_qualitative_metrics(year_data, dept_a, dept_b)
    
    if not quant_metrics or not qual_metrics:
        return None
    
    return {
        'year': year,
        'mutual_avg_score': quant_metrics['mutual_avg_score'],
        'score_balance': quant_metrics['score_balance'],
        'total_responses': quant_metrics['total_responses'],
        'emotion_score': qual_metrics['emotion_score'],
        'positive_ratio': qual_metrics['positive_ratio'],
        'negative_ratio': qual_metrics['negative_ratio']
    }

def analyze_relationship_trends(df, dept_a, dept_b):
    """
    2022~2025ë…„ ë¶€ì„œê°„ ê´€ê³„ ë³€í™” ì¶”ì´ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
    """
    years = ['2022', '2023', '2024', '2025']
    yearly_data = {}
    
    # ì—°ë„ë³„ ì§€í‘œ ìˆ˜ì§‘
    for year in years:
        metrics = calculate_yearly_metrics(df, dept_a, dept_b, year)
        if metrics:
            yearly_data[year] = metrics
    
    if len(yearly_data) < 2:
        return None  # ìµœì†Œ 2ë…„ ë°ì´í„° í•„ìš”
    
    # íŠ¸ë Œë“œ ì§€í‘œ ê³„ì‚°
    scores = [yearly_data[year]['mutual_avg_score'] for year in sorted(yearly_data.keys())]
    emotions = [yearly_data[year]['emotion_score'] for year in sorted(yearly_data.keys())]
    responses = [yearly_data[year]['total_responses'] for year in sorted(yearly_data.keys())]
    
    # ì„ í˜• íšŒê·€ ê¸°ìš¸ê¸° ê³„ì‚° (íŠ¸ë Œë“œ)
    def calculate_slope(values):
        if len(values) < 2:
            return 0
        x = list(range(len(values)))
        n = len(values)
        slope = (n * sum(x[i] * values[i] for i in range(n)) - sum(x) * sum(values)) / (n * sum(x[i]**2 for i in range(n)) - sum(x)**2)
        return slope
    
    score_trend = calculate_slope(scores)
    emotion_trend = calculate_slope(emotions)
    response_trend = calculate_slope(responses)
    
    # ìµœê·¼ vs ì´ˆê¸° ë¹„êµ
    recent_years = [year for year in ['2024', '2025'] if year in yearly_data]
    early_years = [year for year in ['2022', '2023'] if year in yearly_data]
    
    recent_avg_score = np.mean([yearly_data[year]['mutual_avg_score'] for year in recent_years]) if recent_years else 0
    early_avg_score = np.mean([yearly_data[year]['mutual_avg_score'] for year in early_years]) if early_years else 0
    
    recent_improvement = recent_avg_score - early_avg_score if early_years and recent_years else 0
    
    # ë³€ë™ì„± ê³„ì‚°
    score_volatility = np.std(scores) if len(scores) > 1 else 0
    
    # ì—°ì†ì„± ë¶„ì„
    data_years = len(yearly_data)
    data_continuity = data_years / 4  # 4ë…„ ì¤‘ ëª‡ ë…„ì˜ ë°ì´í„°ê°€ ìˆëŠ”ì§€
    
    return {
        'yearly_data': yearly_data,
        'score_trend': round(score_trend, 3),
        'emotion_trend': round(emotion_trend, 3),
        'response_trend': round(response_trend, 3),
        'recent_improvement': round(recent_improvement, 2),
        'score_volatility': round(score_volatility, 2),
        'data_years': data_years,
        'data_continuity': round(data_continuity, 3)
    }

def classify_trend_pattern(trend_analysis):
    """
    ê´€ê³„ ë³€í™” íŒ¨í„´ì„ ë¶„ë¥˜í•©ë‹ˆë‹¤.
    """
    if not trend_analysis:
        return 'insufficient_data', "ë°ì´í„° ë¶€ì¡±"
    
    score_trend = trend_analysis['score_trend']
    recent_improvement = trend_analysis['recent_improvement']
    volatility = trend_analysis['score_volatility']
    data_years = trend_analysis['data_years']
    
    # íŒ¨í„´ ë¶„ë¥˜ ë¡œì§
    if data_years < 2:
        return 'insufficient_data', "ë¶„ì„ ê¸°ê°„ ë¶€ì¡±"
    
    # ê°•í•œ ê°œì„  ì¶”ì„¸
    if score_trend > 2 and recent_improvement > 5:
        return 'improving', f"ì§€ì†ì  ê°œì„  (ê¸°ìš¸ê¸°: {score_trend:.1f}, ìµœê·¼ê°œì„ : {recent_improvement:.1f}ì )"
    
    # ê°•í•œ ì•…í™” ì¶”ì„¸  
    if score_trend < -2 and recent_improvement < -5:
        return 'declining', f"ì§€ì†ì  ì•…í™” (ê¸°ìš¸ê¸°: {score_trend:.1f}, ìµœê·¼í•˜ë½: {recent_improvement:.1f}ì )"
    
    # íšŒë³µ íŒ¨í„´
    if score_trend > 1 and recent_improvement > 3:
        return 'recovering', f"íšŒë³µ ì¤‘ (ìµœê·¼ê°œì„ : {recent_improvement:.1f}ì )"
    
    # ì•ˆì • íŒ¨í„´
    if abs(score_trend) <= 1 and volatility < 5:
        return 'stable', f"ì•ˆì •ì  (ë³€ë™ì„±: {volatility:.1f})"
    
    # ê¸‰ë³€ íŒ¨í„´
    if volatility > 10:
        return 'volatile', f"ë³€ë™ì„± ë†’ìŒ (í‘œì¤€í¸ì°¨: {volatility:.1f})"
    
    # ê¸°ë³¸ (ë³´í†µ)
    return 'neutral', f"ë³´í†µ ìˆ˜ì¤€ (ê¸°ìš¸ê¸°: {score_trend:.1f})"

def generate_trend_alert(trend_analysis, yearly_data):
    """
    ì¡°ê¸° ê²½ë³´ ì‹œìŠ¤í…œ: ê´€ê³„ ì•…í™” ìœ„í—˜ì„ ê°ì§€í•©ë‹ˆë‹¤.
    """
    if not trend_analysis or not yearly_data:
        return 'normal', "ë°ì´í„° ë¶€ì¡±"
    
    recent_improvement = trend_analysis['recent_improvement']
    score_trend = trend_analysis['score_trend']
    
    # ìµœê·¼ 2ë…„ ì—°ì† í•˜ë½ ì²´í¬
    years = sorted(yearly_data.keys())
    if len(years) >= 3:
        last_3_scores = [yearly_data[year]['mutual_avg_score'] for year in years[-3:]]
        consecutive_decline = all(last_3_scores[i] > last_3_scores[i+1] for i in range(len(last_3_scores)-1))
        
        if consecutive_decline and (last_3_scores[0] - last_3_scores[-1]) >= 10:
            return 'urgent', f"ğŸš¨ ê¸´ê¸‰: 3ë…„ ì—°ì† í•˜ë½ ({last_3_scores[0]:.1f}â†’{last_3_scores[-1]:.1f}ì )"
    
    # ìµœê·¼ ê¸‰ê²©í•œ í•˜ë½
    if recent_improvement <= -10:
        return 'urgent', f"ğŸš¨ ê¸´ê¸‰: ìµœê·¼ ê¸‰ê²©í•œ í•˜ë½ ({recent_improvement:.1f}ì )"
    
    # ì£¼ì˜ í•„ìš”
    if recent_improvement <= -5 or score_trend <= -2:
        return 'warning', f"âš ï¸ ì£¼ì˜: ê´€ê³„ ì•…í™” ì§•í›„ (ê°œì„ ë„: {recent_improvement:.1f}ì )"
    
    # ê°œì„  ì¤‘
    if recent_improvement >= 5 and score_trend >= 1:
        return 'improving', f"ğŸ“ˆ ê°œì„ : ì§€ì†ì  ê´€ê³„ í–¥ìƒ (ê°œì„ ë„: {recent_improvement:.1f}ì )"
    
    # ëª¨ë²” ì‚¬ë¡€
    recent_scores = [yearly_data[year]['mutual_avg_score'] for year in sorted(yearly_data.keys())[-2:]]
    if all(score >= 80 for score in recent_scores) and len(yearly_data) >= 3:
        return 'excellent', f"â­ ëª¨ë²”: ì§€ì†ì  ìš°ìˆ˜ ê´€ê³„ (ìµœê·¼ í‰ê· : {np.mean(recent_scores):.1f}ì )"
    
    return 'normal', "âœ… ì •ìƒ ë²”ìœ„"

# ============================================================================
# ğŸ† ê´€ê³„ í’ˆì§ˆ ë“±ê¸‰ ë¶„ë¥˜
# ============================================================================

def classify_relationship_grade(quant_metrics, qual_metrics):
    """
    ì •ëŸ‰ì /ì •ì„±ì  ì§€í‘œë¥¼ ì¢…í•©í•˜ì—¬ ê´€ê³„ í’ˆì§ˆ ë“±ê¸‰ì„ ë¶„ë¥˜í•©ë‹ˆë‹¤.
    """
    if not quant_metrics or not qual_metrics:
        return 'F', "ë°ì´í„° ë¶€ì¡±"
    
    # ê¸°ë³¸ ì¡°ê±´ í™•ì¸
    if quant_metrics['total_responses'] < MIN_RESPONSES:
        return 'F', f"ì‘ë‹µìˆ˜ ë¶€ì¡± ({quant_metrics['total_responses']}ê±´)"
    
    if min(quant_metrics['a_to_b_responses'], quant_metrics['b_to_a_responses']) < MIN_MUTUAL_RESPONSES:
        return 'F', f"ìƒí˜¸í‰ê°€ ë¶€ì¡± (Aâ†’B: {quant_metrics['a_to_b_responses']}, Bâ†’A: {quant_metrics['b_to_a_responses']})"
    
    # ë“±ê¸‰ë³„ ê¸°ì¤€ í™•ì¸
    for grade in ['S', 'A', 'B', 'C']:
        criteria = GRADE_THRESHOLDS[grade]
        
        conditions = [
            quant_metrics['mutual_avg_score'] >= criteria['score'],
            quant_metrics['score_balance'] >= criteria['balance'],
            quant_metrics['total_responses'] >= criteria['responses'],
            qual_metrics['emotion_score'] >= criteria['emotion'],
            quant_metrics['continuity'] >= criteria['continuity']
        ]
        
        if all(conditions):
            reasons = [
                f"í‰ê· ì ìˆ˜ {quant_metrics['mutual_avg_score']:.1f}ì ",
                f"ì ìˆ˜ê· í˜• {quant_metrics['score_balance']:.1f}%",
                f"ì‘ë‹µìˆ˜ {quant_metrics['total_responses']}ê±´",
                f"ê°ì •ì ìˆ˜ {qual_metrics['emotion_score']:.2f}",
                f"ì§€ì†ì„± {quant_metrics['continuity']:.2f}"
            ]
            return grade, " | ".join(reasons)
    
    return 'D', f"ê¸°ì¤€ ë¯¸ë‹¬ (ì ìˆ˜: {quant_metrics['mutual_avg_score']:.1f}, ê°ì •: {qual_metrics['emotion_score']:.2f})"

# ============================================================================
# ğŸ“ ê°œì„  í¬ì¸íŠ¸ ì œì•ˆ
# ============================================================================

def suggest_improvements(quant_metrics, qual_metrics, grade):
    """
    ê´€ê³„ í’ˆì§ˆ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°œì„  í¬ì¸íŠ¸ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.
    """
    suggestions = []
    
    if grade in ['D', 'F']:
        if quant_metrics['mutual_avg_score'] < 70:
            suggestions.append("âš ï¸ í˜‘ì—… í”„ë¡œì„¸ìŠ¤ ì „ë°˜ ì ê²€ í•„ìš”")
        
        if qual_metrics['negative_ratio'] > 30:
            suggestions.append("ğŸ˜Ÿ ë¶€ì • ê°ì • í•´ì†Œë¥¼ ìœ„í•œ ì†Œí†µ ê°•í™” í•„ìš”")
        
        if quant_metrics['score_balance'] < 60:
            suggestions.append("âš–ï¸ ì¼ë°©ì  ê´€ê³„ - ìƒí˜¸ ì´í•´ ì¦ì§„ í•„ìš”")
    
    elif grade in ['B', 'C']:
        if quant_metrics['response_balance'] < 70:
            suggestions.append("ğŸ“Š ì–‘ë°©í–¥ ì†Œí†µ ê· í˜• ê°œì„ ")
        
        if qual_metrics['emotion_consistency'] < 60:
            suggestions.append("ğŸ¯ ì¼ê´€ëœ í˜‘ì—… ê²½í—˜ ì œê³µ")
        
        if quant_metrics['continuity'] < 0.5:
            suggestions.append("ğŸ“… ì§€ì†ì  í˜‘ì—… ê´€ê³„ êµ¬ì¶•")
    
    else:  # A, S grade
        if quant_metrics['total_responses'] < 30:
            suggestions.append("ğŸ“ˆ í˜‘ì—… í™•ëŒ€ ê¸°íšŒ íƒìƒ‰")
        
        if qual_metrics['emotion_score'] < 1.8:
            suggestions.append("ğŸ’¬ ë” ê¸ì •ì ì¸ í˜‘ì—… ê²½í—˜ í™•ì‚°")
    
    if not suggestions:
        suggestions.append("âœ… í˜„ì¬ ê´€ê³„ í’ˆì§ˆ ìš°ìˆ˜ - í˜„ ìˆ˜ì¤€ ìœ ì§€")
    
    return " | ".join(suggestions)

# ============================================================================
# ğŸ”„ ë©”ì¸ ë¶„ì„ í•¨ìˆ˜
# ============================================================================

def analyze_all_department_relationships():
    """
    ëª¨ë“  ë¶€ì„œê°„ ê´€ê³„ë¥¼ ë¶„ì„í•˜ê³  ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
    """
    # ë°ì´í„° ë¡œë“œ
    df = load_and_preprocess_data()
    if df is None:
        return
    
    print("\nğŸ“Š ë¶€ì„œê°„ ê´€ê³„ ë¶„ì„ ì¤‘...")
    
    # ìƒí˜¸í‰ê°€ ìŒ ì°¾ê¸°
    dept_pairs = set()
    all_depts = set(df['í‰ê°€_ë¶€ì„œëª…'].unique()) | set(df['í”¼í‰ê°€ëŒ€ìƒ ë¶€ì„œëª…'].unique())
    
    for dept_a in all_depts:
        for dept_b in all_depts:
            if dept_a != dept_b:
                # ì–‘ë°©í–¥ í‰ê°€ê°€ ëª¨ë‘ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
                a_to_b_exists = len(df[(df['í‰ê°€_ë¶€ì„œëª…'] == dept_a) & (df['í”¼í‰ê°€ëŒ€ìƒ ë¶€ì„œëª…'] == dept_b)]) > 0
                b_to_a_exists = len(df[(df['í‰ê°€_ë¶€ì„œëª…'] == dept_b) & (df['í”¼í‰ê°€ëŒ€ìƒ ë¶€ì„œëª…'] == dept_a)]) > 0
                
                if a_to_b_exists and b_to_a_exists:
                    # ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•´ ì•ŒíŒŒë²³ ìˆœìœ¼ë¡œ ì •ë ¬
                    pair = tuple(sorted([dept_a, dept_b]))
                    dept_pairs.add(pair)
    
    print(f"ğŸ” ë¶„ì„ ëŒ€ìƒ ë¶€ì„œ ìŒ: {len(dept_pairs)}ê°œ")
    
    # ë¶„ì„ ê²°ê³¼ ì €ì¥
    results = []
    
    for i, (dept_a, dept_b) in enumerate(dept_pairs, 1):
        if i % 50 == 0:
            print(f"   ì§„í–‰ë¥ : {i}/{len(dept_pairs)} ({i/len(dept_pairs)*100:.1f}%)")
        
        # ì •ëŸ‰ì  ì§€í‘œ ê³„ì‚°
        quant_metrics = calculate_quantitative_metrics(df, dept_a, dept_b)
        if not quant_metrics:
            continue
            
        # ì •ì„±ì  ì§€í‘œ ê³„ì‚°
        qual_metrics = calculate_qualitative_metrics(df, dept_a, dept_b)
        if not qual_metrics:
            continue
        
        # ë“±ê¸‰ ë¶„ë¥˜
        grade, reason = classify_relationship_grade(quant_metrics, qual_metrics)
        
        # ê°œì„  í¬ì¸íŠ¸ ì œì•ˆ
        improvements = suggest_improvements(quant_metrics, qual_metrics, grade)
        
        # ê²°ê³¼ ì¢…í•©
        result = {
            'ë¶€ì„œ_A': dept_a,
            'ë¶€ì„œ_B': dept_b,
            'ê´€ê³„_ë“±ê¸‰': grade,
            'ë“±ê¸‰_ì‚¬ìœ ': reason,
            'Aâ†’B_ì ìˆ˜': quant_metrics['a_to_b_score'],
            'Bâ†’A_ì ìˆ˜': quant_metrics['b_to_a_score'],
            'ìƒí˜¸_í‰ê· ì ìˆ˜': quant_metrics['mutual_avg_score'],
            'ì ìˆ˜_ê· í˜•ë„': quant_metrics['score_balance'],
            'Aâ†’B_ì‘ë‹µìˆ˜': quant_metrics['a_to_b_responses'],
            'Bâ†’A_ì‘ë‹µìˆ˜': quant_metrics['b_to_a_responses'],
            'ì´_ì‘ë‹µìˆ˜': quant_metrics['total_responses'],
            'ì‘ë‹µ_ê· í˜•ë„': quant_metrics['response_balance'],
            'í˜‘ì—…_ì§€ì†ì„±': quant_metrics['continuity'],
            'ì ìˆ˜_ì¼ê´€ì„±': quant_metrics['consistency_score'],
            'ë°ì´í„°_ì‹ ë¢°ë„': quant_metrics['reliability_score'],
            'ê°ì •_ì ìˆ˜': qual_metrics['emotion_score'],
            'ê°ì •_ê°•ë„': qual_metrics['emotion_intensity'],
            'ê°ì •_ì¼ê´€ì„±': qual_metrics['emotion_consistency'],
            'ê¸ì •_ë¹„ìœ¨': qual_metrics['positive_ratio'],
            'ë¶€ì •_ë¹„ìœ¨': qual_metrics['negative_ratio'],
            'ê°œì„ _í¬ì¸íŠ¸': improvements
        }
        
        results.append(result)
    
    # ê²°ê³¼ ì €ì¥
    if results:
        results_df = pd.DataFrame(results)
        results_df = results_df.sort_values(['ê´€ê³„_ë“±ê¸‰', 'ìƒí˜¸_í‰ê· ì ìˆ˜'], ascending=[True, False])
        
        # ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥
        with pd.ExcelWriter(OUTPUT_FILE, engine='openpyxl') as writer:
            # ì „ì²´ ê²°ê³¼
            results_df.to_excel(writer, sheet_name='ì „ì²´_ê´€ê³„ë¶„ì„', index=False)
            
            # ë“±ê¸‰ë³„ ë¶„ë¥˜
            for grade in ['S', 'A', 'B', 'C', 'D', 'F']:
                grade_data = results_df[results_df['ê´€ê³„_ë“±ê¸‰'] == grade]
                if not grade_data.empty:
                    grade_data.to_excel(writer, sheet_name=f'{grade}ë“±ê¸‰_ê´€ê³„', index=False)
        
        print(f"\nğŸ‰ ë¶„ì„ ì™„ë£Œ! ê²°ê³¼ê°€ '{OUTPUT_FILE}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # ê²°ê³¼ ìš”ì•½
        print(f"\nğŸ“ˆ ê´€ê³„ í’ˆì§ˆ ë¶„ì„ ê²°ê³¼:")
        grade_counts = results_df['ê´€ê³„_ë“±ê¸‰'].value_counts()
        for grade in ['S', 'A', 'B', 'C', 'D', 'F']:
            count = grade_counts.get(grade, 0)
            print(f"   - {grade}ë“±ê¸‰: {count}ê°œ ê´€ê³„ ({count/len(results_df)*100:.1f}%)")
        
        # ìš°ìˆ˜/ë¬¸ì œ ê´€ê³„ í•˜ì´ë¼ì´íŠ¸
        excellent = len(results_df[results_df['ê´€ê³„_ë“±ê¸‰'].isin(['S', 'A'])])
        problematic = len(results_df[results_df['ê´€ê³„_ë“±ê¸‰'].isin(['D', 'F'])])
        
        print(f"\nğŸŒŸ ìš°ìˆ˜í•œ ê´€ê³„: {excellent}ê°œ ({excellent/len(results_df)*100:.1f}%)")
        print(f"âš ï¸ ê°œì„  í•„ìš” ê´€ê³„: {problematic}ê°œ ({problematic/len(results_df)*100:.1f}%)")
        
    else:
        print("\nâš ï¸ ë¶„ì„í•  ìƒí˜¸í‰ê°€ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

def analyze_temporal_relationships():
    """
    ì‹œê³„ì—´ ê´€ê³„ ë³€í™” ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    # ë°ì´í„° ë¡œë“œ
    df = load_and_preprocess_data()
    if df is None:
        return
    
    print("\nğŸ“ˆ ì‹œê³„ì—´ ê´€ê³„ ë³€í™” ë¶„ì„ ì¤‘...")
    
    # ìƒí˜¸í‰ê°€ ìŒ ì°¾ê¸° (ê¸°ì¡´ê³¼ ë™ì¼)
    dept_pairs = set()
    all_depts = set(df['í‰ê°€_ë¶€ì„œëª…'].unique()) | set(df['í”¼í‰ê°€ëŒ€ìƒ ë¶€ì„œëª…'].unique())
    
    for dept_a in all_depts:
        for dept_b in all_depts:
            if dept_a != dept_b:
                a_to_b_exists = len(df[(df['í‰ê°€_ë¶€ì„œëª…'] == dept_a) & (df['í”¼í‰ê°€ëŒ€ìƒ ë¶€ì„œëª…'] == dept_b)]) > 0
                b_to_a_exists = len(df[(df['í‰ê°€_ë¶€ì„œëª…'] == dept_b) & (df['í”¼í‰ê°€ëŒ€ìƒ ë¶€ì„œëª…'] == dept_a)]) > 0
                
                if a_to_b_exists and b_to_a_exists:
                    pair = tuple(sorted([dept_a, dept_b]))
                    dept_pairs.add(pair)
    
    print(f"ğŸ” ì‹œê³„ì—´ ë¶„ì„ ëŒ€ìƒ ë¶€ì„œ ìŒ: {len(dept_pairs)}ê°œ")
    
    # ì‹œê³„ì—´ ë¶„ì„ ê²°ê³¼ ì €ì¥
    temporal_results = []
    alert_results = []
    
    for i, (dept_a, dept_b) in enumerate(dept_pairs, 1):
        if i % 100 == 0:
            print(f"   ì§„í–‰ë¥ : {i}/{len(dept_pairs)} ({i/len(dept_pairs)*100:.1f}%)")
        
        # ì‹œê³„ì—´ íŠ¸ë Œë“œ ë¶„ì„
        trend_analysis = analyze_relationship_trends(df, dept_a, dept_b)
        if not trend_analysis:
            continue
        
        # íŒ¨í„´ ë¶„ë¥˜
        pattern_type, pattern_desc = classify_trend_pattern(trend_analysis)
        
        # ì¡°ê¸° ê²½ë³´
        alert_level, alert_msg = generate_trend_alert(trend_analysis, trend_analysis['yearly_data'])
        
        # ì—°ë„ë³„ ìƒì„¸ ë°ì´í„° ì¤€ë¹„
        yearly_scores = []
        yearly_emotions = []
        yearly_responses = []
        
        for year in ['2022', '2023', '2024', '2025']:
            if year in trend_analysis['yearly_data']:
                data = trend_analysis['yearly_data'][year]
                yearly_scores.append(f"{year}:{data['mutual_avg_score']:.1f}")
                yearly_emotions.append(f"{year}:{data['emotion_score']:.2f}")
                yearly_responses.append(f"{year}:{data['total_responses']}")
            else:
                yearly_scores.append(f"{year}:N/A")
                yearly_emotions.append(f"{year}:N/A")
                yearly_responses.append(f"{year}:N/A")
        
        # ì‹œê³„ì—´ ê²°ê³¼
        temporal_result = {
            'ë¶€ì„œ_A': dept_a,
            'ë¶€ì„œ_B': dept_b,
            'ë³€í™”_íŒ¨í„´': pattern_type,
            'íŒ¨í„´_ì„¤ëª…': pattern_desc,
            'ì ìˆ˜_ê¸°ìš¸ê¸°': trend_analysis['score_trend'],
            'ìµœê·¼_ê°œì„ ë„': trend_analysis['recent_improvement'],
            'ì ìˆ˜_ë³€ë™ì„±': trend_analysis['score_volatility'],
            'ë°ì´í„°_ì—°ì†ì„±': trend_analysis['data_continuity'],
            'ì—°ë„ë³„_ì ìˆ˜': ' | '.join(yearly_scores),
            'ì—°ë„ë³„_ê°ì •': ' | '.join(yearly_emotions),
            'ì—°ë„ë³„_ì‘ë‹µìˆ˜': ' | '.join(yearly_responses),
            'ê²½ë³´_ìˆ˜ì¤€': alert_level,
            'ê²½ë³´_ë©”ì‹œì§€': alert_msg
        }
        
        temporal_results.append(temporal_result)
        
        # ê²½ë³´ê°€ í•„ìš”í•œ ê´€ê³„ëŠ” ë³„ë„ ìˆ˜ì§‘
        if alert_level in ['urgent', 'warning']:
            alert_results.append(temporal_result)
    
    # ê²°ê³¼ ì €ì¥
    if temporal_results:
        # ê¸°ì¡´ ê²°ê³¼ íŒŒì¼ì— ì‹œê³„ì—´ ì‹œíŠ¸ ì¶”ê°€
        temporal_df = pd.DataFrame(temporal_results)
        temporal_df = temporal_df.sort_values(['ê²½ë³´_ìˆ˜ì¤€', 'ì ìˆ˜_ê¸°ìš¸ê¸°'], ascending=[True, True])
        
        # íŒ¨í„´ë³„ ë¶„ë¥˜
        pattern_summary = temporal_df['ë³€í™”_íŒ¨í„´'].value_counts()
        alert_summary = temporal_df['ê²½ë³´_ìˆ˜ì¤€'].value_counts()
        
        # ì—‘ì…€ íŒŒì¼ ì—…ë°ì´íŠ¸
        with pd.ExcelWriter(OUTPUT_FILE, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:
            # ì‹œê³„ì—´ ë¶„ì„ ì „ì²´ ê²°ê³¼
            temporal_df.to_excel(writer, sheet_name='ì‹œê³„ì—´_ê´€ê³„ë³€í™”', index=False)
            
            # íŒ¨í„´ë³„ ë¶„ë¥˜
            improving_data = temporal_df[temporal_df['ë³€í™”_íŒ¨í„´'].isin(['improving', 'recovering'])]
            if not improving_data.empty:
                improving_data.to_excel(writer, sheet_name='ê°œì„ _ì¶”ì„¸_ê´€ê³„', index=False)
            
            declining_data = temporal_df[temporal_df['ë³€í™”_íŒ¨í„´'].isin(['declining'])]
            if not declining_data.empty:
                declining_data.to_excel(writer, sheet_name='ì•…í™”_ì¶”ì„¸_ê´€ê³„', index=False)
            
            # ê²½ë³´ ëŒ€ìƒ
            if alert_results:
                alert_df = pd.DataFrame(alert_results)
                alert_df.to_excel(writer, sheet_name='ì¡°ê¸°_ê²½ë³´_ëŒ€ìƒ', index=False)
        
        print(f"\nğŸ‰ ì‹œê³„ì—´ ë¶„ì„ ì™„ë£Œ! ê²°ê³¼ê°€ '{OUTPUT_FILE}' íŒŒì¼ì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # ê²°ê³¼ ìš”ì•½
        print(f"\nğŸ“ˆ ì‹œê³„ì—´ ë¶„ì„ ê²°ê³¼:")
        print(f"   - ë¶„ì„ ëŒ€ìƒ: {len(temporal_results)}ê°œ ê´€ê³„")
        
        print(f"\nğŸ”„ ë³€í™” íŒ¨í„´ ë¶„í¬:")
        for pattern, count in pattern_summary.items():
            pattern_names = {
                'improving': 'ì§€ì†ì  ê°œì„ ',
                'declining': 'ì§€ì†ì  ì•…í™”', 
                'recovering': 'íšŒë³µ ì¤‘',
                'stable': 'ì•ˆì •ì ',
                'volatile': 'ë³€ë™ì„± ë†’ìŒ',
                'neutral': 'ë³´í†µ ìˆ˜ì¤€',
                'insufficient_data': 'ë°ì´í„° ë¶€ì¡±'
            }
            print(f"   - {pattern_names.get(pattern, pattern)}: {count}ê°œ ({count/len(temporal_results)*100:.1f}%)")
        
        print(f"\nğŸš¨ ì¡°ê¸° ê²½ë³´ í˜„í™©:")
        for alert, count in alert_summary.items():
            alert_names = {
                'urgent': 'ğŸš¨ ê¸´ê¸‰',
                'warning': 'âš ï¸ ì£¼ì˜',
                'improving': 'ğŸ“ˆ ê°œì„ ',
                'excellent': 'â­ ëª¨ë²”',
                'normal': 'âœ… ì •ìƒ'
            }
            print(f"   - {alert_names.get(alert, alert)}: {count}ê°œ ({count/len(temporal_results)*100:.1f}%)")
        
        # ì£¼ìš” ë°œê²¬ì‚¬í•­
        urgent_count = len(temporal_df[temporal_df['ê²½ë³´_ìˆ˜ì¤€'] == 'urgent'])
        improving_count = len(temporal_df[temporal_df['ë³€í™”_íŒ¨í„´'].isin(['improving', 'recovering'])])
        
        print(f"\nğŸ’¡ ì£¼ìš” ë°œê²¬ì‚¬í•­:")
        print(f"   - ê¸´ê¸‰ ê°œì… í•„ìš”: {urgent_count}ê°œ ê´€ê³„")
        print(f"   - ê°œì„  ì¶”ì„¸: {improving_count}ê°œ ê´€ê³„")
        
    else:
        print("\nâš ï¸ ì‹œê³„ì—´ ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    print("=" * 80)
    print("ğŸ¥ ì„œìš¸ì•„ì‚°ë³‘ì› ë¶€ì„œê°„ ê´€ê³„ ë¶„ì„ ì‹œìŠ¤í…œ")
    print("=" * 80)
    
    import sys
    if len(sys.argv) > 1 and sys.argv[1] == "temporal":
        # ì‹œê³„ì—´ ë¶„ì„ë§Œ ì‹¤í–‰
        analyze_temporal_relationships()
    else:
        # 1. ê¸°ë³¸ ê´€ê³„ ë¶„ì„
        analyze_all_department_relationships()
        
        print("\n" + "=" * 80)
        
        # 2. ì‹œê³„ì—´ ê´€ê³„ ë³€í™” ë¶„ì„
        analyze_temporal_relationships()