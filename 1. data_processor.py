#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Local version of the Colab environment analyzer
Adapted from í˜‘ì—…í‰ê°€_250619.ipynb for local execution

This script provides the same functionality as the Colab notebook
but adapted for local environment without Google Colab dependencies.
"""

import pandas as pd
import datetime
from pathlib import Path
import numpy as np
import re
import time
import unicodedata
import os

# Google Cloud libraries (available in local environment)
import vertexai
from vertexai.generative_models import GenerativeModel
from tqdm.auto import tqdm

# Enable tqdm integration with pandas
tqdm.pandas()

class LocalGoogleSheetsAnalyzer:
    """Local version of the Google Sheets analyzer from Colab"""

    def __init__(self, base_path: str, mapping_file_path: str = None, output_path: str = None, **kwargs):
        self.base_path = Path(base_path)
        self.mapping_file_path = mapping_file_path
        self.standard_map_path = kwargs.get('standard_map_path')
        self.output_path = Path(output_path) if output_path else Path(base_path).parent
        self.department_mapping = {}
        self.department_standard_map = {}

        self.question_columns = [
            'â—‹â—‹ì€ íƒ€ ë¶€ì„œì˜ ì…ì¥ì„ ì¡´ì¤‘í•˜ê³  ë°°ë ¤í•˜ì—¬ í˜‘ë ¥í•´ì£¼ë©°. í˜‘ì—… ê´€ë ¨ ì˜ê²¬ì„ ê²½ì²­í•´ì¤€ë‹¤.',
            'â—‹â—‹ì€ ì—…ë¬´ìƒ í•„ìš”í•œ ì •ë³´ì— ëŒ€í•´ ê³µìœ ê°€ ì˜ ì´ë£¨ì–´ì§„ë‹¤.',
            'â—‹â—‹ì€ ì—…ë¬´ì— ëŒ€í•œ ëª…í™•í•œ ë‹´ë‹¹ìê°€ ìˆê³  ì—…ë¬´ë¥¼ ì¼ê´€ì„±ìˆê²Œ ì²˜ë¦¬í•´ì¤€ë‹¤.',
            'â—‹â—‹ì€ ì´ì „ë³´ë‹¤ ì—…ë¬´ í˜‘ë ¥ì— ëŒ€í•œ íƒœë„ë‚˜ ì˜ì§€ê°€ ê°œì„ ë˜ê³  ìˆë‹¤.',
            'ì „ë°˜ì ìœ¼ë¡œ â—‹â—‹ê³¼ì˜ í˜‘ì—…ì— ëŒ€í•´ ë§Œì¡±í•œë‹¤.'
        ]

    def load_department_standard_map(self):
        """ë¶€ì„œëª… í‘œì¤€í™” ë§¤í•‘ ë¡œë“œ"""
        if not self.standard_map_path or not Path(self.standard_map_path).exists():
            print(f"âš ï¸ ë¶€ì„œëª… í‘œì¤€í™” ë§¤í•‘ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¶€ì„œëª…ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return
        try:
            map_df = pd.read_excel(self.standard_map_path)
            if 'ë³€ê²½ì „_ë¶€ì„œëª…' in map_df.columns and 'í‘œì¤€_ë¶€ì„œëª…' in map_df.columns:
                self.department_standard_map = dict(zip(map_df['ë³€ê²½ì „_ë¶€ì„œëª…'], map_df['í‘œì¤€_ë¶€ì„œëª…']))
                print(f"âœ… ë¶€ì„œëª… í‘œì¤€í™” ë§¤í•‘ ë¡œë“œ ì™„ë£Œ: {len(self.department_standard_map)}ê°œ")
        except Exception as e:
            print(f"âŒ ë¶€ì„œëª… í‘œì¤€í™” ë§¤í•‘ íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜: {e}")

    def load_department_mapping(self):
        """ë¶€ì„œ-ë¶€ë¬¸ ë§¤í•‘ ë¡œë“œ"""
        if not self.mapping_file_path or not Path(self.mapping_file_path).exists(): 
            return
        try:
            mapping_df = pd.read_excel(self.mapping_file_path)
            if 'ë¶€ì„œëª…' in mapping_df.columns and 'ë¶€ë¬¸' in mapping_df.columns:
                self.department_mapping = dict(zip(
                    mapping_df.dropna(subset=['ë¶€ì„œëª…', 'ë¶€ë¬¸'])['ë¶€ì„œëª…'], 
                    mapping_df.dropna(subset=['ë¶€ì„œëª…', 'ë¶€ë¬¸'])['ë¶€ë¬¸']
                ))
                print(f"âœ… ë¶€ë¬¸ ë§¤í•‘ ë¡œë“œ ì™„ë£Œ: {len(self.department_mapping)}ê°œ")
        except Exception as e:
            print(f"âŒ ë¶€ë¬¸ ë§¤í•‘ íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜: {e}")

    def load_and_process_data(self, file_identifiers):
        """ë°ì´í„° ë¡œë“œ ë° ì²˜ë¦¬"""
        print("ğŸ“‚ ë°ì´í„° ë¡œë“œ ì‹œì‘...")
        self.load_department_mapping()
        self.load_department_standard_map()

        integrated_df = pd.DataFrame()
        for identifier in file_identifiers:
            normalized_identifier = unicodedata.normalize('NFKC', identifier)
            file_name_base = unicodedata.normalize('NFKC', 'ì„¤ë¬¸ì¡°ì‚¬ì§„í–‰í˜„í™©[VCRCRIC120S]_')
            file_name = f"{file_name_base}{normalized_identifier}.xlsx"

            actual_file_path = None
            for item in self.base_path.iterdir():
                if unicodedata.normalize('NFKC', item.name) == file_name:
                    actual_file_path = item
                    break

            if not actual_file_path:
                print(f"âš ï¸ íŒŒì¼ ì—†ìŒ: {file_name}")
                continue

            try:
                df = pd.read_excel(actual_file_path, sheet_name='ì§ì›ë³„ëŒ€ìƒë¬¸í•­ í˜„í™©')
                
                # Check if 'ì„¤ë¬¸ì‹œí–‰ì—°ë„' column exists in original data
                if 'ì„¤ë¬¸ì‹œí–‰ì—°ë„' in df.columns:
                    # Use original data as-is
                    pass
                else:
                    # Extract year from identifier (e.g., "2022_1" -> "2022")
                    year = identifier.split('_')[0] if '_' in identifier else identifier
                    df['ì„¤ë¬¸ì‹œí–‰ì—°ë„'] = year
                
                df['response_id'] = [f"{identifier}_{i+1}" for i in range(len(df))]
                integrated_df = pd.concat([integrated_df, df], ignore_index=True)
                print(f"  -> {actual_file_path.name} ({len(df)}í–‰ ë¡œë“œ)")
            except Exception as e:
                print(f"âŒ íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜ {actual_file_path.name}: {e}")

        if integrated_df.empty: 
            return pd.DataFrame(), {}
        
        processed_df = self._preprocess_data(integrated_df)
        self._print_processing_summary(processed_df, {})
        return processed_df, {}

    def _preprocess_data(self, df):
        """ë°ì´í„° ì „ì²˜ë¦¬"""
        print("\nğŸ”„ ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘...")
        df = df.drop_duplicates(keep='first')

        def convert_score(score):
            if pd.isna(score): return None
            try:
                score_float = float(score)
                return ((score_float - 1) / 4) * 100 if score_float in [1, 2, 3, 4, 5] else None
            except (ValueError, TypeError): return None
        
        for q_col in self.question_columns:
            if q_col in df.columns: 
                df[q_col] = df[q_col].apply(convert_score)

        # Column renaming
        rename_map = {'UNITëª…': 'í‰ê°€_Unitëª…'}
        for col in df.columns:
            if 'ì–´ë–¤ ì—…ë¬´ë¥¼ í˜‘ë ¥í•˜ì—¬' in str(col):
                rename_map[col] = 'í˜‘ì—… ìœ í˜•'
            elif 'ë§Œì¡±ìŠ¤ëŸ¬ì› ê±°ë‚˜ ì•„ì‰¬ì› ë˜ ê²½í—˜' in str(col):
                rename_map[col] = 'í˜‘ì—… í›„ê¸°'

        df.rename(columns=rename_map, inplace=True)
        print("âœ… ì„œìˆ í˜• ì»¬ëŸ¼ ë° Unitëª… ë³€ê²½ ì™„ë£Œ")

        # Standardize department names
        cols_to_standardize = ['í‰ê°€_ë¶€ì„œëª…', 'í”¼í‰ê°€ëŒ€ìƒ ë¶€ì„œëª…']
        if 'ë¶€ì„œëª…' in df.columns: 
            df.rename(columns={'ë¶€ì„œëª…': 'í‰ê°€_ë¶€ì„œëª…'}, inplace=True)

        if self.department_standard_map:
            for col_name in cols_to_standardize:
                if col_name in df.columns:
                    original_col_name = f"{col_name.replace(' ', '_')}_ì›ë³¸"
                    df.rename(columns={col_name: original_col_name}, inplace=True)
                    df[col_name] = df[original_col_name].map(self.department_standard_map).fillna(df[original_col_name])
                    print(f"âœ… '{col_name}' ì»¬ëŸ¼ í‘œì¤€í™” ì ìš© ì™„ë£Œ")

        # Calculate scores
        available_q_cols = [col for col in self.question_columns if col in df.columns]
        if available_q_cols:
            df['ê²°ì¸¡ê°’'] = df[available_q_cols].isnull().any(axis=1).apply(lambda x: 'Y' if x else 'N')
            df['ì¢…í•©ì ìˆ˜'] = df[available_q_cols].mean(axis=1, skipna=True).round(2)
        
        def check_extreme_value(row):
            return 'ê·¹ë‹¨ê°’' if pd.notna(row.get('ì¢…í•©ì ìˆ˜')) and row['ì¢…í•©ì ìˆ˜'] == 0 else 'ì •ìƒ'
        df['ê·¹ë‹¨ê°’'] = df.apply(check_extreme_value, axis=1)

        # Map departments to divisions
        if 'í”¼í‰ê°€ëŒ€ìƒ ë¶€ì„œëª…' in df.columns:
            df['ë¶€ë¬¸'] = df['í”¼í‰ê°€ëŒ€ìƒ ë¶€ì„œëª…'].map(self.department_mapping).fillna('ë¯¸ë¶„ë¥˜')
            print("ğŸ¢ í”¼í‰ê°€ëŒ€ìƒ ë¶€ë¬¸ ë§¤í•‘ ì™„ë£Œ")

        if 'í‰ê°€_ë¶€ì„œëª…' in df.columns:
            df['í‰ê°€_ë¶€ë¬¸'] = df['í‰ê°€_ë¶€ì„œëª…'].map(self.department_mapping).fillna('ë¯¸ë¶„ë¥˜')
            print("ğŸ¢ í‰ê°€ì ë¶€ë¬¸ ë§¤í•‘ ì™„ë£Œ")

        return df

    def _print_processing_summary(self, df, processing_stats):
        """ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "="*60 + "\nğŸ“Š ë°ì´í„° ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½\n" + "="*60)
        print(f"ğŸ“‹ ì´ ì‘ë‹µ ìˆ˜: {len(df):,}ê°œ")
        if 'ê²°ì¸¡ê°’' in df.columns:
            print(f"â“ ê²°ì¸¡ê°’ í¬í•¨ ì‘ë‹µ: {(df['ê²°ì¸¡ê°’'] == 'Y').sum():,}ê°œ")
        if 'ê·¹ë‹¨ê°’' in df.columns:
            print(f"âš ï¸ ê·¹ë‹¨ê°’(ì¢…í•©ì ìˆ˜ 0ì ) ì‘ë‹µ: {(df['ê·¹ë‹¨ê°’'] == 'ê·¹ë‹¨ê°’').sum():,}ê°œ")
        if 'ì¢…í•©ì ìˆ˜' in df.columns and df['ì¢…í•©ì ìˆ˜'].notna().any():
            print(f"ğŸ“Š ì „ì²´ í‰ê·  ì ìˆ˜ (0-100ì  ì²™ë„): {df['ì¢…í•©ì ìˆ˜'].mean():.2f}ì ")
        print("="*60)

    def prepare_final_output(self, df):
        """ìµœì¢… ì¶œë ¥ì„ ìœ„í•œ ì»¬ëŸ¼ ìˆœì„œ ì •ë¦¬"""
        desired_columns = [
            'response_id', 'ì„¤ë¬¸ì‹œí–‰ì—°ë„', 'í‰ê°€_ë¶€ì„œëª…', 'í‰ê°€_ë¶€ì„œëª…_ì›ë³¸', 'í‰ê°€_Unitëª…', 'í‰ê°€_ë¶€ë¬¸',
            'í”¼í‰ê°€ëŒ€ìƒ ë¶€ì„œëª…', 'í”¼í‰ê°€ëŒ€ìƒ_ë¶€ì„œëª…_ì›ë³¸', 'í”¼í‰ê°€ëŒ€ìƒ UNITëª…', 'í”¼í‰ê°€ëŒ€ìƒ ë¶€ë¬¸',
            'â—‹â—‹ì€ íƒ€ ë¶€ì„œì˜ ì…ì¥ì„ ì¡´ì¤‘í•˜ê³  ë°°ë ¤í•˜ì—¬ í˜‘ë ¥í•´ì£¼ë©°. í˜‘ì—… ê´€ë ¨ ì˜ê²¬ì„ ê²½ì²­í•´ì¤€ë‹¤.',
            'â—‹â—‹ì€ ì—…ë¬´ìƒ í•„ìš”í•œ ì •ë³´ì— ëŒ€í•´ ê³µìœ ê°€ ì˜ ì´ë£¨ì–´ì§„ë‹¤.',
            'â—‹â—‹ì€ ì—…ë¬´ì— ëŒ€í•œ ëª…í™•í•œ ë‹´ë‹¹ìê°€ ìˆê³  ì—…ë¬´ë¥¼ ì¼ê´€ì„±ìˆê²Œ ì²˜ë¦¬í•´ì¤€ë‹¤.',
            'â—‹â—‹ì€ ì´ì „ë³´ë‹¤ ì—…ë¬´ í˜‘ë ¥ì— ëŒ€í•œ íƒœë„ë‚˜ ì˜ì§€ê°€ ê°œì„ ë˜ê³  ìˆë‹¤.',
            'ì „ë°˜ì ìœ¼ë¡œ â—‹â—‹ê³¼ì˜ í˜‘ì—…ì— ëŒ€í•´ ë§Œì¡±í•œë‹¤.',
            'ì¢…í•©ì ìˆ˜', 'ê·¹ë‹¨ê°’', 'ê²°ì¸¡ê°’', 'í˜‘ì—… ìœ í˜•', 'í˜‘ì—… í›„ê¸°'
        ]
        
        # ì»¬ëŸ¼ëª… ë§¤í•‘ (ê¸°ì¡´ ì»¬ëŸ¼ëª… â†’ ì›í•˜ëŠ” ì»¬ëŸ¼ëª…)
        column_mapping = {
            'ë¶€ë¬¸': 'í”¼í‰ê°€ëŒ€ìƒ ë¶€ë¬¸'
        }
        
        # ì»¬ëŸ¼ëª… ë³€ê²½
        df_final = df.copy()
        for old_name, new_name in column_mapping.items():
            if old_name in df_final.columns and new_name != old_name:
                df_final.rename(columns={old_name: new_name}, inplace=True)
                # ì›í•˜ëŠ” ìˆœì„œë¡œ ì»¬ëŸ¼ ì¬ì •ë ¬ (ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ)
        available_columns = [col for col in desired_columns if col in df_final.columns]
        df_final = df_final[available_columns]
        
        print(f"âœ… ìµœì¢… ì¶œë ¥ í˜•ì‹ ì •ë¦¬ ì™„ë£Œ: {len(available_columns)}ê°œ ì»¬ëŸ¼")
        return df_final


def main_local_analysis(base_path: str, project_id: str = None):
    """ë©”ì¸ ë¡œì»¬ ë¶„ì„ í•¨ìˆ˜"""
    print("ğŸš€ ë¡œì»¬ í™˜ê²½ì—ì„œ í˜‘ì—… í‰ê°€ ë¶„ì„ ì‹œì‘")
    
    # íŒŒì¼ ì¸ì‹
    raw_data_path = Path(base_path)
    file_prefix = "ì„¤ë¬¸ì¡°ì‚¬ì§„í–‰í˜„í™©[VCRCRIC120S]_"
    file_suffix = ".xlsx"
    file_identifiers = []
    
    if raw_data_path.exists():
        normalized_prefix = unicodedata.normalize('NFKC', file_prefix)
        for item in raw_data_path.iterdir():
            if item.is_file():
                normalized_filename = unicodedata.normalize('NFKC', item.name)
                if normalized_filename.startswith(normalized_prefix) and normalized_filename.endswith(file_suffix):
                    identifier = normalized_filename.replace(normalized_prefix, "").replace(file_suffix, "")
                    file_identifiers.append(identifier)
        file_identifiers.sort()

    if not file_identifiers:
        print(f"\nâŒ ë¶„ì„í•  ì„¤ë¬¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None

    print(f"\nâœ… {len(file_identifiers)}ê°œì˜ ì„¤ë¬¸ íŒŒì¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤: {file_identifiers}")

    # ë§¤í•‘ íŒŒì¼ ê²½ë¡œ ì„¤ì •
    standard_map_path = raw_data_path / "ë¶€ì„œëª…_í‘œì¤€í™”_ë§¤í•‘.xlsx"
    mapping_file_path = raw_data_path / "ë¶€ì„œ_ë¶€ë¬¸_ë§¤í•‘.xlsx"

    
    # ë°ì´í„° ë¶„ì„ê¸° ì‹¤í–‰
    analyzer = LocalGoogleSheetsAnalyzer(
        base_path, 
        mapping_file_path=str(mapping_file_path) if mapping_file_path.exists() else None,
        standard_map_path=str(standard_map_path) if standard_map_path.exists() else None
    )
    processed_df, _ = analyzer.load_and_process_data(file_identifiers)
    
    if processed_df.empty:
        print("âŒ ì²˜ë¦¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None

    # AI ë¶„ì„ (ì˜µì…˜)
    if project_id:
        ai_analyzer = LocalAIAnalyzer(project_id)
        processed_df = ai_analyzer.analyze_dataframe(processed_df)

    # ìµœì¢… ì¶œë ¥ í˜•ì‹ ì •ë¦¬
    analyzer = LocalGoogleSheetsAnalyzer(
        base_path, 
        mapping_file_path=str(mapping_file_path) if mapping_file_path.exists() else None,
        standard_map_path=str(standard_map_path) if standard_map_path.exists() else None
    )
    final_df = analyzer.prepare_final_output(processed_df)
    
    # ê²°ê³¼ ì €ì¥
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    output_filename = f"1. data_processor_ê²°ê³¼_{timestamp}.xlsx"
    output_path = Path(base_path) / output_filename
    final_df.to_excel(output_path, index=False)
    print(f"âœ… ê²°ê³¼ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {output_path}")

    return final_df


if __name__ == "__main__":
    # ì‚¬ìš© ì˜ˆì‹œ
    BASE_PATH = "./rawdata"  # rawdata í´ë”ì—ì„œ íŒŒì¼ ê²€ìƒ‰
    PROJECT_ID = None  # Google Cloud í”„ë¡œì íŠ¸ ID (ì˜µì…˜)
    
    result_df = main_local_analysis(BASE_PATH, PROJECT_ID)
    if result_df is not None:
        print("\nğŸ‰ ë¡œì»¬ ë¶„ì„ ì™„ë£Œ!")
        print(f"ì´ {len(result_df)}ê°œì˜ ë ˆì½”ë“œê°€ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")